SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/avro-tools-1.7.6-cdh5.4.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/05/13 09:06:07 INFO SparkContext: Running Spark version 1.3.0
15/05/13 09:06:07 WARN SparkConf: 
SPARK_CLASSPATH was detected (set to '/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:').
This is deprecated in Spark 1.0+.

Please instead use:
 - ./spark-submit with --driver-class-path to augment the driver classpath
 - spark.executor.extraClassPath to augment the executor classpath
        
15/05/13 09:06:07 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:' as a work-around.
15/05/13 09:06:07 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:' as a work-around.
15/05/13 09:06:09 INFO SecurityManager: Changing view acls to: root
15/05/13 09:06:09 INFO SecurityManager: Changing modify acls to: root
15/05/13 09:06:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/05/13 09:06:09 INFO Slf4jLogger: Slf4jLogger started
15/05/13 09:06:09 INFO Remoting: Starting remoting
15/05/13 09:06:10 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@pti-base.insafanalytics.com:38673]
15/05/13 09:06:10 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@pti-base.insafanalytics.com:38673]
15/05/13 09:06:10 INFO Utils: Successfully started service 'sparkDriver' on port 38673.
15/05/13 09:06:10 INFO SparkEnv: Registering MapOutputTracker
15/05/13 09:06:10 INFO SparkEnv: Registering BlockManagerMaster
15/05/13 09:06:10 INFO DiskBlockManager: Created local directory at /tmp/spark-ca7c3614-c9bc-4e1d-94e0-bda06b0b2ad8/blockmgr-f08f159e-da4a-493b-b75d-4606bb8da02d
15/05/13 09:06:10 INFO MemoryStore: MemoryStore started with capacity 265.4 MB
15/05/13 09:06:10 INFO HttpFileServer: HTTP File server directory is /tmp/spark-a580aa70-2042-4376-96ba-5f19e61d9d53/httpd-e9ab9903-6464-4f76-a423-dd1078ba9081
15/05/13 09:06:10 INFO HttpServer: Starting HTTP Server
15/05/13 09:06:10 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/13 09:06:10 INFO AbstractConnector: Started SocketConnector@0.0.0.0:52591
15/05/13 09:06:10 INFO Utils: Successfully started service 'HTTP file server' on port 52591.
15/05/13 09:06:10 INFO SparkEnv: Registering OutputCommitCoordinator
15/05/13 09:06:10 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/13 09:06:10 WARN AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:207)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:217)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:217)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1832)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1823)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:217)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:307)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:307)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:307)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:06:10 WARN AbstractLifeCycle: FAILED org.spark-project.jetty.server.Server@b45e780: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:214)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.open(SelectChannelConnector.java:187)
	at org.spark-project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:316)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.doStart(SelectChannelConnector.java:265)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.spark-project.jetty.server.Server.doStart(Server.java:293)
	at org.spark-project.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:64)
	at org.apache.spark.ui.JettyUtils$.org$apache$spark$ui$JettyUtils$$connect$1(JettyUtils.scala:207)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:217)
	at org.apache.spark.ui.JettyUtils$$anonfun$2.apply(JettyUtils.scala:217)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1832)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1823)
	at org.apache.spark.ui.JettyUtils$.startJettyServer(JettyUtils.scala:217)
	at org.apache.spark.ui.WebUI.bind(WebUI.scala:102)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:307)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:307)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:307)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:214)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
15/05/13 09:06:10 INFO ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
15/05/13 09:06:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
15/05/13 09:06:10 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/13 09:06:10 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4041
15/05/13 09:06:10 INFO Utils: Successfully started service 'SparkUI' on port 4041.
15/05/13 09:06:10 INFO SparkUI: Started SparkUI at http://pti-base.insafanalytics.com:4041
15/05/13 09:06:10 INFO Utils: Copying /root/spark-scripts/pti.py to /tmp/spark-9d62caf7-53b5-4125-ade6-706f6b8c476d/userFiles-2851e2fa-871c-4b5c-9edd-5c073071f5d8/pti.py
15/05/13 09:06:10 INFO SparkContext: Added file file:/root/spark-scripts/pti.py at http://45.55.231.94:52591/files/pti.py with timestamp 1431522370866
15/05/13 09:06:10 INFO AppClient$ClientActor: Connecting to master akka.tcp://sparkMaster@pti-base.insafanalytics.com:7079/user/Master...
15/05/13 09:06:11 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150513090611-0000
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor added: app-20150513090611-0000/0 on worker-20150513085628-pti-base.insafanalytics.com-44943 (pti-base.insafanalytics.com:44943) with 2 cores
15/05/13 09:06:11 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150513090611-0000/0 on hostPort pti-base.insafanalytics.com:44943 with 2 cores, 512.0 MB RAM
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor added: app-20150513090611-0000/1 on worker-20150513090034-pti-node-2.insafanalytics.com-41820 (pti-node-2.insafanalytics.com:41820) with 1 cores
15/05/13 09:06:11 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150513090611-0000/1 on hostPort pti-node-2.insafanalytics.com:41820 with 1 cores, 512.0 MB RAM
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor added: app-20150513090611-0000/2 on worker-20150513085913-pti-node-1.insafanalytics.com-39128 (pti-node-1.insafanalytics.com:39128) with 1 cores
15/05/13 09:06:11 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150513090611-0000/2 on hostPort pti-node-1.insafanalytics.com:39128 with 1 cores, 512.0 MB RAM
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/0 is now RUNNING
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/1 is now RUNNING
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/2 is now RUNNING
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/2 is now LOADING
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/0 is now LOADING
15/05/13 09:06:11 INFO NettyBlockTransferService: Server created on 44723
15/05/13 09:06:11 INFO BlockManagerMaster: Trying to register BlockManager
15/05/13 09:06:11 INFO BlockManagerMasterActor: Registering block manager pti-base.insafanalytics.com:44723 with 265.4 MB RAM, BlockManagerId(<driver>, pti-base.insafanalytics.com, 44723)
15/05/13 09:06:11 INFO BlockManagerMaster: Registered BlockManager
15/05/13 09:06:11 INFO AppClient$ClientActor: Executor updated: app-20150513090611-0000/1 is now LOADING
15/05/13 09:06:12 INFO EventLoggingListener: Logging events to hdfs://pti-base.insafanalytics.com:8020/user/spark/applicationHistory/app-20150513090611-0000
15/05/13 09:06:13 INFO SparkDeploySchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
15/05/13 09:06:13 INFO FileInputDStream: Duration for remembering RDDs set to 60000 ms for org.apache.spark.streaming.dstream.FileInputDStream@1c538bd7
15/05/13 09:06:13 INFO ForEachDStream: metadataCleanupDelay = -1
15/05/13 09:06:13 INFO MappedDStream: metadataCleanupDelay = -1
15/05/13 09:06:13 INFO FileInputDStream: metadataCleanupDelay = -1
15/05/13 09:06:13 INFO FileInputDStream: Slide time = 60000 ms
15/05/13 09:06:13 INFO FileInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/05/13 09:06:13 INFO FileInputDStream: Checkpoint interval = null
15/05/13 09:06:13 INFO FileInputDStream: Remember duration = 60000 ms
15/05/13 09:06:13 INFO FileInputDStream: Initialized and validated org.apache.spark.streaming.dstream.FileInputDStream@1c538bd7
15/05/13 09:06:13 INFO MappedDStream: Slide time = 60000 ms
15/05/13 09:06:13 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/05/13 09:06:13 INFO MappedDStream: Checkpoint interval = null
15/05/13 09:06:13 INFO MappedDStream: Remember duration = 60000 ms
15/05/13 09:06:13 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3c133d32
15/05/13 09:06:13 INFO ForEachDStream: Slide time = 60000 ms
15/05/13 09:06:13 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/05/13 09:06:13 INFO ForEachDStream: Checkpoint interval = null
15/05/13 09:06:13 INFO ForEachDStream: Remember duration = 60000 ms
15/05/13 09:06:13 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@1316bc6e
15/05/13 09:06:13 INFO RecurringTimer: Started timer for JobGenerator at time 1431522420000
15/05/13 09:06:13 INFO JobGenerator: Started JobGenerator at 1431522420000 ms
15/05/13 09:06:13 INFO JobScheduler: Started JobScheduler
15/05/13 09:06:15 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@pti-node-1.insafanalytics.com:53098/user/Executor#17620082] with ID 2
15/05/13 09:06:15 INFO BlockManagerMasterActor: Registering block manager pti-node-1.insafanalytics.com:34093 with 265.4 MB RAM, BlockManagerId(2, pti-node-1.insafanalytics.com, 34093)
15/05/13 09:06:17 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@pti-base.insafanalytics.com:57799/user/Executor#-2106926606] with ID 0
15/05/13 09:06:17 INFO BlockManagerMasterActor: Registering block manager pti-base.insafanalytics.com:60443 with 265.4 MB RAM, BlockManagerId(0, pti-base.insafanalytics.com, 60443)
15/05/13 09:06:19 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@pti-node-2.insafanalytics.com:55640/user/Executor#1095725703] with ID 1
15/05/13 09:06:20 INFO BlockManagerMasterActor: Registering block manager pti-node-2.insafanalytics.com:33883 with 265.4 MB RAM, BlockManagerId(1, pti-node-2.insafanalytics.com, 33883)
15/05/13 09:07:00 INFO FileInputDStream: Finding new files took 165 ms
15/05/13 09:07:00 INFO FileInputDStream: New files at time 1431522420000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522338996.json
15/05/13 09:07:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=0, maxMem=278302556
15/05/13 09:07:00 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 232.9 KB, free 265.2 MB)
15/05/13 09:07:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=238532, maxMem=278302556
15/05/13 09:07:00 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.9 KB, free 265.1 MB)
15/05/13 09:07:00 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.4 MB)
15/05/13 09:07:00 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/05/13 09:07:00 INFO SparkContext: Created broadcast 0 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:07:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:07:01 INFO JobScheduler: Added jobs for time 1431522420000 ms
15/05/13 09:07:01 INFO JobGenerator: Checkpointing graph for time 1431522420000 ms
15/05/13 09:07:01 INFO DStreamGraph: Updating checkpoint data for time 1431522420000 ms
15/05/13 09:07:01 INFO JobScheduler: Starting job streaming job 1431522420000 ms.0 from job set of time 1431522420000 ms
15/05/13 09:07:01 INFO DStreamGraph: Updated checkpoint data for time 1431522420000 ms
15/05/13 09:07:01 INFO CheckpointWriter: Saving checkpoint for time 1431522420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522420000'
15/05/13 09:07:01 INFO CheckpointWriter: Checkpoint for time 1431522420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522420000', took 7631 bytes and 135 ms
15/05/13 09:07:01 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:07:01 INFO DAGScheduler: Got job 0 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:07:01 INFO DAGScheduler: Final stage: Stage 0(reduce at JsonRDD.scala:51)
15/05/13 09:07:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:07:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:07:01 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[6] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:07:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=274240, maxMem=278302556
15/05/13 09:07:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 5.9 KB, free 265.1 MB)
15/05/13 09:07:01 INFO MemoryStore: ensureFreeSpace(4221) called with curMem=280280, maxMem=278302556
15/05/13 09:07:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.1 KB, free 265.1 MB)
15/05/13 09:07:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:07:01 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/05/13 09:07:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:839
15/05/13 09:07:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[6] at map at JsonRDD.scala:51)
15/05/13 09:07:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/05/13 09:07:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:07:02 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:07:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.4 MB)
15/05/13 09:07:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7312 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:07:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/05/13 09:07:11 INFO DAGScheduler: Stage 0 (reduce at JsonRDD.scala:51) finished in 9.410 s
15/05/13 09:07:11 INFO DAGScheduler: Job 0 finished: reduce at JsonRDD.scala:51, took 9.623271 s
15/05/13 09:07:13 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:07:13 INFO DAGScheduler: Got job 1 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:07:13 INFO DAGScheduler: Final stage: Stage 1(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:07:13 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:07:13 INFO DAGScheduler: Missing parents: List()
15/05/13 09:07:13 INFO DAGScheduler: Submitting Stage 1 (MapPartitionsRDD[13] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:07:13 INFO MemoryStore: ensureFreeSpace(20760) called with curMem=284501, maxMem=278302556
15/05/13 09:07:13 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 20.3 KB, free 265.1 MB)
15/05/13 09:07:13 INFO MemoryStore: ensureFreeSpace(10787) called with curMem=305261, maxMem=278302556
15/05/13 09:07:13 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.5 KB, free 265.1 MB)
15/05/13 09:07:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 265.4 MB)
15/05/13 09:07:13 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/05/13 09:07:13 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:839
15/05/13 09:07:13 INFO DAGScheduler: Submitting 1 missing tasks from Stage 1 (MapPartitionsRDD[13] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:07:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/05/13 09:07:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:07:13 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.5 KB, free: 265.4 MB)
15/05/13 09:07:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 453 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:07:14 INFO DAGScheduler: Stage 1 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.456 s
15/05/13 09:07:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/05/13 09:07:14 INFO DAGScheduler: Job 1 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.538998 s
/usr/local/lib/python2.7/site-packages/pandas/io/sql.py:588: FutureWarning: The 'mysql' flavor with DBAPI connection is deprecated and will be removed in future versions. MySQL will be further supported with SQLAlchemy engines.
  warnings.warn(_MYSQL_WARNING, FutureWarning)
15/05/13 09:07:14 INFO JobScheduler: Finished job streaming job 1431522420000 ms.0 from job set of time 1431522420000 ms
15/05/13 09:07:14 INFO JobScheduler: Total delay: 14.283 s for time 1431522420000 ms (execution: 13.231 s)
15/05/13 09:07:14 INFO FileInputDStream: Cleared 0 old files that were older than 1431522360000 ms: 
15/05/13 09:07:14 INFO JobGenerator: Checkpointing graph for time 1431522420000 ms
15/05/13 09:07:14 INFO DStreamGraph: Updating checkpoint data for time 1431522420000 ms
15/05/13 09:07:14 INFO DStreamGraph: Updated checkpoint data for time 1431522420000 ms
15/05/13 09:07:14 INFO CheckpointWriter: Saving checkpoint for time 1431522420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522420000'
15/05/13 09:07:14 INFO CheckpointWriter: Checkpoint for time 1431522420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522420000', took 7626 bytes and 93 ms
15/05/13 09:07:14 INFO DStreamGraph: Clearing checkpoint data for time 1431522420000 ms
15/05/13 09:07:14 INFO DStreamGraph: Cleared checkpoint data for time 1431522420000 ms
15/05/13 09:07:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:08:00 INFO FileInputDStream: Finding new files took 34 ms
15/05/13 09:08:00 INFO FileInputDStream: New files at time 1431522480000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522399956.json
15/05/13 09:08:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=316048, maxMem=278302556
15/05/13 09:08:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 232.9 KB, free 264.9 MB)
15/05/13 09:08:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=554580, maxMem=278302556
15/05/13 09:08:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.9 KB, free 264.8 MB)
15/05/13 09:08:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:08:00 INFO BlockManagerMaster: Updated info of block broadcast_3_piece0
15/05/13 09:08:00 INFO SparkContext: Created broadcast 3 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:08:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:08:00 INFO JobScheduler: Added jobs for time 1431522480000 ms
15/05/13 09:08:00 INFO JobGenerator: Checkpointing graph for time 1431522480000 ms
15/05/13 09:08:00 INFO DStreamGraph: Updating checkpoint data for time 1431522480000 ms
15/05/13 09:08:00 INFO JobScheduler: Starting job streaming job 1431522480000 ms.0 from job set of time 1431522480000 ms
15/05/13 09:08:00 INFO DStreamGraph: Updated checkpoint data for time 1431522480000 ms
15/05/13 09:08:00 INFO CheckpointWriter: Saving checkpoint for time 1431522480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522480000'
15/05/13 09:08:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:08:00 INFO DAGScheduler: Got job 2 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:08:00 INFO DAGScheduler: Final stage: Stage 2(reduce at JsonRDD.scala:51)
15/05/13 09:08:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:08:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:08:00 INFO DAGScheduler: Submitting Stage 2 (MapPartitionsRDD[20] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:08:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=590290, maxMem=278302556
15/05/13 09:08:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522180000
15/05/13 09:08:00 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 5.9 KB, free 264.8 MB)
15/05/13 09:08:00 INFO CheckpointWriter: Checkpoint for time 1431522480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522480000', took 7660 bytes and 139 ms
15/05/13 09:08:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=596330, maxMem=278302556
15/05/13 09:08:00 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.8 MB)
15/05/13 09:08:00 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:08:00 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/05/13 09:08:00 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:839
15/05/13 09:08:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 2 (MapPartitionsRDD[20] at map at JsonRDD.scala:51)
15/05/13 09:08:00 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/05/13 09:08:00 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:08:01 INFO BlockManager: Removing broadcast 2
15/05/13 09:08:01 INFO BlockManager: Removing block broadcast_2
15/05/13 09:08:01 INFO MemoryStore: Block broadcast_2 of size 20760 dropped from memory (free 277722759)
15/05/13 09:08:01 INFO BlockManager: Removing block broadcast_2_piece0
15/05/13 09:08:01 INFO MemoryStore: Block broadcast_2_piece0 of size 10787 dropped from memory (free 277733546)
15/05/13 09:08:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 265.3 MB)
15/05/13 09:08:01 INFO BlockManagerMaster: Updated info of block broadcast_2_piece0
15/05/13 09:08:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.5 KB, free: 265.4 MB)
15/05/13 09:08:01 INFO ContextCleaner: Cleaned broadcast 2
15/05/13 09:08:01 INFO BlockManager: Removing broadcast 1
15/05/13 09:08:01 INFO BlockManager: Removing block broadcast_1_piece0
15/05/13 09:08:01 INFO MemoryStore: Block broadcast_1_piece0 of size 4221 dropped from memory (free 277737767)
15/05/13 09:08:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:08:01 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/05/13 09:08:01 INFO BlockManager: Removing block broadcast_1
15/05/13 09:08:01 INFO MemoryStore: Block broadcast_1 of size 6040 dropped from memory (free 277743807)
15/05/13 09:08:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:08:01 INFO ContextCleaner: Cleaned broadcast 1
15/05/13 09:08:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:08:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.4 MB)
15/05/13 09:08:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 11435 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:08:11 INFO DAGScheduler: Stage 2 (reduce at JsonRDD.scala:51) finished in 11.467 s
15/05/13 09:08:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/05/13 09:08:11 INFO DAGScheduler: Job 2 finished: reduce at JsonRDD.scala:51, took 11.555086 s
15/05/13 09:08:12 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:08:12 INFO DAGScheduler: Got job 3 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:08:12 INFO DAGScheduler: Final stage: Stage 3(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:08:12 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:08:12 INFO DAGScheduler: Missing parents: List()
15/05/13 09:08:12 INFO DAGScheduler: Submitting Stage 3 (MapPartitionsRDD[27] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:08:12 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=558749, maxMem=278302556
15/05/13 09:08:12 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 20.8 KB, free 264.9 MB)
15/05/13 09:08:12 INFO MemoryStore: ensureFreeSpace(11128) called with curMem=580053, maxMem=278302556
15/05/13 09:08:12 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.9 KB, free 264.8 MB)
15/05/13 09:08:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:08:12 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/05/13 09:08:12 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:839
15/05/13 09:08:12 INFO DAGScheduler: Submitting 1 missing tasks from Stage 3 (MapPartitionsRDD[27] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:08:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/05/13 09:08:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:08:12 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 265.4 MB)
15/05/13 09:08:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 518 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:08:13 INFO DAGScheduler: Stage 3 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.521 s
15/05/13 09:08:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/05/13 09:08:13 INFO DAGScheduler: Job 3 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.571853 s
15/05/13 09:08:13 INFO JobScheduler: Finished job streaming job 1431522480000 ms.0 from job set of time 1431522480000 ms
15/05/13 09:08:13 INFO JobScheduler: Total delay: 13.128 s for time 1431522480000 ms (execution: 12.896 s)
15/05/13 09:08:13 INFO MapPartitionsRDD: Removing RDD 2 from persistence list
15/05/13 09:08:13 INFO BlockManager: Removing RDD 2
15/05/13 09:08:13 INFO UnionRDD: Removing RDD 1 from persistence list
15/05/13 09:08:13 INFO BlockManager: Removing RDD 1
15/05/13 09:08:13 INFO FileInputDStream: Cleared 0 old files that were older than 1431522420000 ms: 
15/05/13 09:08:13 INFO JobGenerator: Checkpointing graph for time 1431522480000 ms
15/05/13 09:08:13 INFO DStreamGraph: Updating checkpoint data for time 1431522480000 ms
15/05/13 09:08:13 INFO DStreamGraph: Updated checkpoint data for time 1431522480000 ms
15/05/13 09:08:13 INFO CheckpointWriter: Saving checkpoint for time 1431522480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522480000'
15/05/13 09:08:13 INFO CheckpointWriter: Checkpoint for time 1431522480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522480000', took 7655 bytes and 303 ms
15/05/13 09:08:13 INFO DStreamGraph: Clearing checkpoint data for time 1431522480000 ms
15/05/13 09:08:13 INFO DStreamGraph: Cleared checkpoint data for time 1431522480000 ms
15/05/13 09:08:13 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:09:00 INFO FileInputDStream: Finding new files took 34 ms
15/05/13 09:09:00 INFO FileInputDStream: New files at time 1431522540000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522464224.json
15/05/13 09:09:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=591181, maxMem=278302556
15/05/13 09:09:00 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 232.9 KB, free 264.6 MB)
15/05/13 09:09:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=829713, maxMem=278302556
15/05/13 09:09:00 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.9 KB, free 264.6 MB)
15/05/13 09:09:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:09:00 INFO BlockManagerMaster: Updated info of block broadcast_6_piece0
15/05/13 09:09:00 INFO SparkContext: Created broadcast 6 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:09:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:09:00 INFO JobScheduler: Added jobs for time 1431522540000 ms
15/05/13 09:09:00 INFO JobGenerator: Checkpointing graph for time 1431522540000 ms
15/05/13 09:09:00 INFO DStreamGraph: Updating checkpoint data for time 1431522540000 ms
15/05/13 09:09:00 INFO JobScheduler: Starting job streaming job 1431522540000 ms.0 from job set of time 1431522540000 ms
15/05/13 09:09:00 INFO DStreamGraph: Updated checkpoint data for time 1431522540000 ms
15/05/13 09:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431522540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000'
15/05/13 09:09:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:09:00 INFO DAGScheduler: Got job 4 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:09:00 INFO DAGScheduler: Final stage: Stage 4(reduce at JsonRDD.scala:51)
15/05/13 09:09:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:09:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:09:00 INFO DAGScheduler: Submitting Stage 4 (MapPartitionsRDD[34] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:09:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=865421, maxMem=278302556
15/05/13 09:09:00 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.9 KB, free 264.6 MB)
15/05/13 09:09:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=871461, maxMem=278302556
15/05/13 09:09:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000
15/05/13 09:09:00 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.6 MB)
15/05/13 09:09:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:09:00 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/05/13 09:09:00 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:839
15/05/13 09:09:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 4 (MapPartitionsRDD[34] at map at JsonRDD.scala:51)
15/05/13 09:09:00 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/05/13 09:09:00 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:09:00 INFO CheckpointWriter: Checkpoint for time 1431522540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000', took 7661 bytes and 201 ms
15/05/13 09:09:00 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:09:00 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:09:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1633 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:09:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/05/13 09:09:02 INFO DAGScheduler: Stage 4 (reduce at JsonRDD.scala:51) finished in 1.692 s
15/05/13 09:09:02 INFO DAGScheduler: Job 4 finished: reduce at JsonRDD.scala:51, took 1.750006 s
15/05/13 09:09:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:09:02 INFO DAGScheduler: Got job 5 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:09:02 INFO DAGScheduler: Final stage: Stage 5(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:09:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:09:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:09:02 INFO DAGScheduler: Submitting Stage 5 (MapPartitionsRDD[41] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:09:02 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=875687, maxMem=278302556
15/05/13 09:09:02 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 20.6 KB, free 264.6 MB)
15/05/13 09:09:02 INFO MemoryStore: ensureFreeSpace(11028) called with curMem=896783, maxMem=278302556
15/05/13 09:09:02 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.8 KB, free 264.5 MB)
15/05/13 09:09:02 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 265.3 MB)
15/05/13 09:09:02 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/05/13 09:09:02 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:839
15/05/13 09:09:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 5 (MapPartitionsRDD[41] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:09:02 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/05/13 09:09:02 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:09:03 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.8 KB, free: 265.4 MB)
15/05/13 09:09:14 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.4 MB)
15/05/13 09:09:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 14399 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:09:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/05/13 09:09:17 INFO DAGScheduler: Stage 5 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 14.403 s
15/05/13 09:09:17 INFO DAGScheduler: Job 5 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 14.441017 s
15/05/13 09:09:17 INFO JobScheduler: Finished job streaming job 1431522540000 ms.0 from job set of time 1431522540000 ms
15/05/13 09:09:17 INFO JobScheduler: Total delay: 17.194 s for time 1431522540000 ms (execution: 16.952 s)
15/05/13 09:09:17 INFO MapPartitionsRDD: Removing RDD 16 from persistence list
15/05/13 09:09:17 INFO BlockManager: Removing RDD 16
15/05/13 09:09:17 INFO UnionRDD: Removing RDD 15 from persistence list
15/05/13 09:09:17 INFO BlockManager: Removing RDD 15
15/05/13 09:09:17 INFO FileInputDStream: Cleared 1 old files that were older than 1431522480000 ms: 1431522420000 ms
15/05/13 09:09:17 INFO JobGenerator: Checkpointing graph for time 1431522540000 ms
15/05/13 09:09:17 INFO DStreamGraph: Updating checkpoint data for time 1431522540000 ms
15/05/13 09:09:17 INFO DStreamGraph: Updated checkpoint data for time 1431522540000 ms
15/05/13 09:09:17 INFO CheckpointWriter: Saving checkpoint for time 1431522540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000'
15/05/13 09:09:17 INFO CheckpointWriter: Checkpoint for time 1431522540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000', took 7655 bytes and 126 ms
15/05/13 09:09:17 INFO DStreamGraph: Clearing checkpoint data for time 1431522540000 ms
15/05/13 09:09:17 INFO DStreamGraph: Cleared checkpoint data for time 1431522540000 ms
15/05/13 09:09:17 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:10:00 INFO FileInputDStream: Finding new files took 41 ms
15/05/13 09:10:00 INFO FileInputDStream: New files at time 1431522600000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522524982.json
15/05/13 09:10:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=907811, maxMem=278302556
15/05/13 09:10:00 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 232.9 KB, free 264.3 MB)
15/05/13 09:10:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=1146343, maxMem=278302556
15/05/13 09:10:00 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.9 KB, free 264.3 MB)
15/05/13 09:10:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:10:00 INFO BlockManagerMaster: Updated info of block broadcast_9_piece0
15/05/13 09:10:00 INFO SparkContext: Created broadcast 9 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:10:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:10:00 INFO JobScheduler: Added jobs for time 1431522600000 ms
15/05/13 09:10:00 INFO JobGenerator: Checkpointing graph for time 1431522600000 ms
15/05/13 09:10:00 INFO JobScheduler: Starting job streaming job 1431522600000 ms.0 from job set of time 1431522600000 ms
15/05/13 09:10:00 INFO DStreamGraph: Updating checkpoint data for time 1431522600000 ms
15/05/13 09:10:00 INFO DStreamGraph: Updated checkpoint data for time 1431522600000 ms
15/05/13 09:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431522600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000'
15/05/13 09:10:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:10:00 INFO DAGScheduler: Got job 6 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:10:00 INFO DAGScheduler: Final stage: Stage 6(reduce at JsonRDD.scala:51)
15/05/13 09:10:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:10:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:10:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82676): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431522600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000'
15/05/13 09:10:00 INFO DAGScheduler: Submitting Stage 6 (MapPartitionsRDD[48] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:10:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=1182051, maxMem=278302556
15/05/13 09:10:00 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.9 KB, free 264.3 MB)
15/05/13 09:10:00 INFO MemoryStore: ensureFreeSpace(4231) called with curMem=1188091, maxMem=278302556
15/05/13 09:10:00 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.3 MB)
15/05/13 09:10:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:10:00 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/05/13 09:10:00 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:839
15/05/13 09:10:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 6 (MapPartitionsRDD[48] at map at JsonRDD.scala:51)
15/05/13 09:10:00 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/05/13 09:10:00 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:10:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522300000
15/05/13 09:10:00 INFO CheckpointWriter: Checkpoint for time 1431522600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000', took 7666 bytes and 166 ms
15/05/13 09:10:00 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:00 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 656 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:10:01 INFO DAGScheduler: Stage 6 (reduce at JsonRDD.scala:51) finished in 0.703 s
15/05/13 09:10:01 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/05/13 09:10:01 INFO DAGScheduler: Job 6 finished: reduce at JsonRDD.scala:51, took 0.756547 s
15/05/13 09:10:01 INFO BlockManager: Removing broadcast 10
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_10
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_10 of size 6040 dropped from memory (free 277116274)
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_10_piece0
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_10_piece0 of size 4231 dropped from memory (free 277120505)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_10_piece0
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO ContextCleaner: Cleaned broadcast 10
15/05/13 09:10:01 INFO BlockManager: Removing broadcast 8
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_8
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_8 of size 21096 dropped from memory (free 277141601)
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_8_piece0
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_8_piece0 of size 11028 dropped from memory (free 277152629)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_8_piece0
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_8_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.8 KB, free: 265.4 MB)
15/05/13 09:10:01 INFO ContextCleaner: Cleaned broadcast 8
15/05/13 09:10:01 INFO BlockManager: Removing broadcast 7
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_7
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_7 of size 6040 dropped from memory (free 277158669)
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_7_piece0
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_7_piece0 of size 4226 dropped from memory (free 277162895)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_7_piece0
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_7_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO ContextCleaner: Cleaned broadcast 7
15/05/13 09:10:01 INFO BlockManager: Removing broadcast 5
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_5_piece0
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_5_piece0 of size 11128 dropped from memory (free 277174023)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_5_piece0
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_5
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_5 of size 21304 dropped from memory (free 277195327)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_5_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO ContextCleaner: Cleaned broadcast 5
15/05/13 09:10:01 INFO BlockManager: Removing broadcast 4
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_4
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_4 of size 6040 dropped from memory (free 277201367)
15/05/13 09:10:01 INFO BlockManager: Removing block broadcast_4_piece0
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_4_piece0 of size 4227 dropped from memory (free 277205594)
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_4_piece0
15/05/13 09:10:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO ContextCleaner: Cleaned broadcast 4
15/05/13 09:10:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:10:01 INFO DAGScheduler: Got job 7 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:10:01 INFO DAGScheduler: Final stage: Stage 7(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:10:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:10:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:10:01 INFO DAGScheduler: Submitting Stage 7 (MapPartitionsRDD[55] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:10:01 INFO MemoryStore: ensureFreeSpace(18392) called with curMem=1096962, maxMem=278302556
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 18.0 KB, free 264.3 MB)
15/05/13 09:10:01 INFO MemoryStore: ensureFreeSpace(10041) called with curMem=1115354, maxMem=278302556
15/05/13 09:10:01 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 9.8 KB, free 264.3 MB)
15/05/13 09:10:01 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 9.8 KB, free: 265.3 MB)
15/05/13 09:10:01 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/05/13 09:10:01 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:839
15/05/13 09:10:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 7 (MapPartitionsRDD[55] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:10:01 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
15/05/13 09:10:01 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:10:01 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 9.8 KB, free: 265.4 MB)
15/05/13 09:10:02 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:10:02 INFO DAGScheduler: Stage 7 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.536 s
15/05/13 09:10:02 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 534 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:10:02 INFO DAGScheduler: Job 7 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.584733 s
15/05/13 09:10:02 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/05/13 09:10:02 INFO JobScheduler: Finished job streaming job 1431522600000 ms.0 from job set of time 1431522600000 ms
15/05/13 09:10:02 INFO MapPartitionsRDD: Removing RDD 30 from persistence list
15/05/13 09:10:02 INFO JobScheduler: Total delay: 2.527 s for time 1431522600000 ms (execution: 2.248 s)
15/05/13 09:10:02 INFO BlockManager: Removing RDD 30
15/05/13 09:10:02 INFO UnionRDD: Removing RDD 29 from persistence list
15/05/13 09:10:02 INFO BlockManager: Removing RDD 29
15/05/13 09:10:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431522540000 ms: 1431522480000 ms
15/05/13 09:10:02 INFO JobGenerator: Checkpointing graph for time 1431522600000 ms
15/05/13 09:10:02 INFO DStreamGraph: Updating checkpoint data for time 1431522600000 ms
15/05/13 09:10:02 INFO DStreamGraph: Updated checkpoint data for time 1431522600000 ms
15/05/13 09:10:02 INFO CheckpointWriter: Saving checkpoint for time 1431522600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000'
15/05/13 09:10:02 INFO CheckpointWriter: Checkpoint for time 1431522600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000', took 7657 bytes and 74 ms
15/05/13 09:10:02 INFO DStreamGraph: Clearing checkpoint data for time 1431522600000 ms
15/05/13 09:10:02 INFO DStreamGraph: Cleared checkpoint data for time 1431522600000 ms
15/05/13 09:10:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:11:00 INFO FileInputDStream: Finding new files took 42 ms
15/05/13 09:11:00 INFO FileInputDStream: New files at time 1431522660000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522585516.json
15/05/13 09:11:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=1125395, maxMem=278302556
15/05/13 09:11:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 232.9 KB, free 264.1 MB)
15/05/13 09:11:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=1363927, maxMem=278302556
15/05/13 09:11:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.9 KB, free 264.1 MB)
15/05/13 09:11:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:11:00 INFO BlockManagerMaster: Updated info of block broadcast_12_piece0
15/05/13 09:11:00 INFO SparkContext: Created broadcast 12 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:11:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:11:00 INFO JobScheduler: Added jobs for time 1431522660000 ms
15/05/13 09:11:00 INFO JobGenerator: Checkpointing graph for time 1431522660000 ms
15/05/13 09:11:00 INFO DStreamGraph: Updating checkpoint data for time 1431522660000 ms
15/05/13 09:11:00 INFO DStreamGraph: Updated checkpoint data for time 1431522660000 ms
15/05/13 09:11:00 INFO JobScheduler: Starting job streaming job 1431522660000 ms.0 from job set of time 1431522660000 ms
15/05/13 09:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431522660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000'
15/05/13 09:11:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82684): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431522660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000'
15/05/13 09:11:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:11:00 INFO DAGScheduler: Got job 8 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:11:00 INFO DAGScheduler: Final stage: Stage 8(reduce at JsonRDD.scala:51)
15/05/13 09:11:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:11:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:11:00 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[62] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:11:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=1399635, maxMem=278302556
15/05/13 09:11:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 5.9 KB, free 264.1 MB)
15/05/13 09:11:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=1405675, maxMem=278302556
15/05/13 09:11:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.1 KB, free 264.1 MB)
15/05/13 09:11:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:11:00 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/05/13 09:11:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:839
15/05/13 09:11:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 8 (MapPartitionsRDD[62] at map at JsonRDD.scala:51)
15/05/13 09:11:00 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
15/05/13 09:11:00 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:11:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82686): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431522660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000'
15/05/13 09:11:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:11:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522360000.bk
15/05/13 09:11:00 INFO CheckpointWriter: Checkpoint for time 1431522660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000', took 7667 bytes and 232 ms
15/05/13 09:11:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:11:01 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 822 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:11:01 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/05/13 09:11:01 INFO DAGScheduler: Stage 8 (reduce at JsonRDD.scala:51) finished in 0.866 s
15/05/13 09:11:01 INFO DAGScheduler: Job 8 finished: reduce at JsonRDD.scala:51, took 0.910915 s
15/05/13 09:11:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:11:01 INFO DAGScheduler: Got job 9 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:11:01 INFO DAGScheduler: Final stage: Stage 9(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:11:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:11:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:11:01 INFO DAGScheduler: Submitting Stage 9 (MapPartitionsRDD[69] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:11:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=1409903, maxMem=278302556
15/05/13 09:11:01 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 20.8 KB, free 264.0 MB)
15/05/13 09:11:01 INFO MemoryStore: ensureFreeSpace(11098) called with curMem=1431207, maxMem=278302556
15/05/13 09:11:01 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.8 KB, free 264.0 MB)
15/05/13 09:11:01 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 265.2 MB)
15/05/13 09:11:01 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/05/13 09:11:01 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:839
15/05/13 09:11:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 9 (MapPartitionsRDD[69] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:11:01 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/05/13 09:11:01 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:11:01 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 265.3 MB)
15/05/13 09:11:01 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 307 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:11:01 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/05/13 09:11:01 INFO DAGScheduler: Stage 9 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.309 s
15/05/13 09:11:01 INFO DAGScheduler: Job 9 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.352576 s
15/05/13 09:11:01 INFO JobScheduler: Finished job streaming job 1431522660000 ms.0 from job set of time 1431522660000 ms
15/05/13 09:11:01 INFO JobScheduler: Total delay: 1.949 s for time 1431522660000 ms (execution: 1.751 s)
15/05/13 09:11:01 INFO MapPartitionsRDD: Removing RDD 44 from persistence list
15/05/13 09:11:01 INFO BlockManager: Removing RDD 44
15/05/13 09:11:01 INFO UnionRDD: Removing RDD 43 from persistence list
15/05/13 09:11:01 INFO BlockManager: Removing RDD 43
15/05/13 09:11:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431522600000 ms: 1431522540000 ms
15/05/13 09:11:01 INFO JobGenerator: Checkpointing graph for time 1431522660000 ms
15/05/13 09:11:01 INFO DStreamGraph: Updating checkpoint data for time 1431522660000 ms
15/05/13 09:11:01 INFO DStreamGraph: Updated checkpoint data for time 1431522660000 ms
15/05/13 09:11:01 INFO CheckpointWriter: Saving checkpoint for time 1431522660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000'
15/05/13 09:11:02 INFO CheckpointWriter: Checkpoint for time 1431522660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000', took 7658 bytes and 63 ms
15/05/13 09:11:02 INFO DStreamGraph: Clearing checkpoint data for time 1431522660000 ms
15/05/13 09:11:02 INFO DStreamGraph: Cleared checkpoint data for time 1431522660000 ms
15/05/13 09:11:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:12:00 INFO FileInputDStream: Finding new files took 30 ms
15/05/13 09:12:00 INFO FileInputDStream: New files at time 1431522720000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522646838.json
15/05/13 09:12:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=1442305, maxMem=278302556
15/05/13 09:12:00 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 232.9 KB, free 263.8 MB)
15/05/13 09:12:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=1680837, maxMem=278302556
15/05/13 09:12:00 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 34.9 KB, free 263.8 MB)
15/05/13 09:12:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:12:00 INFO BlockManagerMaster: Updated info of block broadcast_15_piece0
15/05/13 09:12:00 INFO SparkContext: Created broadcast 15 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:12:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:12:00 INFO JobScheduler: Added jobs for time 1431522720000 ms
15/05/13 09:12:00 INFO JobGenerator: Checkpointing graph for time 1431522720000 ms
15/05/13 09:12:00 INFO JobScheduler: Starting job streaming job 1431522720000 ms.0 from job set of time 1431522720000 ms
15/05/13 09:12:00 INFO DStreamGraph: Updating checkpoint data for time 1431522720000 ms
15/05/13 09:12:00 INFO DStreamGraph: Updated checkpoint data for time 1431522720000 ms
15/05/13 09:12:00 INFO CheckpointWriter: Saving checkpoint for time 1431522720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522720000'
15/05/13 09:12:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:12:00 INFO DAGScheduler: Got job 10 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:12:00 INFO DAGScheduler: Final stage: Stage 10(reduce at JsonRDD.scala:51)
15/05/13 09:12:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:12:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:12:00 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD[76] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:12:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=1716545, maxMem=278302556
15/05/13 09:12:00 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 5.9 KB, free 263.8 MB)
15/05/13 09:12:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=1722585, maxMem=278302556
15/05/13 09:12:00 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.1 KB, free 263.8 MB)
15/05/13 09:12:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:12:00 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/05/13 09:12:00 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:839
15/05/13 09:12:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 10 (MapPartitionsRDD[76] at map at JsonRDD.scala:51)
15/05/13 09:12:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/05/13 09:12:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522420000
15/05/13 09:12:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:12:00 INFO CheckpointWriter: Checkpoint for time 1431522720000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522720000', took 7668 bytes and 120 ms
15/05/13 09:12:00 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.4 MB)
15/05/13 09:12:00 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:12:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 1867 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:12:02 INFO DAGScheduler: Stage 10 (reduce at JsonRDD.scala:51) finished in 1.903 s
15/05/13 09:12:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/05/13 09:12:02 INFO DAGScheduler: Job 10 finished: reduce at JsonRDD.scala:51, took 1.955244 s
15/05/13 09:12:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:12:02 INFO DAGScheduler: Got job 11 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:12:02 INFO DAGScheduler: Final stage: Stage 11(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:12:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:12:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:12:02 INFO DAGScheduler: Submitting Stage 11 (MapPartitionsRDD[83] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:12:02 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=1726811, maxMem=278302556
15/05/13 09:12:02 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 20.0 KB, free 263.7 MB)
15/05/13 09:12:02 INFO MemoryStore: ensureFreeSpace(10764) called with curMem=1747299, maxMem=278302556
15/05/13 09:12:02 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 10.5 KB, free 263.7 MB)
15/05/13 09:12:02 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 265.2 MB)
15/05/13 09:12:02 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/05/13 09:12:02 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:839
15/05/13 09:12:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 11 (MapPartitionsRDD[83] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:12:02 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/05/13 09:12:02 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:12:02 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 265.3 MB)
15/05/13 09:12:03 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 540 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:12:03 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/05/13 09:12:03 INFO DAGScheduler: Stage 11 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.545 s
15/05/13 09:12:03 INFO DAGScheduler: Job 11 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.600474 s
15/05/13 09:12:03 INFO JobScheduler: Finished job streaming job 1431522720000 ms.0 from job set of time 1431522720000 ms
15/05/13 09:12:03 INFO JobScheduler: Total delay: 3.126 s for time 1431522720000 ms (execution: 2.952 s)
15/05/13 09:12:03 INFO MapPartitionsRDD: Removing RDD 58 from persistence list
15/05/13 09:12:03 INFO BlockManager: Removing RDD 58
15/05/13 09:12:03 INFO UnionRDD: Removing RDD 57 from persistence list
15/05/13 09:12:03 INFO BlockManager: Removing RDD 57
15/05/13 09:12:03 INFO FileInputDStream: Cleared 1 old files that were older than 1431522660000 ms: 1431522600000 ms
15/05/13 09:12:03 INFO JobGenerator: Checkpointing graph for time 1431522720000 ms
15/05/13 09:12:03 INFO DStreamGraph: Updating checkpoint data for time 1431522720000 ms
15/05/13 09:12:03 INFO DStreamGraph: Updated checkpoint data for time 1431522720000 ms
15/05/13 09:12:03 INFO CheckpointWriter: Saving checkpoint for time 1431522720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522720000'
15/05/13 09:12:03 INFO CheckpointWriter: Checkpoint for time 1431522720000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522720000', took 7657 bytes and 59 ms
15/05/13 09:12:03 INFO DStreamGraph: Clearing checkpoint data for time 1431522720000 ms
15/05/13 09:12:03 INFO DStreamGraph: Cleared checkpoint data for time 1431522720000 ms
15/05/13 09:12:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:13:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 09:13:00 INFO FileInputDStream: New files at time 1431522780000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522707410.json
15/05/13 09:13:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=1758063, maxMem=278302556
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 232.9 KB, free 263.5 MB)
15/05/13 09:13:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=1996595, maxMem=278302556
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.9 KB, free 263.5 MB)
15/05/13 09:13:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/05/13 09:13:00 INFO SparkContext: Created broadcast 18 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:13:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:13:00 INFO JobScheduler: Added jobs for time 1431522780000 ms
15/05/13 09:13:00 INFO JobGenerator: Checkpointing graph for time 1431522780000 ms
15/05/13 09:13:00 INFO JobScheduler: Starting job streaming job 1431522780000 ms.0 from job set of time 1431522780000 ms
15/05/13 09:13:00 INFO DStreamGraph: Updating checkpoint data for time 1431522780000 ms
15/05/13 09:13:00 INFO DStreamGraph: Updated checkpoint data for time 1431522780000 ms
15/05/13 09:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431522780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000'
15/05/13 09:13:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82700): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431522780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000'
15/05/13 09:13:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:13:00 INFO DAGScheduler: Got job 12 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:13:00 INFO DAGScheduler: Final stage: Stage 12(reduce at JsonRDD.scala:51)
15/05/13 09:13:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:13:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:13:00 INFO DAGScheduler: Submitting Stage 12 (MapPartitionsRDD[90] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:13:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=2032303, maxMem=278302556
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 5.9 KB, free 263.5 MB)
15/05/13 09:13:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=2038343, maxMem=278302556
15/05/13 09:13:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82702): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431522780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000'
15/05/13 09:13:00 INFO BlockManager: Removing broadcast 11
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_11
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_11 of size 18392 dropped from memory (free 276282605)
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_11_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_11_piece0 of size 10041 dropped from memory (free 276292646)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 9.8 KB, free: 265.1 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_11_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.1 KB, free 263.5 MB)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_11_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 9.8 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/05/13 09:13:00 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:839
15/05/13 09:13:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 12 (MapPartitionsRDD[90] at map at JsonRDD.scala:51)
15/05/13 09:13:00 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
15/05/13 09:13:00 INFO ContextCleaner: Cleaned broadcast 11
15/05/13 09:13:00 INFO BlockManager: Removing broadcast 17
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_17_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_17_piece0 of size 10764 dropped from memory (free 276299182)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_17_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 265.1 MB)
15/05/13 09:13:00 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_17_piece0
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_17
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_17 of size 20488 dropped from memory (free 276319670)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_17_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO ContextCleaner: Cleaned broadcast 17
15/05/13 09:13:00 INFO BlockManager: Removing broadcast 16
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_16_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_16_piece0 of size 4226 dropped from memory (free 276323896)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_16_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_16_piece0
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_16
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_16 of size 6040 dropped from memory (free 276329936)
15/05/13 09:13:00 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_16_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522480000
15/05/13 09:13:00 INFO CheckpointWriter: Checkpoint for time 1431522780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000', took 7665 bytes and 305 ms
15/05/13 09:13:00 INFO ContextCleaner: Cleaned broadcast 16
15/05/13 09:13:00 INFO BlockManager: Removing broadcast 14
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_14_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_14_piece0 of size 11098 dropped from memory (free 276341034)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 265.2 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_14_piece0
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_14
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_14 of size 21304 dropped from memory (free 276362338)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_14_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO ContextCleaner: Cleaned broadcast 14
15/05/13 09:13:00 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO BlockManager: Removing broadcast 13
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_13_piece0
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_13_piece0 of size 4228 dropped from memory (free 276366566)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:13:00 INFO BlockManagerMaster: Updated info of block broadcast_13_piece0
15/05/13 09:13:00 INFO BlockManager: Removing block broadcast_13
15/05/13 09:13:00 INFO MemoryStore: Block broadcast_13 of size 6040 dropped from memory (free 276372606)
15/05/13 09:13:00 INFO BlockManagerInfo: Removed broadcast_13_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:13:00 INFO ContextCleaner: Cleaned broadcast 13
15/05/13 09:13:01 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 454 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:13:01 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/05/13 09:13:01 INFO DAGScheduler: Stage 12 (reduce at JsonRDD.scala:51) finished in 0.472 s
15/05/13 09:13:01 INFO DAGScheduler: Job 12 finished: reduce at JsonRDD.scala:51, took 0.637145 s
15/05/13 09:13:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:13:01 INFO DAGScheduler: Got job 13 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:13:01 INFO DAGScheduler: Final stage: Stage 13(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:13:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:13:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:13:01 INFO DAGScheduler: Submitting Stage 13 (MapPartitionsRDD[97] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:13:01 INFO MemoryStore: ensureFreeSpace(21320) called with curMem=1929950, maxMem=278302556
15/05/13 09:13:01 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 20.8 KB, free 263.5 MB)
15/05/13 09:13:01 INFO MemoryStore: ensureFreeSpace(11112) called with curMem=1951270, maxMem=278302556
15/05/13 09:13:01 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 10.9 KB, free 263.5 MB)
15/05/13 09:13:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 265.2 MB)
15/05/13 09:13:01 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/05/13 09:13:01 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:839
15/05/13 09:13:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 13 (MapPartitionsRDD[97] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:13:01 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
15/05/13 09:13:01 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:13:01 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:13:01 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 219 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:13:01 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/05/13 09:13:01 INFO DAGScheduler: Stage 13 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.220 s
15/05/13 09:13:01 INFO DAGScheduler: Job 13 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.262590 s
15/05/13 09:13:01 INFO JobScheduler: Finished job streaming job 1431522780000 ms.0 from job set of time 1431522780000 ms
15/05/13 09:13:01 INFO JobScheduler: Total delay: 1.668 s for time 1431522780000 ms (execution: 1.398 s)
15/05/13 09:13:01 INFO MapPartitionsRDD: Removing RDD 72 from persistence list
15/05/13 09:13:01 INFO BlockManager: Removing RDD 72
15/05/13 09:13:01 INFO UnionRDD: Removing RDD 71 from persistence list
15/05/13 09:13:01 INFO BlockManager: Removing RDD 71
15/05/13 09:13:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431522720000 ms: 1431522660000 ms
15/05/13 09:13:01 INFO JobGenerator: Checkpointing graph for time 1431522780000 ms
15/05/13 09:13:01 INFO DStreamGraph: Updating checkpoint data for time 1431522780000 ms
15/05/13 09:13:01 INFO DStreamGraph: Updated checkpoint data for time 1431522780000 ms
15/05/13 09:13:01 INFO CheckpointWriter: Saving checkpoint for time 1431522780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000'
15/05/13 09:13:01 INFO CheckpointWriter: Checkpoint for time 1431522780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522780000', took 7655 bytes and 63 ms
15/05/13 09:13:01 INFO DStreamGraph: Clearing checkpoint data for time 1431522780000 ms
15/05/13 09:13:01 INFO DStreamGraph: Cleared checkpoint data for time 1431522780000 ms
15/05/13 09:13:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:14:00 INFO FileInputDStream: Finding new files took 37 ms
15/05/13 09:14:00 INFO FileInputDStream: New files at time 1431522840000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522770789.json
15/05/13 09:14:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=1962382, maxMem=278302556
15/05/13 09:14:00 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 232.9 KB, free 263.3 MB)
15/05/13 09:14:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=2200914, maxMem=278302556
15/05/13 09:14:00 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 34.9 KB, free 263.3 MB)
15/05/13 09:14:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:14:00 INFO BlockManagerMaster: Updated info of block broadcast_21_piece0
15/05/13 09:14:00 INFO SparkContext: Created broadcast 21 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:14:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:14:00 INFO JobScheduler: Added jobs for time 1431522840000 ms
15/05/13 09:14:00 INFO JobGenerator: Checkpointing graph for time 1431522840000 ms
15/05/13 09:14:00 INFO JobScheduler: Starting job streaming job 1431522840000 ms.0 from job set of time 1431522840000 ms
15/05/13 09:14:00 INFO DStreamGraph: Updating checkpoint data for time 1431522840000 ms
15/05/13 09:14:00 INFO DStreamGraph: Updated checkpoint data for time 1431522840000 ms
15/05/13 09:14:00 INFO CheckpointWriter: Saving checkpoint for time 1431522840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000'
15/05/13 09:14:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:14:00 INFO DAGScheduler: Got job 14 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:14:00 INFO DAGScheduler: Final stage: Stage 14(reduce at JsonRDD.scala:51)
15/05/13 09:14:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:14:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:14:00 INFO DAGScheduler: Submitting Stage 14 (MapPartitionsRDD[104] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:14:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=2236622, maxMem=278302556
15/05/13 09:14:00 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 5.9 KB, free 263.3 MB)
15/05/13 09:14:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522540000
15/05/13 09:14:00 INFO CheckpointWriter: Checkpoint for time 1431522840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000', took 7662 bytes and 111 ms
15/05/13 09:14:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=2242662, maxMem=278302556
15/05/13 09:14:00 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.1 KB, free 263.3 MB)
15/05/13 09:14:00 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:14:00 INFO BlockManagerMaster: Updated info of block broadcast_22_piece0
15/05/13 09:14:00 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:839
15/05/13 09:14:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 14 (MapPartitionsRDD[104] at map at JsonRDD.scala:51)
15/05/13 09:14:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
15/05/13 09:14:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:14:00 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:14:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:14:01 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 698 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:14:01 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/05/13 09:14:01 INFO DAGScheduler: Stage 14 (reduce at JsonRDD.scala:51) finished in 0.729 s
15/05/13 09:14:01 INFO DAGScheduler: Job 14 finished: reduce at JsonRDD.scala:51, took 0.773748 s
15/05/13 09:14:01 INFO BlockManager: Removing broadcast 22
15/05/13 09:14:01 INFO BlockManager: Removing block broadcast_22_piece0
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_22_piece0 of size 4227 dropped from memory (free 276059894)
15/05/13 09:14:01 INFO BlockManagerInfo: Removed broadcast_22_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:14:01 INFO BlockManagerMaster: Updated info of block broadcast_22_piece0
15/05/13 09:14:01 INFO BlockManager: Removing block broadcast_22
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_22 of size 6040 dropped from memory (free 276065934)
15/05/13 09:14:01 INFO BlockManagerInfo: Removed broadcast_22_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:14:01 INFO ContextCleaner: Cleaned broadcast 22
15/05/13 09:14:01 INFO BlockManager: Removing broadcast 20
15/05/13 09:14:01 INFO BlockManager: Removing block broadcast_20
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_20 of size 21320 dropped from memory (free 276087254)
15/05/13 09:14:01 INFO BlockManager: Removing block broadcast_20_piece0
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_20_piece0 of size 11112 dropped from memory (free 276098366)
15/05/13 09:14:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 265.1 MB)
15/05/13 09:14:01 INFO BlockManagerMaster: Updated info of block broadcast_20_piece0
15/05/13 09:14:01 INFO BlockManagerInfo: Removed broadcast_20_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:14:01 INFO ContextCleaner: Cleaned broadcast 20
15/05/13 09:14:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:14:01 INFO DAGScheduler: Got job 15 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:14:01 INFO DAGScheduler: Final stage: Stage 15(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:14:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:14:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:14:01 INFO DAGScheduler: Submitting Stage 15 (MapPartitionsRDD[111] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:14:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=2204190, maxMem=278302556
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 20.4 KB, free 263.3 MB)
15/05/13 09:14:01 INFO MemoryStore: ensureFreeSpace(10895) called with curMem=2225110, maxMem=278302556
15/05/13 09:14:01 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.6 KB, free 263.3 MB)
15/05/13 09:14:01 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 265.1 MB)
15/05/13 09:14:01 INFO BlockManagerMaster: Updated info of block broadcast_23_piece0
15/05/13 09:14:01 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:839
15/05/13 09:14:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 15 (MapPartitionsRDD[111] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:14:01 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/05/13 09:14:01 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:14:01 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 265.3 MB)
15/05/13 09:14:01 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:14:02 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 435 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:14:02 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/05/13 09:14:02 INFO DAGScheduler: Stage 15 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.436 s
15/05/13 09:14:02 INFO DAGScheduler: Job 15 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.466982 s
15/05/13 09:14:02 INFO JobScheduler: Finished job streaming job 1431522840000 ms.0 from job set of time 1431522840000 ms
15/05/13 09:14:02 INFO JobScheduler: Total delay: 2.075 s for time 1431522840000 ms (execution: 1.751 s)
15/05/13 09:14:02 INFO MapPartitionsRDD: Removing RDD 86 from persistence list
15/05/13 09:14:02 INFO BlockManager: Removing RDD 86
15/05/13 09:14:02 INFO UnionRDD: Removing RDD 85 from persistence list
15/05/13 09:14:02 INFO BlockManager: Removing RDD 85
15/05/13 09:14:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431522780000 ms: 1431522720000 ms
15/05/13 09:14:02 INFO JobGenerator: Checkpointing graph for time 1431522840000 ms
15/05/13 09:14:02 INFO DStreamGraph: Updating checkpoint data for time 1431522840000 ms
15/05/13 09:14:02 INFO DStreamGraph: Updated checkpoint data for time 1431522840000 ms
15/05/13 09:14:02 INFO CheckpointWriter: Saving checkpoint for time 1431522840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000'
15/05/13 09:14:04 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:14:04 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:14:04 INFO CheckpointWriter: Saving checkpoint for time 1431522840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000'
15/05/13 09:14:04 INFO CheckpointWriter: Checkpoint for time 1431522840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000', took 7654 bytes and 2277 ms
15/05/13 09:14:04 INFO DStreamGraph: Clearing checkpoint data for time 1431522840000 ms
15/05/13 09:14:04 INFO DStreamGraph: Cleared checkpoint data for time 1431522840000 ms
15/05/13 09:14:04 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:15:00 INFO FileInputDStream: Finding new files took 39 ms
15/05/13 09:15:00 INFO FileInputDStream: New files at time 1431522900000 ms:

15/05/13 09:15:00 INFO JobScheduler: Added jobs for time 1431522900000 ms
15/05/13 09:15:00 INFO JobGenerator: Checkpointing graph for time 1431522900000 ms
15/05/13 09:15:00 INFO DStreamGraph: Updating checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 INFO JobScheduler: Starting job streaming job 1431522900000 ms.0 from job set of time 1431522900000 ms
15/05/13 09:15:00 INFO DStreamGraph: Updated checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:15:00 INFO DAGScheduler: Job 16 finished: reduce at JsonRDD.scala:51, took 0.000128 s
15/05/13 09:15:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82723): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 09:15:00 INFO JobScheduler: Finished job streaming job 1431522900000 ms.0 from job set of time 1431522900000 ms
15/05/13 09:15:00 INFO JobScheduler: Total delay: 0.180 s for time 1431522900000 ms (execution: 0.117 s)
15/05/13 09:15:00 INFO MapPartitionsRDD: Removing RDD 100 from persistence list
15/05/13 09:15:00 INFO BlockManager: Removing RDD 100
15/05/13 09:15:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82725): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:15:00 INFO UnionRDD: Removing RDD 99 from persistence list
15/05/13 09:15:00 INFO BlockManager: Removing RDD 99
15/05/13 09:15:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431522840000 ms: 1431522780000 ms
15/05/13 09:15:00 INFO JobGenerator: Checkpointing graph for time 1431522900000 ms
15/05/13 09:15:00 INFO DStreamGraph: Updating checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 INFO DStreamGraph: Updated checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82725): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82727): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:15:00 WARN CheckpointWriter: Could not write checkpoint for time 1431522900000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82729): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82731): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431522900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000'
15/05/13 09:15:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522600000.bk
15/05/13 09:15:00 INFO CheckpointWriter: Checkpoint for time 1431522900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522900000', took 7644 bytes and 185 ms
15/05/13 09:15:00 INFO DStreamGraph: Clearing checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 INFO DStreamGraph: Cleared checkpoint data for time 1431522900000 ms
15/05/13 09:15:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:16:00 INFO FileInputDStream: Finding new files took 47 ms
15/05/13 09:16:00 INFO FileInputDStream: New files at time 1431522960000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522839973.json
15/05/13 09:16:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=2236005, maxMem=278302556
15/05/13 09:16:00 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 232.9 KB, free 263.1 MB)
15/05/13 09:16:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=2474537, maxMem=278302556
15/05/13 09:16:00 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.9 KB, free 263.0 MB)
15/05/13 09:16:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:16:00 INFO BlockManagerMaster: Updated info of block broadcast_24_piece0
15/05/13 09:16:00 INFO SparkContext: Created broadcast 24 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:16:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:16:00 INFO JobScheduler: Starting job streaming job 1431522960000 ms.0 from job set of time 1431522960000 ms
15/05/13 09:16:00 INFO JobScheduler: Added jobs for time 1431522960000 ms
15/05/13 09:16:00 INFO JobGenerator: Checkpointing graph for time 1431522960000 ms
15/05/13 09:16:00 INFO DStreamGraph: Updating checkpoint data for time 1431522960000 ms
15/05/13 09:16:00 INFO DStreamGraph: Updated checkpoint data for time 1431522960000 ms
15/05/13 09:16:00 INFO CheckpointWriter: Saving checkpoint for time 1431522960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522960000'
15/05/13 09:16:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:16:00 INFO DAGScheduler: Got job 17 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:16:00 INFO DAGScheduler: Final stage: Stage 16(reduce at JsonRDD.scala:51)
15/05/13 09:16:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:16:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:16:00 INFO DAGScheduler: Submitting Stage 16 (MapPartitionsRDD[124] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:16:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=2510245, maxMem=278302556
15/05/13 09:16:00 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 5.9 KB, free 263.0 MB)
15/05/13 09:16:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=2516285, maxMem=278302556
15/05/13 09:16:00 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 4.1 KB, free 263.0 MB)
15/05/13 09:16:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:16:00 INFO BlockManagerMaster: Updated info of block broadcast_25_piece0
15/05/13 09:16:00 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:839
15/05/13 09:16:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 16 (MapPartitionsRDD[124] at map at JsonRDD.scala:51)
15/05/13 09:16:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522660000.bk
15/05/13 09:16:00 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
15/05/13 09:16:00 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:16:00 INFO CheckpointWriter: Checkpoint for time 1431522960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522960000', took 7638 bytes and 75 ms
15/05/13 09:16:00 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:16:00 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:16:01 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 948 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:16:01 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
15/05/13 09:16:01 INFO DAGScheduler: Stage 16 (reduce at JsonRDD.scala:51) finished in 0.978 s
15/05/13 09:16:01 INFO DAGScheduler: Job 17 finished: reduce at JsonRDD.scala:51, took 1.012336 s
15/05/13 09:16:01 INFO BlockManager: Removing broadcast 25
15/05/13 09:16:01 INFO BlockManager: Removing block broadcast_25_piece0
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_25_piece0 of size 4228 dropped from memory (free 275786271)
15/05/13 09:16:01 INFO BlockManagerInfo: Removed broadcast_25_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:16:01 INFO BlockManagerMaster: Updated info of block broadcast_25_piece0
15/05/13 09:16:01 INFO BlockManager: Removing block broadcast_25
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_25 of size 6040 dropped from memory (free 275792311)
15/05/13 09:16:01 INFO BlockManagerInfo: Removed broadcast_25_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:16:01 INFO ContextCleaner: Cleaned broadcast 25
15/05/13 09:16:01 INFO BlockManager: Removing broadcast 23
15/05/13 09:16:01 INFO BlockManager: Removing block broadcast_23_piece0
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_23_piece0 of size 10895 dropped from memory (free 275803206)
15/05/13 09:16:01 INFO BlockManagerInfo: Removed broadcast_23_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 265.1 MB)
15/05/13 09:16:01 INFO BlockManagerMaster: Updated info of block broadcast_23_piece0
15/05/13 09:16:01 INFO BlockManager: Removing block broadcast_23
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_23 of size 20920 dropped from memory (free 275824126)
15/05/13 09:16:01 INFO BlockManagerInfo: Removed broadcast_23_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 265.3 MB)
15/05/13 09:16:01 INFO ContextCleaner: Cleaned broadcast 23
15/05/13 09:16:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:16:01 INFO DAGScheduler: Got job 18 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:16:01 INFO DAGScheduler: Final stage: Stage 17(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:16:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:16:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:16:01 INFO DAGScheduler: Submitting Stage 17 (MapPartitionsRDD[131] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:16:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=2478430, maxMem=278302556
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 20.4 KB, free 263.0 MB)
15/05/13 09:16:01 INFO MemoryStore: ensureFreeSpace(10896) called with curMem=2499350, maxMem=278302556
15/05/13 09:16:01 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.6 KB, free 263.0 MB)
15/05/13 09:16:01 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 265.1 MB)
15/05/13 09:16:01 INFO BlockManagerMaster: Updated info of block broadcast_26_piece0
15/05/13 09:16:01 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:839
15/05/13 09:16:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 17 (MapPartitionsRDD[131] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:16:01 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
15/05/13 09:16:01 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:16:01 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 265.3 MB)
15/05/13 09:16:02 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 346 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:16:02 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
15/05/13 09:16:02 INFO DAGScheduler: Stage 17 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.348 s
15/05/13 09:16:02 INFO DAGScheduler: Job 18 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.379252 s
15/05/13 09:16:02 INFO JobScheduler: Finished job streaming job 1431522960000 ms.0 from job set of time 1431522960000 ms
15/05/13 09:16:02 INFO JobScheduler: Total delay: 2.158 s for time 1431522960000 ms (execution: 1.903 s)
15/05/13 09:16:02 INFO MapPartitionsRDD: Removing RDD 113 from persistence list
15/05/13 09:16:02 INFO BlockManager: Removing RDD 113
15/05/13 09:16:02 INFO UnionRDD: Removing RDD 112 from persistence list
15/05/13 09:16:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431522900000 ms: 1431522840000 ms
15/05/13 09:16:02 INFO BlockManager: Removing RDD 112
15/05/13 09:16:02 INFO JobGenerator: Checkpointing graph for time 1431522960000 ms
15/05/13 09:16:02 INFO DStreamGraph: Updating checkpoint data for time 1431522960000 ms
15/05/13 09:16:02 INFO DStreamGraph: Updated checkpoint data for time 1431522960000 ms
15/05/13 09:16:02 INFO CheckpointWriter: Saving checkpoint for time 1431522960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522960000'
15/05/13 09:16:02 INFO CheckpointWriter: Checkpoint for time 1431522960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522960000', took 7643 bytes and 76 ms
15/05/13 09:16:02 INFO DStreamGraph: Clearing checkpoint data for time 1431522960000 ms
15/05/13 09:16:02 INFO DStreamGraph: Cleared checkpoint data for time 1431522960000 ms
15/05/13 09:16:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:17:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 09:17:00 INFO FileInputDStream: New files at time 1431523020000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522901272.json
15/05/13 09:17:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=2510246, maxMem=278302556
15/05/13 09:17:00 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 232.9 KB, free 262.8 MB)
15/05/13 09:17:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=2748778, maxMem=278302556
15/05/13 09:17:00 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 34.9 KB, free 262.8 MB)
15/05/13 09:17:00 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:17:00 INFO BlockManagerMaster: Updated info of block broadcast_27_piece0
15/05/13 09:17:00 INFO SparkContext: Created broadcast 27 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:17:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:17:00 INFO JobScheduler: Starting job streaming job 1431523020000 ms.0 from job set of time 1431523020000 ms
15/05/13 09:17:00 INFO JobScheduler: Added jobs for time 1431523020000 ms
15/05/13 09:17:00 INFO JobGenerator: Checkpointing graph for time 1431523020000 ms
15/05/13 09:17:00 INFO DStreamGraph: Updating checkpoint data for time 1431523020000 ms
15/05/13 09:17:00 INFO DStreamGraph: Updated checkpoint data for time 1431523020000 ms
15/05/13 09:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431523020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523020000'
15/05/13 09:17:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522720000.bk
15/05/13 09:17:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:17:00 INFO CheckpointWriter: Checkpoint for time 1431523020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523020000', took 7645 bytes and 76 ms
15/05/13 09:17:00 INFO DAGScheduler: Got job 19 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:17:00 INFO DAGScheduler: Final stage: Stage 18(reduce at JsonRDD.scala:51)
15/05/13 09:17:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:17:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:17:00 INFO DAGScheduler: Submitting Stage 18 (MapPartitionsRDD[138] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:17:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=2784488, maxMem=278302556
15/05/13 09:17:00 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.9 KB, free 262.7 MB)
15/05/13 09:17:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=2790528, maxMem=278302556
15/05/13 09:17:00 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.1 KB, free 262.7 MB)
15/05/13 09:17:00 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:17:00 INFO BlockManagerMaster: Updated info of block broadcast_28_piece0
15/05/13 09:17:00 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:839
15/05/13 09:17:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 18 (MapPartitionsRDD[138] at map at JsonRDD.scala:51)
15/05/13 09:17:00 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
15/05/13 09:17:00 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:17:00 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:17:00 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:17:01 INFO DAGScheduler: Stage 18 (reduce at JsonRDD.scala:51) finished in 0.935 s
15/05/13 09:17:01 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 907 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:17:01 INFO DAGScheduler: Job 19 finished: reduce at JsonRDD.scala:51, took 0.987874 s
15/05/13 09:17:01 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
15/05/13 09:17:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:17:01 INFO DAGScheduler: Got job 20 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:17:01 INFO DAGScheduler: Final stage: Stage 19(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:17:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:17:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:17:01 INFO DAGScheduler: Submitting Stage 19 (MapPartitionsRDD[145] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:17:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=2794755, maxMem=278302556
15/05/13 09:17:01 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 20.6 KB, free 262.7 MB)
15/05/13 09:17:01 INFO MemoryStore: ensureFreeSpace(10957) called with curMem=2815851, maxMem=278302556
15/05/13 09:17:01 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 10.7 KB, free 262.7 MB)
15/05/13 09:17:01 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:17:01 INFO BlockManagerMaster: Updated info of block broadcast_29_piece0
15/05/13 09:17:01 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:839
15/05/13 09:17:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 19 (MapPartitionsRDD[145] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:17:01 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
15/05/13 09:17:01 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:17:01 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 265.2 MB)
15/05/13 09:17:01 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 172 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:17:01 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
15/05/13 09:17:01 INFO DAGScheduler: Stage 19 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.174 s
15/05/13 09:17:01 INFO DAGScheduler: Job 20 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.209967 s
15/05/13 09:17:01 INFO JobScheduler: Finished job streaming job 1431523020000 ms.0 from job set of time 1431523020000 ms
15/05/13 09:17:01 INFO JobScheduler: Total delay: 1.952 s for time 1431523020000 ms (execution: 1.700 s)
15/05/13 09:17:01 INFO MapPartitionsRDD: Removing RDD 120 from persistence list
15/05/13 09:17:01 INFO BlockManager: Removing RDD 120
15/05/13 09:17:01 INFO UnionRDD: Removing RDD 119 from persistence list
15/05/13 09:17:01 INFO BlockManager: Removing RDD 119
15/05/13 09:17:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431522960000 ms: 1431522900000 ms
15/05/13 09:17:01 INFO JobGenerator: Checkpointing graph for time 1431523020000 ms
15/05/13 09:17:01 INFO DStreamGraph: Updating checkpoint data for time 1431523020000 ms
15/05/13 09:17:01 INFO DStreamGraph: Updated checkpoint data for time 1431523020000 ms
15/05/13 09:17:01 INFO CheckpointWriter: Saving checkpoint for time 1431523020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523020000'
15/05/13 09:17:02 INFO CheckpointWriter: Checkpoint for time 1431523020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523020000', took 7657 bytes and 71 ms
15/05/13 09:17:02 INFO DStreamGraph: Clearing checkpoint data for time 1431523020000 ms
15/05/13 09:17:02 INFO DStreamGraph: Cleared checkpoint data for time 1431523020000 ms
15/05/13 09:17:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:18:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 09:18:00 INFO FileInputDStream: New files at time 1431523080000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431522966593.json
15/05/13 09:18:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=2826808, maxMem=278302556
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 232.9 KB, free 262.5 MB)
15/05/13 09:18:00 INFO BlockManager: Removing broadcast 29
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_29_piece0
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_29_piece0 of size 10957 dropped from memory (free 275248173)
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_29_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:18:00 INFO BlockManagerMaster: Updated info of block broadcast_29_piece0
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_29
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_29_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 265.2 MB)
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_29 of size 21096 dropped from memory (free 275269269)
15/05/13 09:18:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=3033287, maxMem=278302556
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.9 KB, free 262.5 MB)
15/05/13 09:18:00 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:18:00 INFO BlockManagerMaster: Updated info of block broadcast_30_piece0
15/05/13 09:18:00 INFO SparkContext: Created broadcast 30 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:18:00 INFO ContextCleaner: Cleaned broadcast 29
15/05/13 09:18:00 INFO BlockManager: Removing broadcast 28
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_28
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_28 of size 6040 dropped from memory (free 275239601)
15/05/13 09:18:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_28_piece0
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_28_piece0 of size 4227 dropped from memory (free 275243828)
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_28_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:18:00 INFO BlockManagerMaster: Updated info of block broadcast_28_piece0
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_28_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:18:00 INFO ContextCleaner: Cleaned broadcast 28
15/05/13 09:18:00 INFO BlockManager: Removing broadcast 26
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_26
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_26 of size 20920 dropped from memory (free 275264748)
15/05/13 09:18:00 INFO BlockManager: Removing block broadcast_26_piece0
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_26_piece0 of size 10896 dropped from memory (free 275275644)
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_26_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 265.0 MB)
15/05/13 09:18:00 INFO JobScheduler: Added jobs for time 1431523080000 ms
15/05/13 09:18:00 INFO BlockManagerMaster: Updated info of block broadcast_26_piece0
15/05/13 09:18:00 INFO JobScheduler: Starting job streaming job 1431523080000 ms.0 from job set of time 1431523080000 ms
15/05/13 09:18:00 INFO JobGenerator: Checkpointing graph for time 1431523080000 ms
15/05/13 09:18:00 INFO DStreamGraph: Updating checkpoint data for time 1431523080000 ms
15/05/13 09:18:00 INFO DStreamGraph: Updated checkpoint data for time 1431523080000 ms
15/05/13 09:18:00 INFO BlockManagerInfo: Removed broadcast_26_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 265.3 MB)
15/05/13 09:18:00 INFO ContextCleaner: Cleaned broadcast 26
15/05/13 09:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431523080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000'
15/05/13 09:18:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:18:00 INFO DAGScheduler: Got job 21 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:18:00 INFO DAGScheduler: Final stage: Stage 20(reduce at JsonRDD.scala:51)
15/05/13 09:18:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:18:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:18:00 INFO DAGScheduler: Submitting Stage 20 (MapPartitionsRDD[152] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:18:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=3026912, maxMem=278302556
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.9 KB, free 262.5 MB)
15/05/13 09:18:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=3032952, maxMem=278302556
15/05/13 09:18:00 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 4.1 KB, free 262.5 MB)
15/05/13 09:18:00 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:18:00 INFO BlockManagerMaster: Updated info of block broadcast_31_piece0
15/05/13 09:18:00 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:839
15/05/13 09:18:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 20 (MapPartitionsRDD[152] at map at JsonRDD.scala:51)
15/05/13 09:18:00 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
15/05/13 09:18:00 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:18:00 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:18:00 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:18:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82748): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:18:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82748): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:18:01 INFO CheckpointWriter: Saving checkpoint for time 1431523080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000'
15/05/13 09:18:01 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 734 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:18:01 INFO DAGScheduler: Stage 20 (reduce at JsonRDD.scala:51) finished in 0.760 s
15/05/13 09:18:01 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
15/05/13 09:18:01 INFO DAGScheduler: Job 21 finished: reduce at JsonRDD.scala:51, took 0.804128 s
15/05/13 09:18:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:18:01 INFO DAGScheduler: Got job 22 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:18:01 INFO DAGScheduler: Final stage: Stage 21(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:18:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:18:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:18:01 INFO DAGScheduler: Submitting Stage 21 (MapPartitionsRDD[159] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:18:01 INFO MemoryStore: ensureFreeSpace(22064) called with curMem=3037178, maxMem=278302556
15/05/13 09:18:01 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 21.5 KB, free 262.5 MB)
15/05/13 09:18:01 INFO MemoryStore: ensureFreeSpace(11390) called with curMem=3059242, maxMem=278302556
15/05/13 09:18:01 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 11.1 KB, free 262.5 MB)
15/05/13 09:18:01 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.1 KB, free: 265.0 MB)
15/05/13 09:18:01 INFO BlockManagerMaster: Updated info of block broadcast_32_piece0
15/05/13 09:18:01 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:839
15/05/13 09:18:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 21 (MapPartitionsRDD[159] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:18:01 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
15/05/13 09:18:01 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:18:01 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 11.1 KB, free: 265.2 MB)
15/05/13 09:18:03 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82749): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:18:03 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82749): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:18:03 INFO CheckpointWriter: Saving checkpoint for time 1431523080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000'
15/05/13 09:18:03 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82751): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:18:03 WARN CheckpointWriter: Could not write checkpoint for time 1431523080000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000'
15/05/13 09:18:03 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 2042 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:18:03 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
15/05/13 09:18:03 INFO DAGScheduler: Stage 21 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 2.043 s
15/05/13 09:18:03 INFO DAGScheduler: Job 22 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 2.101320 s
15/05/13 09:18:03 INFO JobScheduler: Finished job streaming job 1431523080000 ms.0 from job set of time 1431523080000 ms
15/05/13 09:18:03 INFO JobScheduler: Total delay: 3.832 s for time 1431523080000 ms (execution: 3.355 s)
15/05/13 09:18:03 INFO MapPartitionsRDD: Removing RDD 134 from persistence list
15/05/13 09:18:03 INFO BlockManager: Removing RDD 134
15/05/13 09:18:03 INFO UnionRDD: Removing RDD 133 from persistence list
15/05/13 09:18:03 INFO BlockManager: Removing RDD 133
15/05/13 09:18:03 INFO FileInputDStream: Cleared 1 old files that were older than 1431523020000 ms: 1431522960000 ms
15/05/13 09:18:03 INFO JobGenerator: Checkpointing graph for time 1431523080000 ms
15/05/13 09:18:03 INFO DStreamGraph: Updating checkpoint data for time 1431523080000 ms
15/05/13 09:18:03 INFO DStreamGraph: Updated checkpoint data for time 1431523080000 ms
15/05/13 09:18:03 INFO CheckpointWriter: Saving checkpoint for time 1431523080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000'
15/05/13 09:18:03 INFO CheckpointWriter: Checkpoint for time 1431523080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523080000', took 7654 bytes and 83 ms
15/05/13 09:18:03 INFO DStreamGraph: Clearing checkpoint data for time 1431523080000 ms
15/05/13 09:18:03 INFO DStreamGraph: Cleared checkpoint data for time 1431523080000 ms
15/05/13 09:18:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:19:00 INFO FileInputDStream: Finding new files took 48 ms
15/05/13 09:19:00 INFO FileInputDStream: New files at time 1431523140000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523031506.json
15/05/13 09:19:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=3070632, maxMem=278302556
15/05/13 09:19:00 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 232.9 KB, free 262.3 MB)
15/05/13 09:19:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=3309164, maxMem=278302556
15/05/13 09:19:00 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.9 KB, free 262.2 MB)
15/05/13 09:19:00 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:19:00 INFO BlockManagerMaster: Updated info of block broadcast_33_piece0
15/05/13 09:19:00 INFO SparkContext: Created broadcast 33 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:19:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:19:00 INFO JobScheduler: Added jobs for time 1431523140000 ms
15/05/13 09:19:00 INFO JobGenerator: Checkpointing graph for time 1431523140000 ms
15/05/13 09:19:00 INFO JobScheduler: Starting job streaming job 1431523140000 ms.0 from job set of time 1431523140000 ms
15/05/13 09:19:00 INFO DStreamGraph: Updating checkpoint data for time 1431523140000 ms
15/05/13 09:19:00 INFO DStreamGraph: Updated checkpoint data for time 1431523140000 ms
15/05/13 09:19:00 INFO CheckpointWriter: Saving checkpoint for time 1431523140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000'
15/05/13 09:19:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:19:00 INFO DAGScheduler: Got job 23 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:19:00 INFO DAGScheduler: Final stage: Stage 22(reduce at JsonRDD.scala:51)
15/05/13 09:19:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:19:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:19:00 INFO DAGScheduler: Submitting Stage 22 (MapPartitionsRDD[166] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:19:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=3344872, maxMem=278302556
15/05/13 09:19:00 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 5.9 KB, free 262.2 MB)
15/05/13 09:19:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=3350912, maxMem=278302556
15/05/13 09:19:00 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 4.1 KB, free 262.2 MB)
15/05/13 09:19:00 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:19:00 INFO BlockManagerMaster: Updated info of block broadcast_34_piece0
15/05/13 09:19:00 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:839
15/05/13 09:19:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 22 (MapPartitionsRDD[166] at map at JsonRDD.scala:51)
15/05/13 09:19:00 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
15/05/13 09:19:00 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:19:00 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:19:00 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:19:01 INFO DAGScheduler: Stage 22 (reduce at JsonRDD.scala:51) finished in 0.785 s
15/05/13 09:19:01 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 767 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:19:01 INFO DAGScheduler: Job 23 finished: reduce at JsonRDD.scala:51, took 0.824723 s
15/05/13 09:19:01 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
15/05/13 09:19:01 INFO BlockManager: Removing broadcast 32
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_32
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_32 of size 22064 dropped from memory (free 274969482)
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_32_piece0
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_32_piece0 of size 11390 dropped from memory (free 274980872)
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_32_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.1 KB, free: 265.0 MB)
15/05/13 09:19:01 INFO BlockManagerMaster: Updated info of block broadcast_32_piece0
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_32_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 11.1 KB, free: 265.2 MB)
15/05/13 09:19:01 INFO ContextCleaner: Cleaned broadcast 32
15/05/13 09:19:01 INFO BlockManager: Removing broadcast 34
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_34_piece0
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_34_piece0 of size 4226 dropped from memory (free 274985098)
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_34_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:19:01 INFO BlockManagerMaster: Updated info of block broadcast_34_piece0
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_34
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_34 of size 6040 dropped from memory (free 274991138)
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_34_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:19:01 INFO ContextCleaner: Cleaned broadcast 34
15/05/13 09:19:01 INFO BlockManager: Removing broadcast 31
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_31
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_31 of size 6040 dropped from memory (free 274997178)
15/05/13 09:19:01 INFO BlockManager: Removing block broadcast_31_piece0
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_31_piece0 of size 4226 dropped from memory (free 275001404)
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_31_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:19:01 INFO BlockManagerMaster: Updated info of block broadcast_31_piece0
15/05/13 09:19:01 INFO BlockManagerInfo: Removed broadcast_31_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:19:01 INFO ContextCleaner: Cleaned broadcast 31
15/05/13 09:19:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:19:01 INFO DAGScheduler: Got job 24 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:19:01 INFO DAGScheduler: Final stage: Stage 23(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:19:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:19:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:19:01 INFO DAGScheduler: Submitting Stage 23 (MapPartitionsRDD[173] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:19:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82758): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:19:01 INFO CheckpointWriter: Saving checkpoint for time 1431523140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000'
15/05/13 09:19:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=3301152, maxMem=278302556
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 20.0 KB, free 262.2 MB)
15/05/13 09:19:01 INFO MemoryStore: ensureFreeSpace(10756) called with curMem=3321640, maxMem=278302556
15/05/13 09:19:01 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 10.5 KB, free 262.2 MB)
15/05/13 09:19:01 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 265.0 MB)
15/05/13 09:19:01 INFO BlockManagerMaster: Updated info of block broadcast_35_piece0
15/05/13 09:19:01 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:839
15/05/13 09:19:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 23 (MapPartitionsRDD[173] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:19:01 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
15/05/13 09:19:01 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:19:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82760): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:19:01 INFO CheckpointWriter: Saving checkpoint for time 1431523140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000'
15/05/13 09:19:01 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82762): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:19:01 WARN CheckpointWriter: Could not write checkpoint for time 1431523140000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000'
15/05/13 09:19:01 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 265.2 MB)
15/05/13 09:19:02 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:19:02 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 636 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:19:02 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
15/05/13 09:19:02 INFO DAGScheduler: Stage 23 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.638 s
15/05/13 09:19:02 INFO DAGScheduler: Job 24 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.670549 s
15/05/13 09:19:02 INFO JobScheduler: Finished job streaming job 1431523140000 ms.0 from job set of time 1431523140000 ms
15/05/13 09:19:02 INFO MapPartitionsRDD: Removing RDD 148 from persistence list
15/05/13 09:19:02 INFO JobScheduler: Total delay: 2.550 s for time 1431523140000 ms (execution: 2.250 s)
15/05/13 09:19:02 INFO BlockManager: Removing RDD 148
15/05/13 09:19:02 INFO UnionRDD: Removing RDD 147 from persistence list
15/05/13 09:19:02 INFO BlockManager: Removing RDD 147
15/05/13 09:19:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431523080000 ms: 1431523020000 ms
15/05/13 09:19:02 INFO JobGenerator: Checkpointing graph for time 1431523140000 ms
15/05/13 09:19:02 INFO DStreamGraph: Updating checkpoint data for time 1431523140000 ms
15/05/13 09:19:02 INFO DStreamGraph: Updated checkpoint data for time 1431523140000 ms
15/05/13 09:19:02 INFO CheckpointWriter: Saving checkpoint for time 1431523140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000'
15/05/13 09:19:02 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522840000.bk
15/05/13 09:19:02 INFO CheckpointWriter: Checkpoint for time 1431523140000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000', took 7658 bytes and 74 ms
15/05/13 09:19:02 INFO DStreamGraph: Clearing checkpoint data for time 1431523140000 ms
15/05/13 09:19:02 INFO DStreamGraph: Cleared checkpoint data for time 1431523140000 ms
15/05/13 09:19:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:20:00 INFO FileInputDStream: Finding new files took 37 ms
15/05/13 09:20:00 INFO FileInputDStream: New files at time 1431523200000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523093827.json
15/05/13 09:20:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=3332396, maxMem=278302556
15/05/13 09:20:00 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 232.9 KB, free 262.0 MB)
15/05/13 09:20:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=3570928, maxMem=278302556
15/05/13 09:20:00 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.9 KB, free 262.0 MB)
15/05/13 09:20:00 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:20:00 INFO BlockManagerMaster: Updated info of block broadcast_36_piece0
15/05/13 09:20:00 INFO SparkContext: Created broadcast 36 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:20:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:20:00 INFO JobScheduler: Added jobs for time 1431523200000 ms
15/05/13 09:20:00 INFO JobScheduler: Starting job streaming job 1431523200000 ms.0 from job set of time 1431523200000 ms
15/05/13 09:20:00 INFO JobGenerator: Checkpointing graph for time 1431523200000 ms
15/05/13 09:20:00 INFO DStreamGraph: Updating checkpoint data for time 1431523200000 ms
15/05/13 09:20:00 INFO DStreamGraph: Updated checkpoint data for time 1431523200000 ms
15/05/13 09:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431523200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000'
15/05/13 09:20:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82773): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:20:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82773): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431523200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000'
15/05/13 09:20:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82775): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:20:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82775): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431523200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000'
15/05/13 09:20:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:20:00 INFO DAGScheduler: Got job 25 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:20:00 INFO DAGScheduler: Final stage: Stage 24(reduce at JsonRDD.scala:51)
15/05/13 09:20:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:20:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:20:00 INFO DAGScheduler: Submitting Stage 24 (MapPartitionsRDD[180] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:20:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=3606636, maxMem=278302556
15/05/13 09:20:00 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 5.9 KB, free 262.0 MB)
15/05/13 09:20:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=3612676, maxMem=278302556
15/05/13 09:20:00 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 4.1 KB, free 262.0 MB)
15/05/13 09:20:00 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:20:00 INFO BlockManagerMaster: Updated info of block broadcast_37_piece0
15/05/13 09:20:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82777): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:20:00 WARN CheckpointWriter: Could not write checkpoint for time 1431523200000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000'
15/05/13 09:20:00 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:839
15/05/13 09:20:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 24 (MapPartitionsRDD[180] at map at JsonRDD.scala:51)
15/05/13 09:20:00 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
15/05/13 09:20:00 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:20:00 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:20:00 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.3 MB)
15/05/13 09:20:00 INFO DAGScheduler: Stage 24 (reduce at JsonRDD.scala:51) finished in 0.568 s
15/05/13 09:20:00 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 24) in 551 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:20:00 INFO DAGScheduler: Job 25 finished: reduce at JsonRDD.scala:51, took 0.625898 s
15/05/13 09:20:00 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
15/05/13 09:20:01 INFO BlockManager: Removing broadcast 35
15/05/13 09:20:01 INFO BlockManager: Removing block broadcast_35
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_35 of size 20488 dropped from memory (free 274706142)
15/05/13 09:20:01 INFO BlockManager: Removing block broadcast_35_piece0
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_35_piece0 of size 10756 dropped from memory (free 274716898)
15/05/13 09:20:01 INFO BlockManagerInfo: Removed broadcast_35_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 265.0 MB)
15/05/13 09:20:01 INFO BlockManagerMaster: Updated info of block broadcast_35_piece0
15/05/13 09:20:01 INFO BlockManagerInfo: Removed broadcast_35_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 265.2 MB)
15/05/13 09:20:01 INFO ContextCleaner: Cleaned broadcast 35
15/05/13 09:20:01 INFO BlockManager: Removing broadcast 37
15/05/13 09:20:01 INFO BlockManager: Removing block broadcast_37_piece0
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_37_piece0 of size 4226 dropped from memory (free 274721124)
15/05/13 09:20:01 INFO BlockManagerInfo: Removed broadcast_37_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:20:01 INFO BlockManagerMaster: Updated info of block broadcast_37_piece0
15/05/13 09:20:01 INFO BlockManager: Removing block broadcast_37
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_37 of size 6040 dropped from memory (free 274727164)
15/05/13 09:20:01 INFO BlockManagerInfo: Removed broadcast_37_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:20:01 INFO ContextCleaner: Cleaned broadcast 37
15/05/13 09:20:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:20:01 INFO DAGScheduler: Got job 26 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:20:01 INFO DAGScheduler: Final stage: Stage 25(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:20:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:20:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:20:01 INFO DAGScheduler: Submitting Stage 25 (MapPartitionsRDD[187] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:20:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=3575392, maxMem=278302556
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 20.8 KB, free 262.0 MB)
15/05/13 09:20:01 INFO MemoryStore: ensureFreeSpace(11118) called with curMem=3596696, maxMem=278302556
15/05/13 09:20:01 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 10.9 KB, free 262.0 MB)
15/05/13 09:20:01 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 265.0 MB)
15/05/13 09:20:01 INFO BlockManagerMaster: Updated info of block broadcast_38_piece0
15/05/13 09:20:01 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:839
15/05/13 09:20:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 25 (MapPartitionsRDD[187] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:20:01 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
15/05/13 09:20:01 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:20:01 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 265.3 MB)
15/05/13 09:20:01 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 153 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:20:01 INFO DAGScheduler: Stage 25 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.155 s
15/05/13 09:20:01 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
15/05/13 09:20:01 INFO DAGScheduler: Job 26 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.195736 s
15/05/13 09:20:01 INFO JobScheduler: Finished job streaming job 1431523200000 ms.0 from job set of time 1431523200000 ms
15/05/13 09:20:01 INFO JobScheduler: Total delay: 1.525 s for time 1431523200000 ms (execution: 1.311 s)
15/05/13 09:20:01 INFO MapPartitionsRDD: Removing RDD 162 from persistence list
15/05/13 09:20:01 INFO BlockManager: Removing RDD 162
15/05/13 09:20:01 INFO UnionRDD: Removing RDD 161 from persistence list
15/05/13 09:20:01 INFO BlockManager: Removing RDD 161
15/05/13 09:20:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523140000 ms: 1431523080000 ms
15/05/13 09:20:01 INFO JobGenerator: Checkpointing graph for time 1431523200000 ms
15/05/13 09:20:01 INFO DStreamGraph: Updating checkpoint data for time 1431523200000 ms
15/05/13 09:20:01 INFO DStreamGraph: Updated checkpoint data for time 1431523200000 ms
15/05/13 09:20:01 INFO CheckpointWriter: Saving checkpoint for time 1431523200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000'
15/05/13 09:20:01 INFO CheckpointWriter: Checkpoint for time 1431523200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523200000', took 7654 bytes and 59 ms
15/05/13 09:20:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523200000 ms
15/05/13 09:20:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523200000 ms
15/05/13 09:20:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:21:00 INFO FileInputDStream: Finding new files took 64 ms
15/05/13 09:21:00 INFO FileInputDStream: New files at time 1431523260000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523156793.json
15/05/13 09:21:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=3607814, maxMem=278302556
15/05/13 09:21:00 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 232.9 KB, free 261.7 MB)
15/05/13 09:21:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=3846346, maxMem=278302556
15/05/13 09:21:00 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.9 KB, free 261.7 MB)
15/05/13 09:21:00 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:21:00 INFO BlockManagerMaster: Updated info of block broadcast_39_piece0
15/05/13 09:21:00 INFO SparkContext: Created broadcast 39 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:21:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:21:00 INFO JobScheduler: Added jobs for time 1431523260000 ms
15/05/13 09:21:00 INFO JobGenerator: Checkpointing graph for time 1431523260000 ms
15/05/13 09:21:00 INFO DStreamGraph: Updating checkpoint data for time 1431523260000 ms
15/05/13 09:21:00 INFO JobScheduler: Starting job streaming job 1431523260000 ms.0 from job set of time 1431523260000 ms
15/05/13 09:21:00 INFO DStreamGraph: Updated checkpoint data for time 1431523260000 ms
15/05/13 09:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431523260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000'
15/05/13 09:21:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82784): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:21:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82784): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431523260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000'
15/05/13 09:21:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:21:00 INFO DAGScheduler: Got job 27 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:21:00 INFO DAGScheduler: Final stage: Stage 26(reduce at JsonRDD.scala:51)
15/05/13 09:21:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:21:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:21:00 INFO DAGScheduler: Submitting Stage 26 (MapPartitionsRDD[194] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:21:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=3882054, maxMem=278302556
15/05/13 09:21:00 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 5.9 KB, free 261.7 MB)
15/05/13 09:21:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=3888094, maxMem=278302556
15/05/13 09:21:00 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 4.1 KB, free 261.7 MB)
15/05/13 09:21:00 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:21:00 INFO BlockManagerMaster: Updated info of block broadcast_40_piece0
15/05/13 09:21:00 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:839
15/05/13 09:21:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82786): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431523260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000'
15/05/13 09:21:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 26 (MapPartitionsRDD[194] at map at JsonRDD.scala:51)
15/05/13 09:21:00 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
15/05/13 09:21:00 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:21:00 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.3 MB)
15/05/13 09:21:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431522960000.bk
15/05/13 09:21:00 INFO CheckpointWriter: Checkpoint for time 1431523260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000', took 7666 bytes and 188 ms
15/05/13 09:21:00 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:21:00 INFO DAGScheduler: Stage 26 (reduce at JsonRDD.scala:51) finished in 0.413 s
15/05/13 09:21:00 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 394 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:21:00 INFO DAGScheduler: Job 27 finished: reduce at JsonRDD.scala:51, took 0.450142 s
15/05/13 09:21:00 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
15/05/13 09:21:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:21:01 INFO DAGScheduler: Got job 28 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:21:01 INFO DAGScheduler: Final stage: Stage 27(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:21:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:21:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:21:01 INFO DAGScheduler: Submitting Stage 27 (MapPartitionsRDD[201] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:21:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=3892320, maxMem=278302556
15/05/13 09:21:01 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 20.0 KB, free 261.7 MB)
15/05/13 09:21:01 INFO MemoryStore: ensureFreeSpace(10758) called with curMem=3912808, maxMem=278302556
15/05/13 09:21:01 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 10.5 KB, free 261.7 MB)
15/05/13 09:21:01 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 264.9 MB)
15/05/13 09:21:01 INFO BlockManagerMaster: Updated info of block broadcast_41_piece0
15/05/13 09:21:01 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:839
15/05/13 09:21:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 27 (MapPartitionsRDD[201] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:21:01 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
15/05/13 09:21:01 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:21:01 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 265.2 MB)
15/05/13 09:21:01 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:21:01 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 456 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:21:01 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
15/05/13 09:21:01 INFO DAGScheduler: Stage 27 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.463 s
15/05/13 09:21:01 INFO DAGScheduler: Job 28 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.504897 s
15/05/13 09:21:01 INFO JobScheduler: Finished job streaming job 1431523260000 ms.0 from job set of time 1431523260000 ms
15/05/13 09:21:01 INFO JobScheduler: Total delay: 1.708 s for time 1431523260000 ms (execution: 1.442 s)
15/05/13 09:21:01 INFO MapPartitionsRDD: Removing RDD 176 from persistence list
15/05/13 09:21:01 INFO BlockManager: Removing RDD 176
15/05/13 09:21:01 INFO UnionRDD: Removing RDD 175 from persistence list
15/05/13 09:21:01 INFO BlockManager: Removing RDD 175
15/05/13 09:21:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523200000 ms: 1431523140000 ms
15/05/13 09:21:01 INFO JobGenerator: Checkpointing graph for time 1431523260000 ms
15/05/13 09:21:01 INFO DStreamGraph: Updating checkpoint data for time 1431523260000 ms
15/05/13 09:21:01 INFO DStreamGraph: Updated checkpoint data for time 1431523260000 ms
15/05/13 09:21:01 INFO CheckpointWriter: Saving checkpoint for time 1431523260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000'
15/05/13 09:21:01 INFO CheckpointWriter: Checkpoint for time 1431523260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000', took 7657 bytes and 76 ms
15/05/13 09:21:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523260000 ms
15/05/13 09:21:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523260000 ms
15/05/13 09:21:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:22:00 INFO FileInputDStream: Finding new files took 63 ms
15/05/13 09:22:00 INFO FileInputDStream: New files at time 1431523320000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523217605.json
15/05/13 09:22:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=3923566, maxMem=278302556
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 232.9 KB, free 261.4 MB)
15/05/13 09:22:00 INFO BlockManager: Removing broadcast 40
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_40_piece0
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_40_piece0 of size 4226 dropped from memory (free 274144684)
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:22:00 INFO BlockManagerMaster: Updated info of block broadcast_40_piece0
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_40
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_40 of size 6040 dropped from memory (free 274150724)
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_40_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:22:00 INFO ContextCleaner: Cleaned broadcast 40
15/05/13 09:22:00 INFO BlockManager: Removing broadcast 38
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_38_piece0
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_38_piece0 of size 11118 dropped from memory (free 274161842)
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 264.9 MB)
15/05/13 09:22:00 INFO BlockManagerMaster: Updated info of block broadcast_38_piece0
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_38
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_38 of size 21304 dropped from memory (free 274183146)
15/05/13 09:22:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=4119410, maxMem=278302556
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.9 KB, free 261.4 MB)
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_38_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 265.2 MB)
15/05/13 09:22:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:22:00 INFO BlockManagerMaster: Updated info of block broadcast_42_piece0
15/05/13 09:22:00 INFO SparkContext: Created broadcast 42 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:22:00 INFO ContextCleaner: Cleaned broadcast 38
15/05/13 09:22:00 INFO BlockManager: Removing broadcast 41
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_41_piece0
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_41_piece0 of size 10758 dropped from memory (free 274158196)
15/05/13 09:22:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_41_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 264.9 MB)
15/05/13 09:22:00 INFO BlockManagerMaster: Updated info of block broadcast_41_piece0
15/05/13 09:22:00 INFO BlockManager: Removing block broadcast_41
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_41 of size 20488 dropped from memory (free 274178684)
15/05/13 09:22:00 INFO BlockManagerInfo: Removed broadcast_41_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 265.1 MB)
15/05/13 09:22:00 INFO ContextCleaner: Cleaned broadcast 41
15/05/13 09:22:00 INFO JobScheduler: Added jobs for time 1431523320000 ms
15/05/13 09:22:00 INFO JobGenerator: Checkpointing graph for time 1431523320000 ms
15/05/13 09:22:00 INFO DStreamGraph: Updating checkpoint data for time 1431523320000 ms
15/05/13 09:22:00 INFO DStreamGraph: Updated checkpoint data for time 1431523320000 ms
15/05/13 09:22:00 INFO JobScheduler: Starting job streaming job 1431523320000 ms.0 from job set of time 1431523320000 ms
15/05/13 09:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431523320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000'
15/05/13 09:22:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82792): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431523320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000'
15/05/13 09:22:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:22:00 INFO DAGScheduler: Got job 29 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:22:00 INFO DAGScheduler: Final stage: Stage 28(reduce at JsonRDD.scala:51)
15/05/13 09:22:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:22:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:22:00 INFO DAGScheduler: Submitting Stage 28 (MapPartitionsRDD[208] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:22:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=4123872, maxMem=278302556
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 5.9 KB, free 261.5 MB)
15/05/13 09:22:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=4129912, maxMem=278302556
15/05/13 09:22:00 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 4.1 KB, free 261.5 MB)
15/05/13 09:22:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:22:00 INFO BlockManagerMaster: Updated info of block broadcast_43_piece0
15/05/13 09:22:00 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:839
15/05/13 09:22:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 28 (MapPartitionsRDD[208] at map at JsonRDD.scala:51)
15/05/13 09:22:00 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
15/05/13 09:22:00 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:22:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523020000
15/05/13 09:22:00 INFO CheckpointWriter: Checkpoint for time 1431523320000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000', took 7663 bytes and 170 ms
15/05/13 09:22:00 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:22:00 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:22:01 INFO DAGScheduler: Stage 28 (reduce at JsonRDD.scala:51) finished in 0.701 s
15/05/13 09:22:01 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 685 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:22:01 INFO DAGScheduler: Job 29 finished: reduce at JsonRDD.scala:51, took 0.740578 s
15/05/13 09:22:01 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
15/05/13 09:22:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:22:01 INFO DAGScheduler: Got job 30 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:22:01 INFO DAGScheduler: Final stage: Stage 29(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:22:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:22:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:22:01 INFO DAGScheduler: Submitting Stage 29 (MapPartitionsRDD[215] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:22:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=4134138, maxMem=278302556
15/05/13 09:22:01 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 20.4 KB, free 261.4 MB)
15/05/13 09:22:01 INFO MemoryStore: ensureFreeSpace(10911) called with curMem=4155058, maxMem=278302556
15/05/13 09:22:01 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 10.7 KB, free 261.4 MB)
15/05/13 09:22:01 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.9 MB)
15/05/13 09:22:01 INFO BlockManagerMaster: Updated info of block broadcast_44_piece0
15/05/13 09:22:01 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:839
15/05/13 09:22:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 29 (MapPartitionsRDD[215] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:22:01 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
15/05/13 09:22:01 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:22:01 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:22:01 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 188 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:22:01 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
15/05/13 09:22:01 INFO DAGScheduler: Stage 29 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.190 s
15/05/13 09:22:01 INFO DAGScheduler: Job 30 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.231764 s
15/05/13 09:22:01 INFO JobScheduler: Finished job streaming job 1431523320000 ms.0 from job set of time 1431523320000 ms
15/05/13 09:22:01 INFO JobScheduler: Total delay: 1.774 s for time 1431523320000 ms (execution: 1.470 s)
15/05/13 09:22:01 INFO MapPartitionsRDD: Removing RDD 190 from persistence list
15/05/13 09:22:01 INFO BlockManager: Removing RDD 190
15/05/13 09:22:01 INFO UnionRDD: Removing RDD 189 from persistence list
15/05/13 09:22:01 INFO BlockManager: Removing RDD 189
15/05/13 09:22:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523260000 ms: 1431523200000 ms
15/05/13 09:22:01 INFO JobGenerator: Checkpointing graph for time 1431523320000 ms
15/05/13 09:22:01 INFO DStreamGraph: Updating checkpoint data for time 1431523320000 ms
15/05/13 09:22:01 INFO DStreamGraph: Updated checkpoint data for time 1431523320000 ms
15/05/13 09:22:01 INFO CheckpointWriter: Saving checkpoint for time 1431523320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000'
15/05/13 09:22:01 INFO CheckpointWriter: Checkpoint for time 1431523320000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000', took 7655 bytes and 68 ms
15/05/13 09:22:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523320000 ms
15/05/13 09:22:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523320000 ms
15/05/13 09:22:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:23:00 INFO FileInputDStream: Finding new files took 47 ms
15/05/13 09:23:00 INFO FileInputDStream: New files at time 1431523380000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523278113.json
15/05/13 09:23:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=4165969, maxMem=278302556
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 232.9 KB, free 261.2 MB)
15/05/13 09:23:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=4404501, maxMem=278302556
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.9 KB, free 261.2 MB)
15/05/13 09:23:00 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:23:00 INFO BlockManagerMaster: Updated info of block broadcast_45_piece0
15/05/13 09:23:00 INFO SparkContext: Created broadcast 45 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:23:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:23:00 INFO JobScheduler: Starting job streaming job 1431523380000 ms.0 from job set of time 1431523380000 ms
15/05/13 09:23:00 INFO JobScheduler: Added jobs for time 1431523380000 ms
15/05/13 09:23:00 INFO JobGenerator: Checkpointing graph for time 1431523380000 ms
15/05/13 09:23:00 INFO DStreamGraph: Updating checkpoint data for time 1431523380000 ms
15/05/13 09:23:00 INFO DStreamGraph: Updated checkpoint data for time 1431523380000 ms
15/05/13 09:23:00 INFO CheckpointWriter: Saving checkpoint for time 1431523380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000'
15/05/13 09:23:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:23:00 INFO DAGScheduler: Got job 31 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:23:00 INFO DAGScheduler: Final stage: Stage 30(reduce at JsonRDD.scala:51)
15/05/13 09:23:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:23:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000
15/05/13 09:23:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:23:00 INFO DAGScheduler: Submitting Stage 30 (MapPartitionsRDD[222] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:23:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=4440210, maxMem=278302556
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 5.9 KB, free 261.2 MB)
15/05/13 09:23:00 INFO CheckpointWriter: Checkpoint for time 1431523380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000', took 7661 bytes and 89 ms
15/05/13 09:23:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=4446250, maxMem=278302556
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 4.1 KB, free 261.2 MB)
15/05/13 09:23:00 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:23:00 INFO BlockManagerMaster: Updated info of block broadcast_46_piece0
15/05/13 09:23:00 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:839
15/05/13 09:23:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 30 (MapPartitionsRDD[222] at map at JsonRDD.scala:51)
15/05/13 09:23:00 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
15/05/13 09:23:00 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:23:00 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:23:00 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:23:00 INFO DAGScheduler: Stage 30 (reduce at JsonRDD.scala:51) finished in 0.436 s
15/05/13 09:23:00 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 30) in 428 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:23:00 INFO DAGScheduler: Job 31 finished: reduce at JsonRDD.scala:51, took 0.481442 s
15/05/13 09:23:00 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
15/05/13 09:23:00 INFO BlockManager: Removing broadcast 43
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_43
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_43 of size 6040 dropped from memory (free 273858120)
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_43_piece0
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_43_piece0 of size 4226 dropped from memory (free 273862346)
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:23:00 INFO BlockManagerMaster: Updated info of block broadcast_43_piece0
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_43_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:23:00 INFO ContextCleaner: Cleaned broadcast 43
15/05/13 09:23:00 INFO BlockManager: Removing broadcast 46
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_46_piece0
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_46_piece0 of size 4226 dropped from memory (free 273866572)
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_46_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:23:00 INFO BlockManagerMaster: Updated info of block broadcast_46_piece0
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_46
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_46 of size 6040 dropped from memory (free 273872612)
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_46_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:23:00 INFO ContextCleaner: Cleaned broadcast 46
15/05/13 09:23:00 INFO BlockManager: Removing broadcast 44
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_44_piece0
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_44_piece0 of size 10911 dropped from memory (free 273883523)
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_44_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.9 MB)
15/05/13 09:23:00 INFO BlockManagerMaster: Updated info of block broadcast_44_piece0
15/05/13 09:23:00 INFO BlockManager: Removing block broadcast_44
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_44 of size 20920 dropped from memory (free 273904443)
15/05/13 09:23:00 INFO BlockManagerInfo: Removed broadcast_44_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:23:00 INFO ContextCleaner: Cleaned broadcast 44
15/05/13 09:23:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:23:00 INFO DAGScheduler: Got job 32 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:23:00 INFO DAGScheduler: Final stage: Stage 31(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:23:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:23:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:23:00 INFO DAGScheduler: Submitting Stage 31 (MapPartitionsRDD[229] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:23:00 INFO MemoryStore: ensureFreeSpace(21072) called with curMem=4398113, maxMem=278302556
15/05/13 09:23:00 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 20.6 KB, free 261.2 MB)
15/05/13 09:23:01 INFO MemoryStore: ensureFreeSpace(11003) called with curMem=4419185, maxMem=278302556
15/05/13 09:23:01 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 10.7 KB, free 261.2 MB)
15/05/13 09:23:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.9 MB)
15/05/13 09:23:01 INFO BlockManagerMaster: Updated info of block broadcast_47_piece0
15/05/13 09:23:01 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:839
15/05/13 09:23:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 31 (MapPartitionsRDD[229] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:23:01 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
15/05/13 09:23:01 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:23:01 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:23:01 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:23:01 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 31) in 496 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:23:01 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
15/05/13 09:23:01 INFO DAGScheduler: Stage 31 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.501 s
15/05/13 09:23:01 INFO DAGScheduler: Job 32 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.528264 s
15/05/13 09:23:01 INFO JobScheduler: Finished job streaming job 1431523380000 ms.0 from job set of time 1431523380000 ms
15/05/13 09:23:01 INFO JobScheduler: Total delay: 1.564 s for time 1431523380000 ms (execution: 1.397 s)
15/05/13 09:23:01 INFO MapPartitionsRDD: Removing RDD 204 from persistence list
15/05/13 09:23:01 INFO BlockManager: Removing RDD 204
15/05/13 09:23:01 INFO UnionRDD: Removing RDD 203 from persistence list
15/05/13 09:23:01 INFO BlockManager: Removing RDD 203
15/05/13 09:23:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523320000 ms: 1431523260000 ms
15/05/13 09:23:01 INFO JobGenerator: Checkpointing graph for time 1431523380000 ms
15/05/13 09:23:01 INFO DStreamGraph: Updating checkpoint data for time 1431523380000 ms
15/05/13 09:23:01 INFO DStreamGraph: Updated checkpoint data for time 1431523380000 ms
15/05/13 09:23:01 INFO CheckpointWriter: Saving checkpoint for time 1431523380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000'
15/05/13 09:23:01 INFO CheckpointWriter: Checkpoint for time 1431523380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000', took 7654 bytes and 61 ms
15/05/13 09:23:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523380000 ms
15/05/13 09:23:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523380000 ms
15/05/13 09:23:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:24:00 INFO FileInputDStream: Finding new files took 56 ms
15/05/13 09:24:00 INFO FileInputDStream: New files at time 1431523440000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523340325.json
15/05/13 09:24:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=4430188, maxMem=278302556
15/05/13 09:24:00 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 232.9 KB, free 261.0 MB)
15/05/13 09:24:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=4668720, maxMem=278302556
15/05/13 09:24:00 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 34.9 KB, free 260.9 MB)
15/05/13 09:24:00 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:24:00 INFO BlockManagerMaster: Updated info of block broadcast_48_piece0
15/05/13 09:24:00 INFO SparkContext: Created broadcast 48 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:24:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:24:00 INFO JobScheduler: Added jobs for time 1431523440000 ms
15/05/13 09:24:00 INFO JobScheduler: Starting job streaming job 1431523440000 ms.0 from job set of time 1431523440000 ms
15/05/13 09:24:00 INFO JobGenerator: Checkpointing graph for time 1431523440000 ms
15/05/13 09:24:00 INFO DStreamGraph: Updating checkpoint data for time 1431523440000 ms
15/05/13 09:24:00 INFO DStreamGraph: Updated checkpoint data for time 1431523440000 ms
15/05/13 09:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431523440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000'
15/05/13 09:24:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82806): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:24:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82806): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431523440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000'
15/05/13 09:24:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82808): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:24:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82808): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431523440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000'
15/05/13 09:24:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:24:00 INFO DAGScheduler: Got job 33 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:24:00 INFO DAGScheduler: Final stage: Stage 32(reduce at JsonRDD.scala:51)
15/05/13 09:24:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:24:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:24:00 INFO DAGScheduler: Submitting Stage 32 (MapPartitionsRDD[236] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:24:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=4704428, maxMem=278302556
15/05/13 09:24:00 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 5.9 KB, free 260.9 MB)
15/05/13 09:24:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=4710468, maxMem=278302556
15/05/13 09:24:00 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 4.1 KB, free 260.9 MB)
15/05/13 09:24:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:24:00 INFO BlockManagerMaster: Updated info of block broadcast_49_piece0
15/05/13 09:24:00 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:839
15/05/13 09:24:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 32 (MapPartitionsRDD[236] at map at JsonRDD.scala:51)
15/05/13 09:24:00 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
15/05/13 09:24:00 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:24:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523140000.bk
15/05/13 09:24:00 INFO CheckpointWriter: Checkpoint for time 1431523440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000', took 7661 bytes and 133 ms
15/05/13 09:24:00 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:24:00 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:24:00 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 32) in 535 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:24:00 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
15/05/13 09:24:00 INFO DAGScheduler: Stage 32 (reduce at JsonRDD.scala:51) finished in 0.554 s
15/05/13 09:24:00 INFO DAGScheduler: Job 33 finished: reduce at JsonRDD.scala:51, took 0.593323 s
15/05/13 09:24:01 INFO BlockManager: Removing broadcast 47
15/05/13 09:24:01 INFO BlockManager: Removing block broadcast_47
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_47 of size 21072 dropped from memory (free 273608932)
15/05/13 09:24:01 INFO BlockManager: Removing block broadcast_47_piece0
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_47_piece0 of size 11003 dropped from memory (free 273619935)
15/05/13 09:24:01 INFO BlockManagerInfo: Removed broadcast_47_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.8 MB)
15/05/13 09:24:01 INFO BlockManagerMaster: Updated info of block broadcast_47_piece0
15/05/13 09:24:01 INFO BlockManagerInfo: Removed broadcast_47_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:24:01 INFO ContextCleaner: Cleaned broadcast 47
15/05/13 09:24:01 INFO BlockManager: Removing broadcast 49
15/05/13 09:24:01 INFO BlockManager: Removing block broadcast_49
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_49 of size 6040 dropped from memory (free 273625975)
15/05/13 09:24:01 INFO BlockManager: Removing block broadcast_49_piece0
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_49_piece0 of size 4228 dropped from memory (free 273630203)
15/05/13 09:24:01 INFO BlockManagerInfo: Removed broadcast_49_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:24:01 INFO BlockManagerMaster: Updated info of block broadcast_49_piece0
15/05/13 09:24:01 INFO BlockManagerInfo: Removed broadcast_49_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:24:01 INFO ContextCleaner: Cleaned broadcast 49
15/05/13 09:24:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:24:01 INFO DAGScheduler: Got job 34 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:24:01 INFO DAGScheduler: Final stage: Stage 33(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:24:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:24:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:24:01 INFO DAGScheduler: Submitting Stage 33 (MapPartitionsRDD[243] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:24:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=4672353, maxMem=278302556
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 20.6 KB, free 260.9 MB)
15/05/13 09:24:01 INFO MemoryStore: ensureFreeSpace(11011) called with curMem=4693449, maxMem=278302556
15/05/13 09:24:01 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 10.8 KB, free 260.9 MB)
15/05/13 09:24:01 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 264.8 MB)
15/05/13 09:24:01 INFO BlockManagerMaster: Updated info of block broadcast_50_piece0
15/05/13 09:24:01 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:839
15/05/13 09:24:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 33 (MapPartitionsRDD[243] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:24:01 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
15/05/13 09:24:01 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:24:01 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 265.0 MB)
15/05/13 09:24:01 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 33) in 212 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:24:01 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
15/05/13 09:24:01 INFO DAGScheduler: Stage 33 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.214 s
15/05/13 09:24:01 INFO DAGScheduler: Job 34 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.252437 s
15/05/13 09:24:01 INFO JobScheduler: Finished job streaming job 1431523440000 ms.0 from job set of time 1431523440000 ms
15/05/13 09:24:01 INFO MapPartitionsRDD: Removing RDD 218 from persistence list
15/05/13 09:24:01 INFO BlockManager: Removing RDD 218
15/05/13 09:24:01 INFO UnionRDD: Removing RDD 217 from persistence list
15/05/13 09:24:01 INFO BlockManager: Removing RDD 217
15/05/13 09:24:01 INFO JobScheduler: Total delay: 1.611 s for time 1431523440000 ms (execution: 1.351 s)
15/05/13 09:24:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523380000 ms: 1431523320000 ms
15/05/13 09:24:01 INFO JobGenerator: Checkpointing graph for time 1431523440000 ms
15/05/13 09:24:01 INFO DStreamGraph: Updating checkpoint data for time 1431523440000 ms
15/05/13 09:24:01 INFO DStreamGraph: Updated checkpoint data for time 1431523440000 ms
15/05/13 09:24:01 INFO CheckpointWriter: Saving checkpoint for time 1431523440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000'
15/05/13 09:24:01 INFO CheckpointWriter: Checkpoint for time 1431523440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000', took 7655 bytes and 60 ms
15/05/13 09:24:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523440000 ms
15/05/13 09:24:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523440000 ms
15/05/13 09:24:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:25:00 INFO FileInputDStream: Finding new files took 38 ms
15/05/13 09:25:00 INFO FileInputDStream: New files at time 1431523500000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523401321.json
15/05/13 09:25:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=4704460, maxMem=278302556
15/05/13 09:25:00 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 232.9 KB, free 260.7 MB)
15/05/13 09:25:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=4942992, maxMem=278302556
15/05/13 09:25:00 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 34.9 KB, free 260.7 MB)
15/05/13 09:25:00 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:25:00 INFO BlockManagerMaster: Updated info of block broadcast_51_piece0
15/05/13 09:25:00 INFO SparkContext: Created broadcast 51 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:25:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:25:00 INFO JobScheduler: Added jobs for time 1431523500000 ms
15/05/13 09:25:00 INFO JobGenerator: Checkpointing graph for time 1431523500000 ms
15/05/13 09:25:00 INFO JobScheduler: Starting job streaming job 1431523500000 ms.0 from job set of time 1431523500000 ms
15/05/13 09:25:00 INFO DStreamGraph: Updating checkpoint data for time 1431523500000 ms
15/05/13 09:25:00 INFO DStreamGraph: Updated checkpoint data for time 1431523500000 ms
15/05/13 09:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431523500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000'
15/05/13 09:25:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82817): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:25:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82817): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431523500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000'
15/05/13 09:25:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:25:00 INFO DAGScheduler: Got job 35 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:25:00 INFO DAGScheduler: Final stage: Stage 34(reduce at JsonRDD.scala:51)
15/05/13 09:25:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:25:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:25:00 INFO DAGScheduler: Submitting Stage 34 (MapPartitionsRDD[250] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:25:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=4978700, maxMem=278302556
15/05/13 09:25:00 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 5.9 KB, free 260.7 MB)
15/05/13 09:25:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=4984740, maxMem=278302556
15/05/13 09:25:00 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 4.1 KB, free 260.7 MB)
15/05/13 09:25:00 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:25:00 INFO BlockManagerMaster: Updated info of block broadcast_52_piece0
15/05/13 09:25:00 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:839
15/05/13 09:25:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82819): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431523500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000'
15/05/13 09:25:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 34 (MapPartitionsRDD[250] at map at JsonRDD.scala:51)
15/05/13 09:25:00 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks
15/05/13 09:25:00 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 34, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:25:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82821): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:25:00 WARN CheckpointWriter: Could not write checkpoint for time 1431523500000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000'
15/05/13 09:25:00 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:25:00 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:25:01 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 34) in 866 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:25:01 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool 
15/05/13 09:25:01 INFO DAGScheduler: Stage 34 (reduce at JsonRDD.scala:51) finished in 0.886 s
15/05/13 09:25:01 INFO DAGScheduler: Job 35 finished: reduce at JsonRDD.scala:51, took 0.931117 s
15/05/13 09:25:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:25:01 INFO DAGScheduler: Got job 36 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:25:01 INFO DAGScheduler: Final stage: Stage 35(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:25:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:25:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:25:01 INFO DAGScheduler: Submitting Stage 35 (MapPartitionsRDD[257] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:25:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=4988966, maxMem=278302556
15/05/13 09:25:01 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 20.4 KB, free 260.6 MB)
15/05/13 09:25:01 INFO MemoryStore: ensureFreeSpace(10815) called with curMem=5009886, maxMem=278302556
15/05/13 09:25:01 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 10.6 KB, free 260.6 MB)
15/05/13 09:25:01 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 264.8 MB)
15/05/13 09:25:01 INFO BlockManagerMaster: Updated info of block broadcast_53_piece0
15/05/13 09:25:01 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:839
15/05/13 09:25:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 35 (MapPartitionsRDD[257] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:25:01 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks
15/05/13 09:25:01 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 35, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:25:01 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 265.0 MB)
15/05/13 09:25:01 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:25:01 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 35) in 364 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:25:01 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool 
15/05/13 09:25:01 INFO DAGScheduler: Stage 35 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.367 s
15/05/13 09:25:01 INFO DAGScheduler: Job 36 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.404101 s
15/05/13 09:25:01 INFO JobScheduler: Finished job streaming job 1431523500000 ms.0 from job set of time 1431523500000 ms
15/05/13 09:25:01 INFO JobScheduler: Total delay: 1.887 s for time 1431523500000 ms (execution: 1.730 s)
15/05/13 09:25:01 INFO MapPartitionsRDD: Removing RDD 232 from persistence list
15/05/13 09:25:01 INFO BlockManager: Removing RDD 232
15/05/13 09:25:01 INFO UnionRDD: Removing RDD 231 from persistence list
15/05/13 09:25:01 INFO BlockManager: Removing RDD 231
15/05/13 09:25:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523440000 ms: 1431523380000 ms
15/05/13 09:25:01 INFO JobGenerator: Checkpointing graph for time 1431523500000 ms
15/05/13 09:25:01 INFO DStreamGraph: Updating checkpoint data for time 1431523500000 ms
15/05/13 09:25:01 INFO DStreamGraph: Updated checkpoint data for time 1431523500000 ms
15/05/13 09:25:01 INFO CheckpointWriter: Saving checkpoint for time 1431523500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000'
15/05/13 09:25:02 INFO CheckpointWriter: Checkpoint for time 1431523500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523500000', took 7657 bytes and 90 ms
15/05/13 09:25:02 INFO DStreamGraph: Clearing checkpoint data for time 1431523500000 ms
15/05/13 09:25:02 INFO DStreamGraph: Cleared checkpoint data for time 1431523500000 ms
15/05/13 09:25:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:25:43 INFO BlockManager: Removing broadcast 50
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_50_piece0
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_50_piece0 of size 11011 dropped from memory (free 273292866)
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_50_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 264.8 MB)
15/05/13 09:25:43 INFO BlockManagerMaster: Updated info of block broadcast_50_piece0
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_50
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_50 of size 21096 dropped from memory (free 273313962)
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_50_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 265.0 MB)
15/05/13 09:25:43 INFO ContextCleaner: Cleaned broadcast 50
15/05/13 09:25:43 INFO BlockManager: Removing broadcast 52
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_52
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_52 of size 6040 dropped from memory (free 273320002)
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_52_piece0
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_52_piece0 of size 4226 dropped from memory (free 273324228)
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_52_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:25:43 INFO BlockManagerMaster: Updated info of block broadcast_52_piece0
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_52_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.2 MB)
15/05/13 09:25:43 INFO ContextCleaner: Cleaned broadcast 52
15/05/13 09:25:43 INFO BlockManager: Removing broadcast 53
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_53_piece0
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_53_piece0 of size 10815 dropped from memory (free 273335043)
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_53_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 264.8 MB)
15/05/13 09:25:43 INFO BlockManagerMaster: Updated info of block broadcast_53_piece0
15/05/13 09:25:43 INFO BlockManager: Removing block broadcast_53
15/05/13 09:25:43 INFO MemoryStore: Block broadcast_53 of size 20920 dropped from memory (free 273355963)
15/05/13 09:25:43 INFO BlockManagerInfo: Removed broadcast_53_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 265.0 MB)
15/05/13 09:25:43 INFO ContextCleaner: Cleaned broadcast 53
15/05/13 09:26:01 INFO FileInputDStream: Finding new files took 1302 ms
15/05/13 09:26:01 INFO FileInputDStream: New files at time 1431523560000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523461948.json
15/05/13 09:26:01 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=4946593, maxMem=278302556
15/05/13 09:26:01 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 232.9 KB, free 260.5 MB)
15/05/13 09:26:01 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=5185125, maxMem=278302556
15/05/13 09:26:01 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 34.9 KB, free 260.4 MB)
15/05/13 09:26:01 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:26:01 INFO BlockManagerMaster: Updated info of block broadcast_54_piece0
15/05/13 09:26:01 INFO SparkContext: Created broadcast 54 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:26:01 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:26:01 INFO JobScheduler: Added jobs for time 1431523560000 ms
15/05/13 09:26:01 INFO JobScheduler: Starting job streaming job 1431523560000 ms.0 from job set of time 1431523560000 ms
15/05/13 09:26:01 INFO JobGenerator: Checkpointing graph for time 1431523560000 ms
15/05/13 09:26:01 INFO DStreamGraph: Updating checkpoint data for time 1431523560000 ms
15/05/13 09:26:01 INFO DStreamGraph: Updated checkpoint data for time 1431523560000 ms
15/05/13 09:26:01 INFO CheckpointWriter: Saving checkpoint for time 1431523560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000'
15/05/13 09:26:01 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:26:01 INFO DAGScheduler: Got job 37 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:26:01 INFO DAGScheduler: Final stage: Stage 36(reduce at JsonRDD.scala:51)
15/05/13 09:26:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:26:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:26:01 INFO DAGScheduler: Submitting Stage 36 (MapPartitionsRDD[264] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:26:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=5220833, maxMem=278302556
15/05/13 09:26:01 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 5.9 KB, free 260.4 MB)
15/05/13 09:26:01 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=5226873, maxMem=278302556
15/05/13 09:26:01 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 4.1 KB, free 260.4 MB)
15/05/13 09:26:01 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:26:01 INFO BlockManagerMaster: Updated info of block broadcast_55_piece0
15/05/13 09:26:01 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:839
15/05/13 09:26:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 36 (MapPartitionsRDD[264] at map at JsonRDD.scala:51)
15/05/13 09:26:01 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks
15/05/13 09:26:01 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 36, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:26:01 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:26:01 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:26:02 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 36) in 416 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:26:02 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool 
15/05/13 09:26:02 INFO DAGScheduler: Stage 36 (reduce at JsonRDD.scala:51) finished in 0.431 s
15/05/13 09:26:02 INFO DAGScheduler: Job 37 finished: reduce at JsonRDD.scala:51, took 0.478415 s
15/05/13 09:26:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:26:02 INFO DAGScheduler: Got job 38 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:26:02 INFO DAGScheduler: Final stage: Stage 37(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:26:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:26:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:26:02 INFO DAGScheduler: Submitting Stage 37 (MapPartitionsRDD[271] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:26:02 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=5231099, maxMem=278302556
15/05/13 09:26:02 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 21.0 KB, free 260.4 MB)
15/05/13 09:26:02 INFO MemoryStore: ensureFreeSpace(11109) called with curMem=5252579, maxMem=278302556
15/05/13 09:26:02 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 10.8 KB, free 260.4 MB)
15/05/13 09:26:02 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 264.7 MB)
15/05/13 09:26:02 INFO BlockManagerMaster: Updated info of block broadcast_56_piece0
15/05/13 09:26:02 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:839
15/05/13 09:26:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 37 (MapPartitionsRDD[271] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:26:02 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks
15/05/13 09:26:02 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 37, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:26:02 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 265.0 MB)
15/05/13 09:26:02 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 37) in 153 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:26:02 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool 
15/05/13 09:26:02 INFO DAGScheduler: Stage 37 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.155 s
15/05/13 09:26:02 INFO DAGScheduler: Job 38 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.190742 s
15/05/13 09:26:06 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82832): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:26:06 INFO CheckpointWriter: Saving checkpoint for time 1431523560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000'
15/05/13 09:26:06 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82834): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:26:06 INFO CheckpointWriter: Saving checkpoint for time 1431523560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000'
15/05/13 09:26:06 INFO JobScheduler: Finished job streaming job 1431523560000 ms.0 from job set of time 1431523560000 ms
15/05/13 09:26:06 INFO JobScheduler: Total delay: 6.410 s for time 1431523560000 ms (execution: 4.941 s)
15/05/13 09:26:06 INFO MapPartitionsRDD: Removing RDD 246 from persistence list
15/05/13 09:26:06 INFO BlockManager: Removing RDD 246
15/05/13 09:26:06 INFO UnionRDD: Removing RDD 245 from persistence list
15/05/13 09:26:06 INFO BlockManager: Removing RDD 245
15/05/13 09:26:06 INFO FileInputDStream: Cleared 1 old files that were older than 1431523500000 ms: 1431523440000 ms
15/05/13 09:26:06 INFO JobGenerator: Checkpointing graph for time 1431523560000 ms
15/05/13 09:26:06 INFO DStreamGraph: Updating checkpoint data for time 1431523560000 ms
15/05/13 09:26:06 INFO DStreamGraph: Updated checkpoint data for time 1431523560000 ms
15/05/13 09:26:06 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000.bk
15/05/13 09:26:06 INFO CheckpointWriter: Checkpoint for time 1431523560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000', took 7665 bytes and 4972 ms
15/05/13 09:26:06 INFO CheckpointWriter: Saving checkpoint for time 1431523560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000'
15/05/13 09:26:06 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523260000
15/05/13 09:26:06 INFO CheckpointWriter: Checkpoint for time 1431523560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523560000', took 7654 bytes and 63 ms
15/05/13 09:26:06 INFO DStreamGraph: Clearing checkpoint data for time 1431523560000 ms
15/05/13 09:26:06 INFO DStreamGraph: Cleared checkpoint data for time 1431523560000 ms
15/05/13 09:26:06 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:27:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 09:27:00 INFO FileInputDStream: New files at time 1431523620000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523525455.json
15/05/13 09:27:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=5263688, maxMem=278302556
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 232.9 KB, free 260.2 MB)
15/05/13 09:27:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=5502220, maxMem=278302556
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 34.9 KB, free 260.1 MB)
15/05/13 09:27:00 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:27:00 INFO BlockManagerMaster: Updated info of block broadcast_57_piece0
15/05/13 09:27:00 INFO SparkContext: Created broadcast 57 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:27:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:27:00 INFO JobScheduler: Added jobs for time 1431523620000 ms
15/05/13 09:27:00 INFO JobGenerator: Checkpointing graph for time 1431523620000 ms
15/05/13 09:27:00 INFO JobScheduler: Starting job streaming job 1431523620000 ms.0 from job set of time 1431523620000 ms
15/05/13 09:27:00 INFO DStreamGraph: Updating checkpoint data for time 1431523620000 ms
15/05/13 09:27:00 INFO DStreamGraph: Updated checkpoint data for time 1431523620000 ms
15/05/13 09:27:00 INFO CheckpointWriter: Saving checkpoint for time 1431523620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523620000'
15/05/13 09:27:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:27:00 INFO DAGScheduler: Got job 39 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:27:00 INFO DAGScheduler: Final stage: Stage 38(reduce at JsonRDD.scala:51)
15/05/13 09:27:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:27:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:27:00 INFO DAGScheduler: Submitting Stage 38 (MapPartitionsRDD[278] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:27:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=5537928, maxMem=278302556
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 5.9 KB, free 260.1 MB)
15/05/13 09:27:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=5543968, maxMem=278302556
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 4.1 KB, free 260.1 MB)
15/05/13 09:27:00 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:27:00 INFO BlockManagerMaster: Updated info of block broadcast_58_piece0
15/05/13 09:27:00 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:839
15/05/13 09:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 38 (MapPartitionsRDD[278] at map at JsonRDD.scala:51)
15/05/13 09:27:00 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks
15/05/13 09:27:00 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 38, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:27:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523320000
15/05/13 09:27:00 INFO CheckpointWriter: Checkpoint for time 1431523620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523620000', took 7662 bytes and 116 ms
15/05/13 09:27:00 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:27:00 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:27:00 INFO DAGScheduler: Stage 38 (reduce at JsonRDD.scala:51) finished in 0.390 s
15/05/13 09:27:00 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 38) in 377 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:27:00 INFO DAGScheduler: Job 39 finished: reduce at JsonRDD.scala:51, took 0.431788 s
15/05/13 09:27:00 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool 
15/05/13 09:27:00 INFO BlockManager: Removing broadcast 56
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_56
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_56 of size 21480 dropped from memory (free 272775842)
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_56_piece0
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_56_piece0 of size 11109 dropped from memory (free 272786951)
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_56_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 264.7 MB)
15/05/13 09:27:00 INFO BlockManagerMaster: Updated info of block broadcast_56_piece0
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_56_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 264.9 MB)
15/05/13 09:27:00 INFO ContextCleaner: Cleaned broadcast 56
15/05/13 09:27:00 INFO BlockManager: Removing broadcast 55
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_55_piece0
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_55_piece0 of size 4226 dropped from memory (free 272791177)
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_55_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:27:00 INFO BlockManagerMaster: Updated info of block broadcast_55_piece0
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_55
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_55 of size 6040 dropped from memory (free 272797217)
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_55_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:27:00 INFO ContextCleaner: Cleaned broadcast 55
15/05/13 09:27:00 INFO BlockManager: Removing broadcast 58
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_58
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_58 of size 6040 dropped from memory (free 272803257)
15/05/13 09:27:00 INFO BlockManager: Removing block broadcast_58_piece0
15/05/13 09:27:00 INFO MemoryStore: Block broadcast_58_piece0 of size 4226 dropped from memory (free 272807483)
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_58_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:27:00 INFO BlockManagerMaster: Updated info of block broadcast_58_piece0
15/05/13 09:27:00 INFO BlockManagerInfo: Removed broadcast_58_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:27:00 INFO ContextCleaner: Cleaned broadcast 58
15/05/13 09:27:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:27:00 INFO DAGScheduler: Got job 40 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:27:00 INFO DAGScheduler: Final stage: Stage 39(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:27:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:27:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:27:01 INFO DAGScheduler: Submitting Stage 39 (MapPartitionsRDD[285] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:27:01 INFO MemoryStore: ensureFreeSpace(20936) called with curMem=5495073, maxMem=278302556
15/05/13 09:27:01 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 20.4 KB, free 260.1 MB)
15/05/13 09:27:01 INFO MemoryStore: ensureFreeSpace(10951) called with curMem=5516009, maxMem=278302556
15/05/13 09:27:01 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 10.7 KB, free 260.1 MB)
15/05/13 09:27:01 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.7 MB)
15/05/13 09:27:01 INFO BlockManagerMaster: Updated info of block broadcast_59_piece0
15/05/13 09:27:01 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:839
15/05/13 09:27:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 39 (MapPartitionsRDD[285] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:27:01 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks
15/05/13 09:27:01 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 39, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:27:01 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 265.2 MB)
15/05/13 09:27:01 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.2 MB)
15/05/13 09:27:01 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 39) in 263 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:27:01 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool 
15/05/13 09:27:01 INFO DAGScheduler: Stage 39 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.265 s
15/05/13 09:27:01 INFO DAGScheduler: Job 40 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.300517 s
15/05/13 09:27:01 INFO JobScheduler: Finished job streaming job 1431523620000 ms.0 from job set of time 1431523620000 ms
15/05/13 09:27:01 INFO JobScheduler: Total delay: 1.335 s for time 1431523620000 ms (execution: 1.084 s)
15/05/13 09:27:01 INFO MapPartitionsRDD: Removing RDD 260 from persistence list
15/05/13 09:27:01 INFO BlockManager: Removing RDD 260
15/05/13 09:27:01 INFO UnionRDD: Removing RDD 259 from persistence list
15/05/13 09:27:01 INFO BlockManager: Removing RDD 259
15/05/13 09:27:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523560000 ms: 1431523500000 ms
15/05/13 09:27:01 INFO JobGenerator: Checkpointing graph for time 1431523620000 ms
15/05/13 09:27:01 INFO DStreamGraph: Updating checkpoint data for time 1431523620000 ms
15/05/13 09:27:01 INFO DStreamGraph: Updated checkpoint data for time 1431523620000 ms
15/05/13 09:27:01 INFO CheckpointWriter: Saving checkpoint for time 1431523620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523620000'
15/05/13 09:27:01 INFO CheckpointWriter: Checkpoint for time 1431523620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523620000', took 7655 bytes and 76 ms
15/05/13 09:27:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523620000 ms
15/05/13 09:27:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523620000 ms
15/05/13 09:27:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:28:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 09:28:00 INFO FileInputDStream: New files at time 1431523680000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523585755.json
15/05/13 09:28:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=5526960, maxMem=278302556
15/05/13 09:28:00 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 232.9 KB, free 259.9 MB)
15/05/13 09:28:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=5765492, maxMem=278302556
15/05/13 09:28:00 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 34.9 KB, free 259.9 MB)
15/05/13 09:28:00 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:28:00 INFO BlockManagerMaster: Updated info of block broadcast_60_piece0
15/05/13 09:28:00 INFO SparkContext: Created broadcast 60 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:28:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:28:00 INFO JobScheduler: Added jobs for time 1431523680000 ms
15/05/13 09:28:00 INFO JobGenerator: Checkpointing graph for time 1431523680000 ms
15/05/13 09:28:00 INFO DStreamGraph: Updating checkpoint data for time 1431523680000 ms
15/05/13 09:28:00 INFO DStreamGraph: Updated checkpoint data for time 1431523680000 ms
15/05/13 09:28:00 INFO JobScheduler: Starting job streaming job 1431523680000 ms.0 from job set of time 1431523680000 ms
15/05/13 09:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431523680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000'
15/05/13 09:28:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:28:00 INFO DAGScheduler: Got job 41 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:28:00 INFO DAGScheduler: Final stage: Stage 40(reduce at JsonRDD.scala:51)
15/05/13 09:28:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:28:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:28:00 INFO DAGScheduler: Submitting Stage 40 (MapPartitionsRDD[292] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:28:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=5801200, maxMem=278302556
15/05/13 09:28:00 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 5.9 KB, free 259.9 MB)
15/05/13 09:28:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=5807240, maxMem=278302556
15/05/13 09:28:00 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 4.1 KB, free 259.9 MB)
15/05/13 09:28:00 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:28:00 INFO BlockManagerMaster: Updated info of block broadcast_61_piece0
15/05/13 09:28:00 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:839
15/05/13 09:28:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 40 (MapPartitionsRDD[292] at map at JsonRDD.scala:51)
15/05/13 09:28:00 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks
15/05/13 09:28:00 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 40, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:28:00 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:28:00 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:28:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82847): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431523680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000'
15/05/13 09:28:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523380000.bk
15/05/13 09:28:00 INFO CheckpointWriter: Checkpoint for time 1431523680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000', took 7663 bytes and 479 ms
15/05/13 09:28:00 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 40) in 532 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:28:00 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool 
15/05/13 09:28:00 INFO DAGScheduler: Stage 40 (reduce at JsonRDD.scala:51) finished in 0.553 s
15/05/13 09:28:00 INFO DAGScheduler: Job 41 finished: reduce at JsonRDD.scala:51, took 0.587871 s
15/05/13 09:28:01 INFO BlockManager: Removing broadcast 59
15/05/13 09:28:01 INFO BlockManager: Removing block broadcast_59_piece0
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_59_piece0 of size 10951 dropped from memory (free 272502043)
15/05/13 09:28:01 INFO BlockManagerInfo: Removed broadcast_59_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.7 MB)
15/05/13 09:28:01 INFO BlockManagerMaster: Updated info of block broadcast_59_piece0
15/05/13 09:28:01 INFO BlockManager: Removing block broadcast_59
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_59 of size 20936 dropped from memory (free 272522979)
15/05/13 09:28:01 INFO BlockManagerInfo: Removed broadcast_59_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 265.2 MB)
15/05/13 09:28:01 INFO ContextCleaner: Cleaned broadcast 59
15/05/13 09:28:01 INFO BlockManager: Removing broadcast 61
15/05/13 09:28:01 INFO BlockManager: Removing block broadcast_61_piece0
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_61_piece0 of size 4224 dropped from memory (free 272527203)
15/05/13 09:28:01 INFO BlockManagerInfo: Removed broadcast_61_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:28:01 INFO BlockManagerMaster: Updated info of block broadcast_61_piece0
15/05/13 09:28:01 INFO BlockManager: Removing block broadcast_61
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_61 of size 6040 dropped from memory (free 272533243)
15/05/13 09:28:01 INFO BlockManagerInfo: Removed broadcast_61_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:28:01 INFO ContextCleaner: Cleaned broadcast 61
15/05/13 09:28:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:28:01 INFO DAGScheduler: Got job 42 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:28:01 INFO DAGScheduler: Final stage: Stage 41(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:28:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:28:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:28:01 INFO DAGScheduler: Submitting Stage 41 (MapPartitionsRDD[299] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:28:01 INFO MemoryStore: ensureFreeSpace(20664) called with curMem=5769313, maxMem=278302556
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 20.2 KB, free 259.9 MB)
15/05/13 09:28:01 INFO MemoryStore: ensureFreeSpace(10815) called with curMem=5789977, maxMem=278302556
15/05/13 09:28:01 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 10.6 KB, free 259.9 MB)
15/05/13 09:28:01 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 264.7 MB)
15/05/13 09:28:01 INFO BlockManagerMaster: Updated info of block broadcast_62_piece0
15/05/13 09:28:01 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:839
15/05/13 09:28:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 41 (MapPartitionsRDD[299] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:28:01 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks
15/05/13 09:28:01 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 41, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:28:01 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 265.2 MB)
15/05/13 09:28:01 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:28:01 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 41) in 263 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:28:01 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool 
15/05/13 09:28:01 INFO DAGScheduler: Stage 41 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.265 s
15/05/13 09:28:01 INFO DAGScheduler: Job 42 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.297236 s
15/05/13 09:28:01 INFO JobScheduler: Finished job streaming job 1431523680000 ms.0 from job set of time 1431523680000 ms
15/05/13 09:28:01 INFO JobScheduler: Total delay: 1.542 s for time 1431523680000 ms (execution: 1.292 s)
15/05/13 09:28:01 INFO MapPartitionsRDD: Removing RDD 274 from persistence list
15/05/13 09:28:01 INFO BlockManager: Removing RDD 274
15/05/13 09:28:01 INFO UnionRDD: Removing RDD 273 from persistence list
15/05/13 09:28:01 INFO BlockManager: Removing RDD 273
15/05/13 09:28:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523620000 ms: 1431523560000 ms
15/05/13 09:28:01 INFO JobGenerator: Checkpointing graph for time 1431523680000 ms
15/05/13 09:28:01 INFO DStreamGraph: Updating checkpoint data for time 1431523680000 ms
15/05/13 09:28:01 INFO DStreamGraph: Updated checkpoint data for time 1431523680000 ms
15/05/13 09:28:01 INFO CheckpointWriter: Saving checkpoint for time 1431523680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000'
15/05/13 09:28:03 INFO CheckpointWriter: Checkpoint for time 1431523680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000', took 7654 bytes and 2033 ms
15/05/13 09:28:03 INFO DStreamGraph: Clearing checkpoint data for time 1431523680000 ms
15/05/13 09:28:03 INFO DStreamGraph: Cleared checkpoint data for time 1431523680000 ms
15/05/13 09:28:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:29:00 INFO FileInputDStream: Finding new files took 60 ms
15/05/13 09:29:00 INFO FileInputDStream: New files at time 1431523740000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523647547.json
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=5800792, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 232.9 KB, free 259.7 MB)
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=6039324, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 34.9 KB, free 259.6 MB)
15/05/13 09:29:00 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:29:00 INFO BlockManagerMaster: Updated info of block broadcast_63_piece0
15/05/13 09:29:00 INFO SparkContext: Created broadcast 63 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:29:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:29:00 INFO JobScheduler: Added jobs for time 1431523740000 ms
15/05/13 09:29:00 INFO JobGenerator: Checkpointing graph for time 1431523740000 ms
15/05/13 09:29:00 INFO DStreamGraph: Updating checkpoint data for time 1431523740000 ms
15/05/13 09:29:00 INFO JobScheduler: Starting job streaming job 1431523740000 ms.0 from job set of time 1431523740000 ms
15/05/13 09:29:00 INFO DStreamGraph: Updated checkpoint data for time 1431523740000 ms
15/05/13 09:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431523740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000'
15/05/13 09:29:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:29:00 INFO DAGScheduler: Got job 43 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:29:00 INFO DAGScheduler: Final stage: Stage 42(reduce at JsonRDD.scala:51)
15/05/13 09:29:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:29:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:29:00 INFO DAGScheduler: Submitting Stage 42 (MapPartitionsRDD[306] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:29:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82855): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431523740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000'
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=6075032, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 5.9 KB, free 259.6 MB)
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=6081072, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 4.1 KB, free 259.6 MB)
15/05/13 09:29:00 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:29:00 INFO BlockManagerMaster: Updated info of block broadcast_64_piece0
15/05/13 09:29:00 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:839
15/05/13 09:29:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 42 (MapPartitionsRDD[306] at map at JsonRDD.scala:51)
15/05/13 09:29:00 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks
15/05/13 09:29:00 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 42, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:29:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523440000.bk
15/05/13 09:29:00 INFO CheckpointWriter: Checkpoint for time 1431523740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000', took 7664 bytes and 164 ms
15/05/13 09:29:00 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:29:00 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:29:00 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 42) in 446 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:29:00 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool 
15/05/13 09:29:00 INFO DAGScheduler: Stage 42 (reduce at JsonRDD.scala:51) finished in 0.455 s
15/05/13 09:29:00 INFO DAGScheduler: Job 43 finished: reduce at JsonRDD.scala:51, took 0.493472 s
15/05/13 09:29:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:29:00 INFO DAGScheduler: Got job 44 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:29:00 INFO DAGScheduler: Final stage: Stage 43(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:29:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:29:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:29:00 INFO DAGScheduler: Submitting Stage 43 (MapPartitionsRDD[313] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(20664) called with curMem=6085298, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 20.2 KB, free 259.6 MB)
15/05/13 09:29:00 INFO MemoryStore: ensureFreeSpace(10816) called with curMem=6105962, maxMem=278302556
15/05/13 09:29:00 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 10.6 KB, free 259.6 MB)
15/05/13 09:29:00 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 264.6 MB)
15/05/13 09:29:00 INFO BlockManagerMaster: Updated info of block broadcast_65_piece0
15/05/13 09:29:00 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:839
15/05/13 09:29:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 43 (MapPartitionsRDD[313] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:29:00 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks
15/05/13 09:29:00 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 43, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:29:01 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 265.2 MB)
15/05/13 09:29:01 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:29:01 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 43) in 273 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:29:01 INFO DAGScheduler: Stage 43 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.276 s
15/05/13 09:29:01 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool 
15/05/13 09:29:01 INFO DAGScheduler: Job 44 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.301876 s
15/05/13 09:29:01 INFO JobScheduler: Finished job streaming job 1431523740000 ms.0 from job set of time 1431523740000 ms
15/05/13 09:29:01 INFO JobScheduler: Total delay: 1.318 s for time 1431523740000 ms (execution: 1.075 s)
15/05/13 09:29:01 INFO MapPartitionsRDD: Removing RDD 288 from persistence list
15/05/13 09:29:01 INFO BlockManager: Removing RDD 288
15/05/13 09:29:01 INFO UnionRDD: Removing RDD 287 from persistence list
15/05/13 09:29:01 INFO BlockManager: Removing RDD 287
15/05/13 09:29:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523680000 ms: 1431523620000 ms
15/05/13 09:29:01 INFO JobGenerator: Checkpointing graph for time 1431523740000 ms
15/05/13 09:29:01 INFO DStreamGraph: Updating checkpoint data for time 1431523740000 ms
15/05/13 09:29:01 INFO DStreamGraph: Updated checkpoint data for time 1431523740000 ms
15/05/13 09:29:01 INFO CheckpointWriter: Saving checkpoint for time 1431523740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000'
15/05/13 09:29:01 INFO CheckpointWriter: Checkpoint for time 1431523740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000', took 7655 bytes and 69 ms
15/05/13 09:29:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523740000 ms
15/05/13 09:29:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523740000 ms
15/05/13 09:29:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:30:00 INFO FileInputDStream: Finding new files took 61 ms
15/05/13 09:30:00 INFO FileInputDStream: New files at time 1431523800000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523710401.json
15/05/13 09:30:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=6116778, maxMem=278302556
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 232.9 KB, free 259.3 MB)
15/05/13 09:30:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=6355310, maxMem=278302556
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 34.9 KB, free 259.3 MB)
15/05/13 09:30:00 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:30:00 INFO BlockManagerMaster: Updated info of block broadcast_66_piece0
15/05/13 09:30:00 INFO SparkContext: Created broadcast 66 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:30:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:30:00 INFO JobScheduler: Added jobs for time 1431523800000 ms
15/05/13 09:30:00 INFO JobGenerator: Checkpointing graph for time 1431523800000 ms
15/05/13 09:30:00 INFO DStreamGraph: Updating checkpoint data for time 1431523800000 ms
15/05/13 09:30:00 INFO JobScheduler: Starting job streaming job 1431523800000 ms.0 from job set of time 1431523800000 ms
15/05/13 09:30:00 INFO DStreamGraph: Updated checkpoint data for time 1431523800000 ms
15/05/13 09:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431523800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000'
15/05/13 09:30:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82868): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82868): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431523800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000'
15/05/13 09:30:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82870): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82870): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431523800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000'
15/05/13 09:30:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82872): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82872): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:30:00 WARN CheckpointWriter: Could not write checkpoint for time 1431523800000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000'
15/05/13 09:30:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:30:00 INFO DAGScheduler: Got job 45 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:30:00 INFO DAGScheduler: Final stage: Stage 44(reduce at JsonRDD.scala:51)
15/05/13 09:30:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:30:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:30:00 INFO DAGScheduler: Submitting Stage 44 (MapPartitionsRDD[320] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:30:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=6391018, maxMem=278302556
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 5.9 KB, free 259.3 MB)
15/05/13 09:30:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=6397058, maxMem=278302556
15/05/13 09:30:00 INFO BlockManager: Removing broadcast 64
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 4.1 KB, free 259.3 MB)
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_64
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_64 of size 6040 dropped from memory (free 271907314)
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_64_piece0
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_64_piece0 of size 4226 dropped from memory (free 271911540)
15/05/13 09:30:00 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:30:00 INFO BlockManagerMaster: Updated info of block broadcast_67_piece0
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_64_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:30:00 INFO BlockManagerMaster: Updated info of block broadcast_64_piece0
15/05/13 09:30:00 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:839
15/05/13 09:30:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 44 (MapPartitionsRDD[320] at map at JsonRDD.scala:51)
15/05/13 09:30:00 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks
15/05/13 09:30:00 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 44, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_64_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:30:00 INFO ContextCleaner: Cleaned broadcast 64
15/05/13 09:30:00 INFO BlockManager: Removing broadcast 62
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_62_piece0
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_62_piece0 of size 10815 dropped from memory (free 271922355)
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_62_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 264.6 MB)
15/05/13 09:30:00 INFO BlockManagerMaster: Updated info of block broadcast_62_piece0
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_62
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_62 of size 20664 dropped from memory (free 271943019)
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_62_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 265.1 MB)
15/05/13 09:30:00 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:30:00 INFO ContextCleaner: Cleaned broadcast 62
15/05/13 09:30:00 INFO BlockManager: Removing broadcast 65
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_65_piece0
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_65_piece0 of size 10816 dropped from memory (free 271953835)
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_65_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 264.6 MB)
15/05/13 09:30:00 INFO BlockManagerMaster: Updated info of block broadcast_65_piece0
15/05/13 09:30:00 INFO BlockManager: Removing block broadcast_65
15/05/13 09:30:00 INFO MemoryStore: Block broadcast_65 of size 20664 dropped from memory (free 271974499)
15/05/13 09:30:00 INFO BlockManagerInfo: Removed broadcast_65_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 265.1 MB)
15/05/13 09:30:00 INFO ContextCleaner: Cleaned broadcast 65
15/05/13 09:30:00 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:30:01 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 44) in 516 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:30:01 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool 
15/05/13 09:30:01 INFO DAGScheduler: Stage 44 (reduce at JsonRDD.scala:51) finished in 0.535 s
15/05/13 09:30:01 INFO DAGScheduler: Job 45 finished: reduce at JsonRDD.scala:51, took 0.619105 s
15/05/13 09:30:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:30:01 INFO DAGScheduler: Got job 46 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:30:01 INFO DAGScheduler: Final stage: Stage 45(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:30:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:30:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:30:01 INFO DAGScheduler: Submitting Stage 45 (MapPartitionsRDD[327] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:30:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=6328057, maxMem=278302556
15/05/13 09:30:01 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 20.4 KB, free 259.4 MB)
15/05/13 09:30:01 INFO MemoryStore: ensureFreeSpace(10908) called with curMem=6348977, maxMem=278302556
15/05/13 09:30:01 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 10.7 KB, free 259.3 MB)
15/05/13 09:30:01 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.6 MB)
15/05/13 09:30:01 INFO BlockManagerMaster: Updated info of block broadcast_68_piece0
15/05/13 09:30:01 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:839
15/05/13 09:30:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 45 (MapPartitionsRDD[327] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:30:01 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks
15/05/13 09:30:01 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 45, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:30:01 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:30:01 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:30:01 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 45) in 371 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:30:01 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool 
15/05/13 09:30:01 INFO DAGScheduler: Stage 45 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.374 s
15/05/13 09:30:01 INFO DAGScheduler: Job 46 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.409665 s
15/05/13 09:30:01 INFO JobScheduler: Finished job streaming job 1431523800000 ms.0 from job set of time 1431523800000 ms
15/05/13 09:30:01 INFO JobScheduler: Total delay: 1.728 s for time 1431523800000 ms (execution: 1.486 s)
15/05/13 09:30:01 INFO MapPartitionsRDD: Removing RDD 302 from persistence list
15/05/13 09:30:01 INFO BlockManager: Removing RDD 302
15/05/13 09:30:01 INFO UnionRDD: Removing RDD 301 from persistence list
15/05/13 09:30:01 INFO BlockManager: Removing RDD 301
15/05/13 09:30:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523740000 ms: 1431523680000 ms
15/05/13 09:30:01 INFO JobGenerator: Checkpointing graph for time 1431523800000 ms
15/05/13 09:30:01 INFO DStreamGraph: Updating checkpoint data for time 1431523800000 ms
15/05/13 09:30:01 INFO DStreamGraph: Updated checkpoint data for time 1431523800000 ms
15/05/13 09:30:01 INFO CheckpointWriter: Saving checkpoint for time 1431523800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000'
15/05/13 09:30:01 INFO CheckpointWriter: Checkpoint for time 1431523800000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000', took 7657 bytes and 83 ms
15/05/13 09:30:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523800000 ms
15/05/13 09:30:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523800000 ms
15/05/13 09:30:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:31:01 INFO FileInputDStream: Finding new files took 1113 ms
15/05/13 09:31:01 INFO FileInputDStream: New files at time 1431523860000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523776614.json
15/05/13 09:31:01 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=6359885, maxMem=278302556
15/05/13 09:31:01 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 232.9 KB, free 259.1 MB)
15/05/13 09:31:01 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=6598417, maxMem=278302556
15/05/13 09:31:01 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 34.9 KB, free 259.1 MB)
15/05/13 09:31:01 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:31:01 INFO BlockManagerMaster: Updated info of block broadcast_69_piece0
15/05/13 09:31:01 INFO SparkContext: Created broadcast 69 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:31:01 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:31:01 INFO JobScheduler: Added jobs for time 1431523860000 ms
15/05/13 09:31:01 INFO JobGenerator: Checkpointing graph for time 1431523860000 ms
15/05/13 09:31:01 INFO JobScheduler: Starting job streaming job 1431523860000 ms.0 from job set of time 1431523860000 ms
15/05/13 09:31:01 INFO DStreamGraph: Updating checkpoint data for time 1431523860000 ms
15/05/13 09:31:01 INFO DStreamGraph: Updated checkpoint data for time 1431523860000 ms
15/05/13 09:31:01 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:01 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:31:01 INFO DAGScheduler: Got job 47 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:31:01 INFO DAGScheduler: Final stage: Stage 46(reduce at JsonRDD.scala:51)
15/05/13 09:31:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:31:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:31:01 INFO DAGScheduler: Submitting Stage 46 (MapPartitionsRDD[334] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:31:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=6634126, maxMem=278302556
15/05/13 09:31:01 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 5.9 KB, free 259.1 MB)
15/05/13 09:31:01 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=6640166, maxMem=278302556
15/05/13 09:31:01 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 4.1 KB, free 259.1 MB)
15/05/13 09:31:01 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:31:01 INFO BlockManagerMaster: Updated info of block broadcast_70_piece0
15/05/13 09:31:01 INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:839
15/05/13 09:31:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 46 (MapPartitionsRDD[334] at map at JsonRDD.scala:51)
15/05/13 09:31:01 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks
15/05/13 09:31:01 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 46, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:31:01 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:31:01 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:31:01 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 46) in 381 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:31:01 INFO DAGScheduler: Stage 46 (reduce at JsonRDD.scala:51) finished in 0.395 s
15/05/13 09:31:01 INFO DAGScheduler: Job 47 finished: reduce at JsonRDD.scala:51, took 0.417592 s
15/05/13 09:31:01 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool 
15/05/13 09:31:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:31:01 INFO DAGScheduler: Got job 48 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:31:01 INFO DAGScheduler: Final stage: Stage 47(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:31:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:31:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:31:01 INFO DAGScheduler: Submitting Stage 47 (MapPartitionsRDD[341] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:31:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=6644392, maxMem=278302556
15/05/13 09:31:01 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 20.6 KB, free 259.1 MB)
15/05/13 09:31:01 INFO BlockManager: Removing broadcast 68
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_68
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_68 of size 20920 dropped from memory (free 271657988)
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_68_piece0
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_68_piece0 of size 10908 dropped from memory (free 271668896)
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_68_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.6 MB)
15/05/13 09:31:02 INFO BlockManagerMaster: Updated info of block broadcast_68_piece0
15/05/13 09:31:02 INFO MemoryStore: ensureFreeSpace(10955) called with curMem=6633660, maxMem=278302556
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 10.7 KB, free 259.1 MB)
15/05/13 09:31:02 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.6 MB)
15/05/13 09:31:02 INFO BlockManagerMaster: Updated info of block broadcast_71_piece0
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_68_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:31:02 INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:839
15/05/13 09:31:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 47 (MapPartitionsRDD[341] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:31:02 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks
15/05/13 09:31:02 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 47, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:31:02 INFO ContextCleaner: Cleaned broadcast 68
15/05/13 09:31:02 INFO BlockManager: Removing broadcast 67
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_67
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_67 of size 6040 dropped from memory (free 271663981)
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_67_piece0
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_67_piece0 of size 4224 dropped from memory (free 271668205)
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_67_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:31:02 INFO BlockManagerMaster: Updated info of block broadcast_67_piece0
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_67_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:31:02 INFO ContextCleaner: Cleaned broadcast 67
15/05/13 09:31:02 INFO BlockManager: Removing broadcast 70
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_70_piece0
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_70_piece0 of size 4226 dropped from memory (free 271672431)
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_70_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:31:02 INFO BlockManagerMaster: Updated info of block broadcast_70_piece0
15/05/13 09:31:02 INFO BlockManager: Removing block broadcast_70
15/05/13 09:31:02 INFO MemoryStore: Block broadcast_70 of size 6040 dropped from memory (free 271678471)
15/05/13 09:31:02 INFO BlockManagerInfo: Removed broadcast_70_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:31:02 INFO ContextCleaner: Cleaned broadcast 70
15/05/13 09:31:02 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:31:02 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:31:02 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82878): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:31:02 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82878): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:31:02 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:02 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 47) in 454 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:31:02 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool 
15/05/13 09:31:02 INFO DAGScheduler: Stage 47 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.457 s
15/05/13 09:31:02 INFO DAGScheduler: Job 48 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.530470 s
15/05/13 09:31:02 INFO JobScheduler: Finished job streaming job 1431523860000 ms.0 from job set of time 1431523860000 ms
15/05/13 09:31:02 INFO JobScheduler: Total delay: 2.555 s for time 1431523860000 ms (execution: 1.344 s)
15/05/13 09:31:02 INFO MapPartitionsRDD: Removing RDD 316 from persistence list
15/05/13 09:31:02 INFO BlockManager: Removing RDD 316
15/05/13 09:31:02 INFO UnionRDD: Removing RDD 315 from persistence list
15/05/13 09:31:02 INFO BlockManager: Removing RDD 315
15/05/13 09:31:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431523800000 ms: 1431523740000 ms
15/05/13 09:31:02 INFO JobGenerator: Checkpointing graph for time 1431523860000 ms
15/05/13 09:31:02 INFO DStreamGraph: Updating checkpoint data for time 1431523860000 ms
15/05/13 09:31:02 INFO DStreamGraph: Updated checkpoint data for time 1431523860000 ms
15/05/13 09:31:04 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82879): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:31:04 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82879): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:31:04 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:04 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82881): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:31:04 WARN CheckpointWriter: Could not write checkpoint for time 1431523860000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:04 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:04 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82883): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:31:04 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:04 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82885): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:31:04 INFO CheckpointWriter: Saving checkpoint for time 1431523860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:31:04 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82887): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:31:04 WARN CheckpointWriter: Could not write checkpoint for time 1431523860000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523860000'
15/05/13 09:32:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 09:32:00 INFO FileInputDStream: New files at time 1431523920000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523840514.json
15/05/13 09:32:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=6624085, maxMem=278302556
15/05/13 09:32:00 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 232.9 KB, free 258.9 MB)
15/05/13 09:32:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=6862617, maxMem=278302556
15/05/13 09:32:00 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 34.9 KB, free 258.8 MB)
15/05/13 09:32:00 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:32:00 INFO BlockManagerMaster: Updated info of block broadcast_72_piece0
15/05/13 09:32:00 INFO SparkContext: Created broadcast 72 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:32:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:32:00 INFO JobScheduler: Added jobs for time 1431523920000 ms
15/05/13 09:32:00 INFO JobGenerator: Checkpointing graph for time 1431523920000 ms
15/05/13 09:32:00 INFO JobScheduler: Starting job streaming job 1431523920000 ms.0 from job set of time 1431523920000 ms
15/05/13 09:32:00 INFO DStreamGraph: Updating checkpoint data for time 1431523920000 ms
15/05/13 09:32:00 INFO DStreamGraph: Updated checkpoint data for time 1431523920000 ms
15/05/13 09:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431523920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000'
15/05/13 09:32:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:32:00 INFO DAGScheduler: Got job 49 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:32:00 INFO DAGScheduler: Final stage: Stage 48(reduce at JsonRDD.scala:51)
15/05/13 09:32:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:32:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:32:00 INFO DAGScheduler: Submitting Stage 48 (MapPartitionsRDD[348] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:32:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=6898325, maxMem=278302556
15/05/13 09:32:00 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 5.9 KB, free 258.8 MB)
15/05/13 09:32:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=6904365, maxMem=278302556
15/05/13 09:32:00 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 4.1 KB, free 258.8 MB)
15/05/13 09:32:00 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:32:00 INFO BlockManagerMaster: Updated info of block broadcast_73_piece0
15/05/13 09:32:00 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:839
15/05/13 09:32:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 48 (MapPartitionsRDD[348] at map at JsonRDD.scala:51)
15/05/13 09:32:00 INFO TaskSchedulerImpl: Adding task set 48.0 with 1 tasks
15/05/13 09:32:00 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 48, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:32:00 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:32:00 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:32:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82892): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:32:01 INFO CheckpointWriter: Saving checkpoint for time 1431523920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000'
15/05/13 09:32:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82894): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:32:01 INFO CheckpointWriter: Saving checkpoint for time 1431523920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000'
15/05/13 09:32:01 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523620000
15/05/13 09:32:01 INFO CheckpointWriter: Checkpoint for time 1431523920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000', took 7660 bytes and 1459 ms
15/05/13 09:32:01 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 48) in 1461 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:32:01 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool 
15/05/13 09:32:01 INFO DAGScheduler: Stage 48 (reduce at JsonRDD.scala:51) finished in 1.478 s
15/05/13 09:32:01 INFO DAGScheduler: Job 49 finished: reduce at JsonRDD.scala:51, took 1.505900 s
15/05/13 09:32:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:32:02 INFO DAGScheduler: Got job 50 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:32:02 INFO DAGScheduler: Final stage: Stage 49(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:32:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:32:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:32:02 INFO DAGScheduler: Submitting Stage 49 (MapPartitionsRDD[355] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:32:02 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=6908591, maxMem=278302556
15/05/13 09:32:02 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 20.8 KB, free 258.8 MB)
15/05/13 09:32:02 INFO MemoryStore: ensureFreeSpace(11129) called with curMem=6929895, maxMem=278302556
15/05/13 09:32:02 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 10.9 KB, free 258.8 MB)
15/05/13 09:32:02 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 264.5 MB)
15/05/13 09:32:02 INFO BlockManagerMaster: Updated info of block broadcast_74_piece0
15/05/13 09:32:02 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:839
15/05/13 09:32:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 49 (MapPartitionsRDD[355] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:32:02 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks
15/05/13 09:32:02 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 49, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:32:02 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 264.7 MB)
15/05/13 09:32:02 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 49) in 179 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:32:02 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool 
15/05/13 09:32:02 INFO DAGScheduler: Stage 49 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.180 s
15/05/13 09:32:02 INFO DAGScheduler: Job 50 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.213523 s
15/05/13 09:32:02 INFO JobScheduler: Finished job streaming job 1431523920000 ms.0 from job set of time 1431523920000 ms
15/05/13 09:32:02 INFO JobScheduler: Total delay: 2.277 s for time 1431523920000 ms (execution: 2.073 s)
15/05/13 09:32:02 INFO MapPartitionsRDD: Removing RDD 330 from persistence list
15/05/13 09:32:02 INFO BlockManager: Removing RDD 330
15/05/13 09:32:02 INFO UnionRDD: Removing RDD 329 from persistence list
15/05/13 09:32:02 INFO BlockManager: Removing RDD 329
15/05/13 09:32:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431523860000 ms: 1431523800000 ms
15/05/13 09:32:02 INFO JobGenerator: Checkpointing graph for time 1431523920000 ms
15/05/13 09:32:02 INFO DStreamGraph: Updating checkpoint data for time 1431523920000 ms
15/05/13 09:32:02 INFO DStreamGraph: Updated checkpoint data for time 1431523920000 ms
15/05/13 09:32:02 INFO CheckpointWriter: Saving checkpoint for time 1431523920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000'
15/05/13 09:32:02 INFO CheckpointWriter: Checkpoint for time 1431523920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000', took 7653 bytes and 48 ms
15/05/13 09:32:02 INFO DStreamGraph: Clearing checkpoint data for time 1431523920000 ms
15/05/13 09:32:02 INFO DStreamGraph: Cleared checkpoint data for time 1431523920000 ms
15/05/13 09:32:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:33:00 INFO FileInputDStream: Finding new files took 37 ms
15/05/13 09:33:00 INFO FileInputDStream: New files at time 1431523980000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523906068.json
15/05/13 09:33:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=6941024, maxMem=278302556
15/05/13 09:33:00 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 232.9 KB, free 258.6 MB)
15/05/13 09:33:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=7179556, maxMem=278302556
15/05/13 09:33:00 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 34.9 KB, free 258.5 MB)
15/05/13 09:33:00 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:33:00 INFO BlockManagerMaster: Updated info of block broadcast_75_piece0
15/05/13 09:33:00 INFO SparkContext: Created broadcast 75 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:33:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:33:00 INFO JobScheduler: Added jobs for time 1431523980000 ms
15/05/13 09:33:00 INFO JobScheduler: Starting job streaming job 1431523980000 ms.0 from job set of time 1431523980000 ms
15/05/13 09:33:00 INFO JobGenerator: Checkpointing graph for time 1431523980000 ms
15/05/13 09:33:00 INFO DStreamGraph: Updating checkpoint data for time 1431523980000 ms
15/05/13 09:33:00 INFO DStreamGraph: Updated checkpoint data for time 1431523980000 ms
15/05/13 09:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431523980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000'
15/05/13 09:33:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82901): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:33:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82901): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431523980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000'
15/05/13 09:33:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:33:00 INFO DAGScheduler: Got job 51 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:33:00 INFO DAGScheduler: Final stage: Stage 50(reduce at JsonRDD.scala:51)
15/05/13 09:33:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:33:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:33:00 INFO DAGScheduler: Submitting Stage 50 (MapPartitionsRDD[362] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:33:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82903): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431523980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000'
15/05/13 09:33:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=7215265, maxMem=278302556
15/05/13 09:33:00 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 5.9 KB, free 258.5 MB)
15/05/13 09:33:00 INFO MemoryStore: ensureFreeSpace(4229) called with curMem=7221305, maxMem=278302556
15/05/13 09:33:00 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 4.1 KB, free 258.5 MB)
15/05/13 09:33:00 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:33:00 INFO BlockManagerMaster: Updated info of block broadcast_76_piece0
15/05/13 09:33:00 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:839
15/05/13 09:33:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 50 (MapPartitionsRDD[362] at map at JsonRDD.scala:51)
15/05/13 09:33:00 INFO TaskSchedulerImpl: Adding task set 50.0 with 1 tasks
15/05/13 09:33:00 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 50, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:33:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523680000.bk
15/05/13 09:33:00 INFO CheckpointWriter: Checkpoint for time 1431523980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000', took 7664 bytes and 164 ms
15/05/13 09:33:00 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:33:00 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:33:01 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 50) in 628 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:33:01 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool 
15/05/13 09:33:01 INFO DAGScheduler: Stage 50 (reduce at JsonRDD.scala:51) finished in 0.632 s
15/05/13 09:33:01 INFO DAGScheduler: Job 51 finished: reduce at JsonRDD.scala:51, took 0.685346 s
15/05/13 09:33:01 INFO BlockManager: Removing broadcast 73
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_73
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_73 of size 6040 dropped from memory (free 271083062)
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_73_piece0
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_73_piece0 of size 4226 dropped from memory (free 271087288)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_73_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:33:01 INFO BlockManagerMaster: Updated info of block broadcast_73_piece0
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_73_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:33:01 INFO ContextCleaner: Cleaned broadcast 73
15/05/13 09:33:01 INFO BlockManager: Removing broadcast 71
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_71_piece0
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_71_piece0 of size 10955 dropped from memory (free 271098243)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_71_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.5 MB)
15/05/13 09:33:01 INFO BlockManagerMaster: Updated info of block broadcast_71_piece0
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_71
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_71 of size 21096 dropped from memory (free 271119339)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_71_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 265.1 MB)
15/05/13 09:33:01 INFO ContextCleaner: Cleaned broadcast 71
15/05/13 09:33:01 INFO BlockManager: Removing broadcast 76
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_76_piece0
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_76_piece0 of size 4229 dropped from memory (free 271123568)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_76_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:33:01 INFO BlockManagerMaster: Updated info of block broadcast_76_piece0
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_76
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_76 of size 6040 dropped from memory (free 271129608)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_76_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:33:01 INFO ContextCleaner: Cleaned broadcast 76
15/05/13 09:33:01 INFO BlockManager: Removing broadcast 74
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_74_piece0
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_74_piece0 of size 11129 dropped from memory (free 271140737)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_74_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 264.5 MB)
15/05/13 09:33:01 INFO BlockManagerMaster: Updated info of block broadcast_74_piece0
15/05/13 09:33:01 INFO BlockManager: Removing block broadcast_74
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_74 of size 21304 dropped from memory (free 271162041)
15/05/13 09:33:01 INFO BlockManagerInfo: Removed broadcast_74_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 264.7 MB)
15/05/13 09:33:01 INFO ContextCleaner: Cleaned broadcast 74
15/05/13 09:33:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:33:01 INFO DAGScheduler: Got job 52 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:33:01 INFO DAGScheduler: Final stage: Stage 51(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:33:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:33:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:33:01 INFO DAGScheduler: Submitting Stage 51 (MapPartitionsRDD[369] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:33:01 INFO MemoryStore: ensureFreeSpace(21272) called with curMem=7140515, maxMem=278302556
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 20.8 KB, free 258.6 MB)
15/05/13 09:33:01 INFO MemoryStore: ensureFreeSpace(11057) called with curMem=7161787, maxMem=278302556
15/05/13 09:33:01 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 10.8 KB, free 258.6 MB)
15/05/13 09:33:01 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 264.5 MB)
15/05/13 09:33:01 INFO BlockManagerMaster: Updated info of block broadcast_77_piece0
15/05/13 09:33:01 INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:839
15/05/13 09:33:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 51 (MapPartitionsRDD[369] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:33:01 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks
15/05/13 09:33:01 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 51, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:33:01 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 264.7 MB)
15/05/13 09:33:01 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 51) in 157 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:33:01 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool 
15/05/13 09:33:01 INFO DAGScheduler: Stage 51 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.159 s
15/05/13 09:33:01 INFO DAGScheduler: Job 52 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.190016 s
15/05/13 09:33:01 INFO JobScheduler: Finished job streaming job 1431523980000 ms.0 from job set of time 1431523980000 ms
15/05/13 09:33:01 INFO JobScheduler: Total delay: 1.511 s for time 1431523980000 ms (execution: 1.278 s)
15/05/13 09:33:01 INFO MapPartitionsRDD: Removing RDD 344 from persistence list
15/05/13 09:33:01 INFO BlockManager: Removing RDD 344
15/05/13 09:33:01 INFO UnionRDD: Removing RDD 343 from persistence list
15/05/13 09:33:01 INFO BlockManager: Removing RDD 343
15/05/13 09:33:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523920000 ms: 1431523860000 ms
15/05/13 09:33:01 INFO JobGenerator: Checkpointing graph for time 1431523980000 ms
15/05/13 09:33:01 INFO DStreamGraph: Updating checkpoint data for time 1431523980000 ms
15/05/13 09:33:01 INFO DStreamGraph: Updated checkpoint data for time 1431523980000 ms
15/05/13 09:33:01 INFO CheckpointWriter: Saving checkpoint for time 1431523980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000'
15/05/13 09:33:01 INFO CheckpointWriter: Checkpoint for time 1431523980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000', took 7657 bytes and 62 ms
15/05/13 09:33:01 INFO DStreamGraph: Clearing checkpoint data for time 1431523980000 ms
15/05/13 09:33:01 INFO DStreamGraph: Cleared checkpoint data for time 1431523980000 ms
15/05/13 09:33:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:34:00 INFO FileInputDStream: Finding new files took 37 ms
15/05/13 09:34:00 INFO FileInputDStream: New files at time 1431524040000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431523969049.json
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=7172844, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 232.9 KB, free 258.3 MB)
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=7411376, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 34.9 KB, free 258.3 MB)
15/05/13 09:34:00 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:34:00 INFO BlockManagerMaster: Updated info of block broadcast_78_piece0
15/05/13 09:34:00 INFO SparkContext: Created broadcast 78 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:34:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:34:00 INFO JobScheduler: Added jobs for time 1431524040000 ms
15/05/13 09:34:00 INFO JobGenerator: Checkpointing graph for time 1431524040000 ms
15/05/13 09:34:00 INFO DStreamGraph: Updating checkpoint data for time 1431524040000 ms
15/05/13 09:34:00 INFO JobScheduler: Starting job streaming job 1431524040000 ms.0 from job set of time 1431524040000 ms
15/05/13 09:34:00 INFO DStreamGraph: Updated checkpoint data for time 1431524040000 ms
15/05/13 09:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431524040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000'
15/05/13 09:34:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:34:00 INFO DAGScheduler: Got job 53 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:34:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82911): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:34:00 INFO DAGScheduler: Final stage: Stage 52(reduce at JsonRDD.scala:51)
15/05/13 09:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:34:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82911): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431524040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000'
15/05/13 09:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:34:00 INFO DAGScheduler: Submitting Stage 52 (MapPartitionsRDD[376] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=7447085, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 5.9 KB, free 258.3 MB)
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(4223) called with curMem=7453125, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 4.1 KB, free 258.3 MB)
15/05/13 09:34:00 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:34:00 INFO BlockManagerMaster: Updated info of block broadcast_79_piece0
15/05/13 09:34:00 INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:839
15/05/13 09:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 52 (MapPartitionsRDD[376] at map at JsonRDD.scala:51)
15/05/13 09:34:00 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks
15/05/13 09:34:00 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 52, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:34:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523740000.bk
15/05/13 09:34:00 INFO CheckpointWriter: Checkpoint for time 1431524040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000', took 7665 bytes and 134 ms
15/05/13 09:34:00 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:34:00 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:34:00 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 52) in 438 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:34:00 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool 
15/05/13 09:34:00 INFO DAGScheduler: Stage 52 (reduce at JsonRDD.scala:51) finished in 0.448 s
15/05/13 09:34:00 INFO DAGScheduler: Job 53 finished: reduce at JsonRDD.scala:51, took 0.486301 s
15/05/13 09:34:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:34:00 INFO DAGScheduler: Got job 54 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:34:00 INFO DAGScheduler: Final stage: Stage 53(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:34:00 INFO DAGScheduler: Submitting Stage 53 (MapPartitionsRDD[383] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(16272) called with curMem=7457348, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 15.9 KB, free 258.3 MB)
15/05/13 09:34:00 INFO MemoryStore: ensureFreeSpace(9376) called with curMem=7473620, maxMem=278302556
15/05/13 09:34:00 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 9.2 KB, free 258.3 MB)
15/05/13 09:34:00 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 9.2 KB, free: 264.5 MB)
15/05/13 09:34:00 INFO BlockManagerMaster: Updated info of block broadcast_80_piece0
15/05/13 09:34:00 INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:839
15/05/13 09:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 53 (MapPartitionsRDD[383] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:34:00 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks
15/05/13 09:34:00 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 53, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:34:01 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 9.2 KB, free: 264.7 MB)
15/05/13 09:34:01 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:34:01 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 53) in 308 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:34:01 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool 
15/05/13 09:34:01 INFO DAGScheduler: Stage 53 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.311 s
15/05/13 09:34:01 INFO DAGScheduler: Job 54 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.343160 s
15/05/13 09:34:01 INFO JobScheduler: Finished job streaming job 1431524040000 ms.0 from job set of time 1431524040000 ms
15/05/13 09:34:01 INFO JobScheduler: Total delay: 1.318 s for time 1431524040000 ms (execution: 1.146 s)
15/05/13 09:34:01 INFO MapPartitionsRDD: Removing RDD 358 from persistence list
15/05/13 09:34:01 INFO BlockManager: Removing RDD 358
15/05/13 09:34:01 INFO UnionRDD: Removing RDD 357 from persistence list
15/05/13 09:34:01 INFO BlockManager: Removing RDD 357
15/05/13 09:34:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431523980000 ms: 1431523920000 ms
15/05/13 09:34:01 INFO JobGenerator: Checkpointing graph for time 1431524040000 ms
15/05/13 09:34:01 INFO DStreamGraph: Updating checkpoint data for time 1431524040000 ms
15/05/13 09:34:01 INFO DStreamGraph: Updated checkpoint data for time 1431524040000 ms
15/05/13 09:34:01 INFO CheckpointWriter: Saving checkpoint for time 1431524040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000'
15/05/13 09:34:01 INFO CheckpointWriter: Checkpoint for time 1431524040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000', took 7656 bytes and 54 ms
15/05/13 09:34:01 INFO DStreamGraph: Clearing checkpoint data for time 1431524040000 ms
15/05/13 09:34:01 INFO DStreamGraph: Cleared checkpoint data for time 1431524040000 ms
15/05/13 09:34:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:35:00 INFO FileInputDStream: Finding new files took 28 ms
15/05/13 09:35:00 INFO FileInputDStream: New files at time 1431524100000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524030490.json
15/05/13 09:35:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=7482996, maxMem=278302556
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 232.9 KB, free 258.0 MB)
15/05/13 09:35:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=7721528, maxMem=278302556
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 34.9 KB, free 258.0 MB)
15/05/13 09:35:00 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.4 MB)
15/05/13 09:35:00 INFO BlockManagerMaster: Updated info of block broadcast_81_piece0
15/05/13 09:35:00 INFO SparkContext: Created broadcast 81 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:35:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:35:00 INFO JobScheduler: Added jobs for time 1431524100000 ms
15/05/13 09:35:00 INFO JobScheduler: Starting job streaming job 1431524100000 ms.0 from job set of time 1431524100000 ms
15/05/13 09:35:00 INFO JobGenerator: Checkpointing graph for time 1431524100000 ms
15/05/13 09:35:00 INFO DStreamGraph: Updating checkpoint data for time 1431524100000 ms
15/05/13 09:35:00 INFO DStreamGraph: Updated checkpoint data for time 1431524100000 ms
15/05/13 09:35:00 INFO CheckpointWriter: Saving checkpoint for time 1431524100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000'
15/05/13 09:35:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:35:00 INFO DAGScheduler: Got job 55 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:35:00 INFO DAGScheduler: Final stage: Stage 54(reduce at JsonRDD.scala:51)
15/05/13 09:35:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:35:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:35:00 INFO DAGScheduler: Submitting Stage 54 (MapPartitionsRDD[390] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:35:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=7757237, maxMem=278302556
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 5.9 KB, free 258.0 MB)
15/05/13 09:35:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=7763277, maxMem=278302556
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 4.1 KB, free 258.0 MB)
15/05/13 09:35:00 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:35:00 INFO BlockManager: Removing broadcast 77
15/05/13 09:35:00 INFO BlockManagerMaster: Updated info of block broadcast_82_piece0
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_77
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_77 of size 21272 dropped from memory (free 270556324)
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_77_piece0
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_77_piece0 of size 11057 dropped from memory (free 270567381)
15/05/13 09:35:00 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:839
15/05/13 09:35:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 54 (MapPartitionsRDD[390] at map at JsonRDD.scala:51)
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_77_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 264.4 MB)
15/05/13 09:35:00 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks
15/05/13 09:35:00 INFO BlockManagerMaster: Updated info of block broadcast_77_piece0
15/05/13 09:35:00 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 54, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_77_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 264.7 MB)
15/05/13 09:35:00 INFO ContextCleaner: Cleaned broadcast 77
15/05/13 09:35:00 INFO BlockManager: Removing broadcast 80
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_80
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_80 of size 16272 dropped from memory (free 270583653)
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_80_piece0
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_80_piece0 of size 9376 dropped from memory (free 270593029)
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_80_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 9.2 KB, free: 264.4 MB)
15/05/13 09:35:00 INFO BlockManagerMaster: Updated info of block broadcast_80_piece0
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_80_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 9.2 KB, free: 264.7 MB)
15/05/13 09:35:00 INFO ContextCleaner: Cleaned broadcast 80
15/05/13 09:35:00 INFO BlockManager: Removing broadcast 79
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_79_piece0
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_79_piece0 of size 4223 dropped from memory (free 270597252)
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_79_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:35:00 INFO BlockManagerMaster: Updated info of block broadcast_79_piece0
15/05/13 09:35:00 INFO BlockManager: Removing block broadcast_79
15/05/13 09:35:00 INFO MemoryStore: Block broadcast_79 of size 6040 dropped from memory (free 270603292)
15/05/13 09:35:00 INFO BlockManagerInfo: Removed broadcast_79_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:35:00 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:35:00 INFO ContextCleaner: Cleaned broadcast 79
15/05/13 09:35:00 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.1 MB)
15/05/13 09:35:01 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 54) in 943 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:35:01 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool 
15/05/13 09:35:01 INFO DAGScheduler: Stage 54 (reduce at JsonRDD.scala:51) finished in 0.958 s
15/05/13 09:35:01 INFO DAGScheduler: Job 55 finished: reduce at JsonRDD.scala:51, took 1.031928 s
15/05/13 09:35:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:35:01 INFO DAGScheduler: Got job 56 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:35:01 INFO DAGScheduler: Final stage: Stage 55(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:35:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:35:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:35:01 INFO DAGScheduler: Submitting Stage 55 (MapPartitionsRDD[397] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:35:01 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=7699264, maxMem=278302556
15/05/13 09:35:01 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 21.0 KB, free 258.0 MB)
15/05/13 09:35:01 INFO MemoryStore: ensureFreeSpace(11285) called with curMem=7720768, maxMem=278302556
15/05/13 09:35:01 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 11.0 KB, free 258.0 MB)
15/05/13 09:35:01 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 264.4 MB)
15/05/13 09:35:01 INFO BlockManagerMaster: Updated info of block broadcast_83_piece0
15/05/13 09:35:01 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:839
15/05/13 09:35:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 55 (MapPartitionsRDD[397] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:35:01 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks
15/05/13 09:35:01 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 55, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:35:01 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 11.0 KB, free: 265.1 MB)
15/05/13 09:35:01 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 55) in 206 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:35:01 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool 
15/05/13 09:35:01 INFO DAGScheduler: Stage 55 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.206 s
15/05/13 09:35:01 INFO DAGScheduler: Job 56 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.239192 s
15/05/13 09:35:01 INFO JobScheduler: Finished job streaming job 1431524100000 ms.0 from job set of time 1431524100000 ms
15/05/13 09:35:01 INFO JobScheduler: Total delay: 1.742 s for time 1431524100000 ms (execution: 1.627 s)
15/05/13 09:35:01 INFO MapPartitionsRDD: Removing RDD 372 from persistence list
15/05/13 09:35:01 INFO BlockManager: Removing RDD 372
15/05/13 09:35:01 INFO UnionRDD: Removing RDD 371 from persistence list
15/05/13 09:35:01 INFO BlockManager: Removing RDD 371
15/05/13 09:35:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524040000 ms: 1431523980000 ms
15/05/13 09:35:01 INFO JobGenerator: Checkpointing graph for time 1431524100000 ms
15/05/13 09:35:01 INFO DStreamGraph: Updating checkpoint data for time 1431524100000 ms
15/05/13 09:35:01 INFO DStreamGraph: Updated checkpoint data for time 1431524100000 ms
15/05/13 09:35:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82927): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:35:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82927): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:35:01 INFO CheckpointWriter: Saving checkpoint for time 1431524100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000'
15/05/13 09:35:02 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000
15/05/13 09:35:02 INFO CheckpointWriter: Checkpoint for time 1431524100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000', took 7665 bytes and 1955 ms
15/05/13 09:35:02 INFO CheckpointWriter: Saving checkpoint for time 1431524100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000'
15/05/13 09:35:02 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82931): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:35:02 INFO CheckpointWriter: Saving checkpoint for time 1431524100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000'
15/05/13 09:35:02 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82933): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:35:02 INFO CheckpointWriter: Saving checkpoint for time 1431524100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000'
15/05/13 09:35:02 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000.bk
15/05/13 09:35:02 INFO CheckpointWriter: Checkpoint for time 1431524100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524100000', took 7658 bytes and 244 ms
15/05/13 09:35:02 INFO DStreamGraph: Clearing checkpoint data for time 1431524100000 ms
15/05/13 09:35:02 INFO DStreamGraph: Cleared checkpoint data for time 1431524100000 ms
15/05/13 09:35:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:36:00 INFO FileInputDStream: Finding new files took 51 ms
15/05/13 09:36:00 INFO FileInputDStream: New files at time 1431524160000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524095454.json
15/05/13 09:36:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=7732053, maxMem=278302556
15/05/13 09:36:00 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 232.9 KB, free 257.8 MB)
15/05/13 09:36:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=7970585, maxMem=278302556
15/05/13 09:36:00 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 34.9 KB, free 257.8 MB)
15/05/13 09:36:00 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.4 MB)
15/05/13 09:36:00 INFO BlockManagerMaster: Updated info of block broadcast_84_piece0
15/05/13 09:36:00 INFO SparkContext: Created broadcast 84 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:36:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:36:00 INFO JobScheduler: Added jobs for time 1431524160000 ms
15/05/13 09:36:00 INFO JobGenerator: Checkpointing graph for time 1431524160000 ms
15/05/13 09:36:00 INFO DStreamGraph: Updating checkpoint data for time 1431524160000 ms
15/05/13 09:36:00 INFO DStreamGraph: Updated checkpoint data for time 1431524160000 ms
15/05/13 09:36:00 INFO JobScheduler: Starting job streaming job 1431524160000 ms.0 from job set of time 1431524160000 ms
15/05/13 09:36:00 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:36:00 INFO DAGScheduler: Got job 57 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:36:00 INFO DAGScheduler: Final stage: Stage 56(reduce at JsonRDD.scala:51)
15/05/13 09:36:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:36:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:36:00 INFO DAGScheduler: Submitting Stage 56 (MapPartitionsRDD[404] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:36:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=8006293, maxMem=278302556
15/05/13 09:36:00 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 5.9 KB, free 257.8 MB)
15/05/13 09:36:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=8012333, maxMem=278302556
15/05/13 09:36:00 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 4.1 KB, free 257.8 MB)
15/05/13 09:36:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:36:00 INFO BlockManagerMaster: Updated info of block broadcast_85_piece0
15/05/13 09:36:00 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:839
15/05/13 09:36:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 56 (MapPartitionsRDD[404] at map at JsonRDD.scala:51)
15/05/13 09:36:00 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks
15/05/13 09:36:00 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 56, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:36:00 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:36:00 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:36:02 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 56) in 2158 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:36:02 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool 
15/05/13 09:36:02 INFO DAGScheduler: Stage 56 (reduce at JsonRDD.scala:51) finished in 2.171 s
15/05/13 09:36:02 INFO DAGScheduler: Job 57 finished: reduce at JsonRDD.scala:51, took 2.196573 s
15/05/13 09:36:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:36:02 INFO DAGScheduler: Got job 58 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:36:02 INFO DAGScheduler: Final stage: Stage 57(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:36:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:36:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:36:02 INFO DAGScheduler: Submitting Stage 57 (MapPartitionsRDD[411] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:36:02 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=8016559, maxMem=278302556
15/05/13 09:36:02 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 20.6 KB, free 257.7 MB)
15/05/13 09:36:02 INFO MemoryStore: ensureFreeSpace(10960) called with curMem=8037655, maxMem=278302556
15/05/13 09:36:02 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 10.7 KB, free 257.7 MB)
15/05/13 09:36:02 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.4 MB)
15/05/13 09:36:02 INFO BlockManagerMaster: Updated info of block broadcast_86_piece0
15/05/13 09:36:02 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:839
15/05/13 09:36:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 57 (MapPartitionsRDD[411] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:36:02 INFO TaskSchedulerImpl: Adding task set 57.0 with 1 tasks
15/05/13 09:36:02 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 57, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:36:02 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:36:02 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:36:03 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 57) in 402 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:36:03 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool 
15/05/13 09:36:03 INFO DAGScheduler: Stage 57 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.405 s
15/05/13 09:36:03 INFO DAGScheduler: Job 58 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.434759 s
15/05/13 09:36:03 INFO JobScheduler: Finished job streaming job 1431524160000 ms.0 from job set of time 1431524160000 ms
15/05/13 09:36:03 INFO JobScheduler: Total delay: 3.152 s for time 1431524160000 ms (execution: 2.930 s)
15/05/13 09:36:03 INFO MapPartitionsRDD: Removing RDD 386 from persistence list
15/05/13 09:36:03 INFO BlockManager: Removing RDD 386
15/05/13 09:36:03 INFO UnionRDD: Removing RDD 385 from persistence list
15/05/13 09:36:03 INFO BlockManager: Removing RDD 385
15/05/13 09:36:03 INFO FileInputDStream: Cleared 1 old files that were older than 1431524100000 ms: 1431524040000 ms
15/05/13 09:36:03 INFO JobGenerator: Checkpointing graph for time 1431524160000 ms
15/05/13 09:36:03 INFO DStreamGraph: Updating checkpoint data for time 1431524160000 ms
15/05/13 09:36:03 INFO DStreamGraph: Updated checkpoint data for time 1431524160000 ms
15/05/13 09:36:10 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82940): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:36:10 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82940): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:36:10 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82941): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:36:14 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82941): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:36:14 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82943): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:36:14 WARN CheckpointWriter: Could not write checkpoint for time 1431524160000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82945): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:36:14 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82947): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:36:14 INFO CheckpointWriter: Saving checkpoint for time 1431524160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000'
15/05/13 09:36:14 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523800000
15/05/13 09:36:14 INFO BlockManager: Removing broadcast 83
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_83
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_83 of size 21504 dropped from memory (free 270275445)
15/05/13 09:36:14 INFO CheckpointWriter: Checkpoint for time 1431524160000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000', took 7655 bytes and 165 ms
15/05/13 09:36:14 INFO DStreamGraph: Clearing checkpoint data for time 1431524160000 ms
15/05/13 09:36:14 INFO DStreamGraph: Cleared checkpoint data for time 1431524160000 ms
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_83_piece0
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_83_piece0 of size 11285 dropped from memory (free 270286730)
15/05/13 09:36:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_83_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 264.4 MB)
15/05/13 09:36:14 INFO BlockManagerMaster: Updated info of block broadcast_83_piece0
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_83_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 11.0 KB, free: 265.0 MB)
15/05/13 09:36:14 INFO ContextCleaner: Cleaned broadcast 83
15/05/13 09:36:14 INFO BlockManager: Removing broadcast 82
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_82
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_82 of size 6040 dropped from memory (free 270292770)
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_82_piece0
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_82_piece0 of size 4227 dropped from memory (free 270296997)
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_82_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:36:14 INFO BlockManagerMaster: Updated info of block broadcast_82_piece0
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_82_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:36:14 INFO ContextCleaner: Cleaned broadcast 82
15/05/13 09:36:14 INFO BlockManager: Removing broadcast 86
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_86_piece0
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_86_piece0 of size 10960 dropped from memory (free 270307957)
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_86_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.4 MB)
15/05/13 09:36:14 INFO BlockManagerMaster: Updated info of block broadcast_86_piece0
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_86
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_86 of size 21096 dropped from memory (free 270329053)
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_86_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:36:14 INFO ContextCleaner: Cleaned broadcast 86
15/05/13 09:36:14 INFO BlockManager: Removing broadcast 85
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_85_piece0
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_85_piece0 of size 4226 dropped from memory (free 270333279)
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_85_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:36:14 INFO BlockManagerMaster: Updated info of block broadcast_85_piece0
15/05/13 09:36:14 INFO BlockManager: Removing block broadcast_85
15/05/13 09:36:14 INFO MemoryStore: Block broadcast_85 of size 6040 dropped from memory (free 270339319)
15/05/13 09:36:14 INFO BlockManagerInfo: Removed broadcast_85_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:36:14 INFO ContextCleaner: Cleaned broadcast 85
15/05/13 09:37:00 INFO FileInputDStream: Finding new files took 84 ms
15/05/13 09:37:00 INFO FileInputDStream: New files at time 1431524220000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524156260.json
15/05/13 09:37:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=7963237, maxMem=278302556
15/05/13 09:37:00 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 232.9 KB, free 257.6 MB)
15/05/13 09:37:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=8201769, maxMem=278302556
15/05/13 09:37:00 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 34.9 KB, free 257.6 MB)
15/05/13 09:37:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.4 MB)
15/05/13 09:37:00 INFO BlockManagerMaster: Updated info of block broadcast_87_piece0
15/05/13 09:37:00 INFO SparkContext: Created broadcast 87 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:37:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:37:00 INFO JobScheduler: Added jobs for time 1431524220000 ms
15/05/13 09:37:00 INFO JobGenerator: Checkpointing graph for time 1431524220000 ms
15/05/13 09:37:00 INFO JobScheduler: Starting job streaming job 1431524220000 ms.0 from job set of time 1431524220000 ms
15/05/13 09:37:00 INFO DStreamGraph: Updating checkpoint data for time 1431524220000 ms
15/05/13 09:37:00 INFO DStreamGraph: Updated checkpoint data for time 1431524220000 ms
15/05/13 09:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:37:00 INFO DAGScheduler: Got job 59 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:37:00 INFO DAGScheduler: Final stage: Stage 58(reduce at JsonRDD.scala:51)
15/05/13 09:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:37:00 INFO DAGScheduler: Submitting Stage 58 (MapPartitionsRDD[418] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:37:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=8237477, maxMem=278302556
15/05/13 09:37:00 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 5.9 KB, free 257.5 MB)
15/05/13 09:37:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=8243517, maxMem=278302556
15/05/13 09:37:00 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 4.1 KB, free 257.5 MB)
15/05/13 09:37:00 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:37:00 INFO BlockManagerMaster: Updated info of block broadcast_88_piece0
15/05/13 09:37:00 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:839
15/05/13 09:37:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 58 (MapPartitionsRDD[418] at map at JsonRDD.scala:51)
15/05/13 09:37:00 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks
15/05/13 09:37:00 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 58, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:37:00 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:37:00 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:37:00 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 58) in 428 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:37:00 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool 
15/05/13 09:37:00 INFO DAGScheduler: Stage 58 (reduce at JsonRDD.scala:51) finished in 0.437 s
15/05/13 09:37:00 INFO DAGScheduler: Job 59 finished: reduce at JsonRDD.scala:51, took 0.467481 s
15/05/13 09:37:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:37:00 INFO DAGScheduler: Got job 60 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:37:00 INFO DAGScheduler: Final stage: Stage 59(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:37:00 INFO DAGScheduler: Submitting Stage 59 (MapPartitionsRDD[425] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:37:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=8247744, maxMem=278302556
15/05/13 09:37:00 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 20.6 KB, free 257.5 MB)
15/05/13 09:37:01 INFO MemoryStore: ensureFreeSpace(10949) called with curMem=8268840, maxMem=278302556
15/05/13 09:37:01 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 10.7 KB, free 257.5 MB)
15/05/13 09:37:01 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.4 MB)
15/05/13 09:37:01 INFO BlockManagerMaster: Updated info of block broadcast_89_piece0
15/05/13 09:37:01 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:839
15/05/13 09:37:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 59 (MapPartitionsRDD[425] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:37:01 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks
15/05/13 09:37:01 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 59, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:37:01 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 264.6 MB)
15/05/13 09:37:01 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 59) in 161 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:37:01 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool 
15/05/13 09:37:01 INFO DAGScheduler: Stage 59 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.162 s
15/05/13 09:37:01 INFO DAGScheduler: Job 60 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.227566 s
15/05/13 09:37:01 INFO JobScheduler: Finished job streaming job 1431524220000 ms.0 from job set of time 1431524220000 ms
15/05/13 09:37:01 INFO JobScheduler: Total delay: 1.232 s for time 1431524220000 ms (execution: 0.979 s)
15/05/13 09:37:01 INFO MapPartitionsRDD: Removing RDD 400 from persistence list
15/05/13 09:37:01 INFO BlockManager: Removing RDD 400
15/05/13 09:37:01 INFO UnionRDD: Removing RDD 399 from persistence list
15/05/13 09:37:01 INFO BlockManager: Removing RDD 399
15/05/13 09:37:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524160000 ms: 1431524100000 ms
15/05/13 09:37:01 INFO JobGenerator: Checkpointing graph for time 1431524220000 ms
15/05/13 09:37:01 INFO DStreamGraph: Updating checkpoint data for time 1431524220000 ms
15/05/13 09:37:01 INFO DStreamGraph: Updated checkpoint data for time 1431524220000 ms
15/05/13 09:37:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82952): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82952): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:01 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:06 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82953): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:06 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82953): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:06 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:07 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82955): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:37:07 WARN CheckpointWriter: Could not write checkpoint for time 1431524220000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:07 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:08 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82956): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:08 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82956): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:08 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:09 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82959): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:37:09 INFO CheckpointWriter: Saving checkpoint for time 1431524220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:37:12 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82960): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:12 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82960): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:37:12 WARN CheckpointWriter: Could not write checkpoint for time 1431524220000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000'
15/05/13 09:38:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 09:38:00 INFO FileInputDStream: New files at time 1431524280000 ms:

15/05/13 09:38:00 INFO JobScheduler: Starting job streaming job 1431524280000 ms.0 from job set of time 1431524280000 ms
15/05/13 09:38:00 INFO JobScheduler: Added jobs for time 1431524280000 ms
15/05/13 09:38:00 INFO JobGenerator: Checkpointing graph for time 1431524280000 ms
15/05/13 09:38:00 INFO DStreamGraph: Updating checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO DStreamGraph: Updated checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431524280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000'
15/05/13 09:38:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82963): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431524280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000'
15/05/13 09:38:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:38:00 INFO DAGScheduler: Job 61 finished: reduce at JsonRDD.scala:51, took 0.000249 s
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 09:38:00 INFO JobScheduler: Finished job streaming job 1431524280000 ms.0 from job set of time 1431524280000 ms
15/05/13 09:38:00 INFO JobScheduler: Total delay: 0.176 s for time 1431524280000 ms (execution: 0.118 s)
15/05/13 09:38:00 INFO MapPartitionsRDD: Removing RDD 414 from persistence list
15/05/13 09:38:00 INFO BlockManager: Removing RDD 414
15/05/13 09:38:00 INFO UnionRDD: Removing RDD 413 from persistence list
15/05/13 09:38:00 INFO BlockManager: Removing RDD 413
15/05/13 09:38:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431524220000 ms: 1431524160000 ms
15/05/13 09:38:00 INFO JobGenerator: Checkpointing graph for time 1431524280000 ms
15/05/13 09:38:00 INFO DStreamGraph: Updating checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO DStreamGraph: Updated checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523920000.bk
15/05/13 09:38:00 INFO CheckpointWriter: Checkpoint for time 1431524280000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000', took 7642 bytes and 149 ms
15/05/13 09:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431524280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000'
15/05/13 09:38:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000
15/05/13 09:38:00 INFO CheckpointWriter: Checkpoint for time 1431524280000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524280000', took 7645 bytes and 70 ms
15/05/13 09:38:00 INFO DStreamGraph: Clearing checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO DStreamGraph: Cleared checkpoint data for time 1431524280000 ms
15/05/13 09:38:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:39:00 INFO FileInputDStream: Finding new files took 32 ms
15/05/13 09:39:00 INFO FileInputDStream: New files at time 1431524340000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524220047.json
15/05/13 09:39:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=8279789, maxMem=278302556
15/05/13 09:39:00 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 232.9 KB, free 257.3 MB)
15/05/13 09:39:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=8518321, maxMem=278302556
15/05/13 09:39:00 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 34.9 KB, free 257.3 MB)
15/05/13 09:39:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:39:00 INFO BlockManagerMaster: Updated info of block broadcast_90_piece0
15/05/13 09:39:00 INFO SparkContext: Created broadcast 90 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:39:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:39:00 INFO JobScheduler: Added jobs for time 1431524340000 ms
15/05/13 09:39:00 INFO JobGenerator: Checkpointing graph for time 1431524340000 ms
15/05/13 09:39:00 INFO DStreamGraph: Updating checkpoint data for time 1431524340000 ms
15/05/13 09:39:00 INFO JobScheduler: Starting job streaming job 1431524340000 ms.0 from job set of time 1431524340000 ms
15/05/13 09:39:00 INFO DStreamGraph: Updated checkpoint data for time 1431524340000 ms
15/05/13 09:39:00 INFO CheckpointWriter: Saving checkpoint for time 1431524340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524340000'
15/05/13 09:39:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:39:00 INFO DAGScheduler: Got job 62 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:39:00 INFO DAGScheduler: Final stage: Stage 60(reduce at JsonRDD.scala:51)
15/05/13 09:39:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:39:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:39:00 INFO DAGScheduler: Submitting Stage 60 (MapPartitionsRDD[438] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:39:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=8554029, maxMem=278302556
15/05/13 09:39:00 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 5.9 KB, free 257.2 MB)
15/05/13 09:39:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=8560069, maxMem=278302556
15/05/13 09:39:00 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 4.1 KB, free 257.2 MB)
15/05/13 09:39:00 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:39:00 INFO BlockManagerMaster: Updated info of block broadcast_91_piece0
15/05/13 09:39:00 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:839
15/05/13 09:39:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 60 (MapPartitionsRDD[438] at map at JsonRDD.scala:51)
15/05/13 09:39:00 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks
15/05/13 09:39:00 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 60, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:39:00 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:39:00 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:39:01 INFO CheckpointWriter: Checkpoint for time 1431524340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524340000', took 7643 bytes and 1035 ms
15/05/13 09:39:01 INFO DAGScheduler: Stage 60 (reduce at JsonRDD.scala:51) finished in 0.955 s
15/05/13 09:39:01 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 60) in 927 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:39:01 INFO DAGScheduler: Job 62 finished: reduce at JsonRDD.scala:51, took 0.994791 s
15/05/13 09:39:01 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool 
15/05/13 09:39:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:39:01 INFO DAGScheduler: Got job 63 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:39:01 INFO DAGScheduler: Final stage: Stage 61(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:39:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:39:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:39:01 INFO DAGScheduler: Submitting Stage 61 (MapPartitionsRDD[445] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:39:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=8564295, maxMem=278302556
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 20.6 KB, free 257.2 MB)
15/05/13 09:39:01 INFO MemoryStore: ensureFreeSpace(10953) called with curMem=8585391, maxMem=278302556
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 10.7 KB, free 257.2 MB)
15/05/13 09:39:01 INFO BlockManager: Removing broadcast 88
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_88_piece0
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_88_piece0 of size 4227 dropped from memory (free 269710439)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_88_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_88_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:39:01 INFO BlockManagerMaster: Updated info of block broadcast_88_piece0
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_88
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_88 of size 6040 dropped from memory (free 269716479)
15/05/13 09:39:01 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:39:01 INFO BlockManagerMaster: Updated info of block broadcast_92_piece0
15/05/13 09:39:01 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:839
15/05/13 09:39:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 61 (MapPartitionsRDD[445] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:39:01 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks
15/05/13 09:39:01 INFO ContextCleaner: Cleaned broadcast 88
15/05/13 09:39:01 INFO BlockManager: Removing broadcast 91
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_91_piece0
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_91_piece0 of size 4226 dropped from memory (free 269720705)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_91_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:39:01 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 61, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:39:01 INFO BlockManagerMaster: Updated info of block broadcast_91_piece0
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_91
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_91 of size 6040 dropped from memory (free 269726745)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_91_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:39:01 INFO ContextCleaner: Cleaned broadcast 91
15/05/13 09:39:01 INFO BlockManager: Removing broadcast 89
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_89_piece0
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_89_piece0 of size 10949 dropped from memory (free 269737694)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_89_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:39:01 INFO BlockManagerMaster: Updated info of block broadcast_89_piece0
15/05/13 09:39:01 INFO BlockManager: Removing block broadcast_89
15/05/13 09:39:01 INFO MemoryStore: Block broadcast_89 of size 21096 dropped from memory (free 269758790)
15/05/13 09:39:01 INFO BlockManagerInfo: Removed broadcast_89_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 264.6 MB)
15/05/13 09:39:01 INFO ContextCleaner: Cleaned broadcast 89
15/05/13 09:39:01 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:39:01 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:39:02 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 61) in 536 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:39:02 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool 
15/05/13 09:39:02 INFO DAGScheduler: Stage 61 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.539 s
15/05/13 09:39:02 INFO DAGScheduler: Job 63 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.625824 s
15/05/13 09:39:02 INFO JobScheduler: Finished job streaming job 1431524340000 ms.0 from job set of time 1431524340000 ms
15/05/13 09:39:02 INFO JobScheduler: Total delay: 2.280 s for time 1431524340000 ms (execution: 2.060 s)
15/05/13 09:39:02 INFO MapPartitionsRDD: Removing RDD 427 from persistence list
15/05/13 09:39:02 INFO BlockManager: Removing RDD 427
15/05/13 09:39:02 INFO UnionRDD: Removing RDD 426 from persistence list
15/05/13 09:39:02 INFO BlockManager: Removing RDD 426
15/05/13 09:39:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431524280000 ms: 1431524220000 ms
15/05/13 09:39:02 INFO JobGenerator: Checkpointing graph for time 1431524340000 ms
15/05/13 09:39:02 INFO DStreamGraph: Updating checkpoint data for time 1431524340000 ms
15/05/13 09:39:02 INFO DStreamGraph: Updated checkpoint data for time 1431524340000 ms
15/05/13 09:39:02 INFO CheckpointWriter: Saving checkpoint for time 1431524340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524340000'
15/05/13 09:39:02 INFO CheckpointWriter: Checkpoint for time 1431524340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524340000', took 7647 bytes and 59 ms
15/05/13 09:39:02 INFO DStreamGraph: Clearing checkpoint data for time 1431524340000 ms
15/05/13 09:39:02 INFO DStreamGraph: Cleared checkpoint data for time 1431524340000 ms
15/05/13 09:39:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:40:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 09:40:00 INFO FileInputDStream: New files at time 1431524400000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524283852.json
15/05/13 09:40:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=8543766, maxMem=278302556
15/05/13 09:40:00 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 232.9 KB, free 257.0 MB)
15/05/13 09:40:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=8782298, maxMem=278302556
15/05/13 09:40:00 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 34.9 KB, free 257.0 MB)
15/05/13 09:40:00 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:40:00 INFO BlockManagerMaster: Updated info of block broadcast_93_piece0
15/05/13 09:40:00 INFO SparkContext: Created broadcast 93 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:40:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:40:00 INFO JobScheduler: Added jobs for time 1431524400000 ms
15/05/13 09:40:00 INFO JobGenerator: Checkpointing graph for time 1431524400000 ms
15/05/13 09:40:00 INFO JobScheduler: Starting job streaming job 1431524400000 ms.0 from job set of time 1431524400000 ms
15/05/13 09:40:00 INFO DStreamGraph: Updating checkpoint data for time 1431524400000 ms
15/05/13 09:40:00 INFO DStreamGraph: Updated checkpoint data for time 1431524400000 ms
15/05/13 09:40:00 INFO CheckpointWriter: Saving checkpoint for time 1431524400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524400000'
15/05/13 09:40:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431523980000.bk
15/05/13 09:40:00 INFO CheckpointWriter: Checkpoint for time 1431524400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524400000', took 7642 bytes and 65 ms
15/05/13 09:40:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:40:00 INFO DAGScheduler: Got job 64 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:40:00 INFO DAGScheduler: Final stage: Stage 62(reduce at JsonRDD.scala:51)
15/05/13 09:40:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:40:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:40:00 INFO DAGScheduler: Submitting Stage 62 (MapPartitionsRDD[452] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:40:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=8818006, maxMem=278302556
15/05/13 09:40:00 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 5.9 KB, free 257.0 MB)
15/05/13 09:40:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=8824046, maxMem=278302556
15/05/13 09:40:00 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 4.1 KB, free 257.0 MB)
15/05/13 09:40:00 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:40:00 INFO BlockManagerMaster: Updated info of block broadcast_94_piece0
15/05/13 09:40:00 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:839
15/05/13 09:40:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 62 (MapPartitionsRDD[452] at map at JsonRDD.scala:51)
15/05/13 09:40:00 INFO TaskSchedulerImpl: Adding task set 62.0 with 1 tasks
15/05/13 09:40:00 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 62, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:40:00 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:40:00 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.6 MB)
15/05/13 09:40:00 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 62) in 434 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:40:00 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool 
15/05/13 09:40:00 INFO DAGScheduler: Stage 62 (reduce at JsonRDD.scala:51) finished in 0.455 s
15/05/13 09:40:00 INFO DAGScheduler: Job 64 finished: reduce at JsonRDD.scala:51, took 0.513297 s
15/05/13 09:40:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:40:01 INFO DAGScheduler: Got job 65 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:40:01 INFO DAGScheduler: Final stage: Stage 63(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:40:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:40:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:40:01 INFO DAGScheduler: Submitting Stage 63 (MapPartitionsRDD[459] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:40:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=8828272, maxMem=278302556
15/05/13 09:40:01 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 20.4 KB, free 257.0 MB)
15/05/13 09:40:01 INFO MemoryStore: ensureFreeSpace(10907) called with curMem=8849192, maxMem=278302556
15/05/13 09:40:01 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 10.7 KB, free 257.0 MB)
15/05/13 09:40:01 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:40:01 INFO BlockManagerMaster: Updated info of block broadcast_95_piece0
15/05/13 09:40:01 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:839
15/05/13 09:40:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 63 (MapPartitionsRDD[459] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:40:01 INFO TaskSchedulerImpl: Adding task set 63.0 with 1 tasks
15/05/13 09:40:01 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 63, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:40:01 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:40:01 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:40:01 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 63) in 484 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:40:01 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool 
15/05/13 09:40:01 INFO DAGScheduler: Stage 63 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.485 s
15/05/13 09:40:01 INFO DAGScheduler: Job 65 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.533254 s
15/05/13 09:40:01 INFO JobScheduler: Finished job streaming job 1431524400000 ms.0 from job set of time 1431524400000 ms
15/05/13 09:40:01 INFO JobScheduler: Total delay: 1.572 s for time 1431524400000 ms (execution: 1.418 s)
15/05/13 09:40:01 INFO MapPartitionsRDD: Removing RDD 434 from persistence list
15/05/13 09:40:01 INFO BlockManager: Removing RDD 434
15/05/13 09:40:01 INFO UnionRDD: Removing RDD 433 from persistence list
15/05/13 09:40:01 INFO BlockManager: Removing RDD 433
15/05/13 09:40:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524340000 ms: 1431524280000 ms
15/05/13 09:40:01 INFO JobGenerator: Checkpointing graph for time 1431524400000 ms
15/05/13 09:40:01 INFO DStreamGraph: Updating checkpoint data for time 1431524400000 ms
15/05/13 09:40:01 INFO DStreamGraph: Updated checkpoint data for time 1431524400000 ms
15/05/13 09:40:01 INFO CheckpointWriter: Saving checkpoint for time 1431524400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524400000'
15/05/13 09:40:03 INFO CheckpointWriter: Checkpoint for time 1431524400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524400000', took 7654 bytes and 2067 ms
15/05/13 09:40:03 INFO DStreamGraph: Clearing checkpoint data for time 1431524400000 ms
15/05/13 09:40:03 INFO DStreamGraph: Cleared checkpoint data for time 1431524400000 ms
15/05/13 09:40:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:41:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 09:41:00 INFO FileInputDStream: New files at time 1431524460000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524346458.json
15/05/13 09:41:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=8860099, maxMem=278302556
15/05/13 09:41:00 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 232.9 KB, free 256.7 MB)
15/05/13 09:41:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=9098631, maxMem=278302556
15/05/13 09:41:00 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 34.9 KB, free 256.7 MB)
15/05/13 09:41:00 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:41:00 INFO BlockManagerMaster: Updated info of block broadcast_96_piece0
15/05/13 09:41:00 INFO SparkContext: Created broadcast 96 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:41:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:41:00 INFO JobScheduler: Starting job streaming job 1431524460000 ms.0 from job set of time 1431524460000 ms
15/05/13 09:41:00 INFO JobScheduler: Added jobs for time 1431524460000 ms
15/05/13 09:41:00 INFO JobGenerator: Checkpointing graph for time 1431524460000 ms
15/05/13 09:41:00 INFO DStreamGraph: Updating checkpoint data for time 1431524460000 ms
15/05/13 09:41:00 INFO DStreamGraph: Updated checkpoint data for time 1431524460000 ms
15/05/13 09:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431524460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000'
15/05/13 09:41:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82988): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431524460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000'
15/05/13 09:41:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:41:00 INFO DAGScheduler: Got job 66 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:41:00 INFO DAGScheduler: Final stage: Stage 64(reduce at JsonRDD.scala:51)
15/05/13 09:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:41:00 INFO DAGScheduler: Submitting Stage 64 (MapPartitionsRDD[466] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:41:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=9134339, maxMem=278302556
15/05/13 09:41:00 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 5.9 KB, free 256.7 MB)
15/05/13 09:41:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=9140379, maxMem=278302556
15/05/13 09:41:00 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 4.1 KB, free 256.7 MB)
15/05/13 09:41:00 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:41:00 INFO BlockManagerMaster: Updated info of block broadcast_97_piece0
15/05/13 09:41:00 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:839
15/05/13 09:41:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 64 (MapPartitionsRDD[466] at map at JsonRDD.scala:51)
15/05/13 09:41:00 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks
15/05/13 09:41:00 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 64, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:41:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000
15/05/13 09:41:00 INFO CheckpointWriter: Checkpoint for time 1431524460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000', took 7665 bytes and 158 ms
15/05/13 09:41:00 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.1 MB)
15/05/13 09:41:00 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:41:00 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 64) in 364 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:41:00 INFO DAGScheduler: Stage 64 (reduce at JsonRDD.scala:51) finished in 0.386 s
15/05/13 09:41:00 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool 
15/05/13 09:41:00 INFO DAGScheduler: Job 66 finished: reduce at JsonRDD.scala:51, took 0.429087 s
15/05/13 09:41:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:41:00 INFO DAGScheduler: Got job 67 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:41:00 INFO DAGScheduler: Final stage: Stage 65(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:41:00 INFO DAGScheduler: Submitting Stage 65 (MapPartitionsRDD[473] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:41:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=9144605, maxMem=278302556
15/05/13 09:41:00 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 20.4 KB, free 256.7 MB)
15/05/13 09:41:01 INFO MemoryStore: ensureFreeSpace(10843) called with curMem=9165525, maxMem=278302556
15/05/13 09:41:01 INFO BlockManager: Removing broadcast 97
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_97
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 10.6 KB, free 256.7 MB)
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_97 of size 6040 dropped from memory (free 269132228)
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_97_piece0
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_97_piece0 of size 4226 dropped from memory (free 269136454)
15/05/13 09:41:01 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_97_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:41:01 INFO BlockManagerMaster: Updated info of block broadcast_97_piece0
15/05/13 09:41:01 INFO BlockManagerMaster: Updated info of block broadcast_98_piece0
15/05/13 09:41:01 INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:839
15/05/13 09:41:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 65 (MapPartitionsRDD[473] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:41:01 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_97_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:41:01 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 65, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:41:01 INFO ContextCleaner: Cleaned broadcast 97
15/05/13 09:41:01 INFO BlockManager: Removing broadcast 92
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_92
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_92 of size 21096 dropped from memory (free 269157550)
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_92_piece0
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_92_piece0 of size 10953 dropped from memory (free 269168503)
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_92_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:41:01 INFO BlockManagerMaster: Updated info of block broadcast_92_piece0
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_92_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:41:01 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 265.0 MB)
15/05/13 09:41:01 INFO ContextCleaner: Cleaned broadcast 92
15/05/13 09:41:01 INFO BlockManager: Removing broadcast 95
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_95
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_95 of size 20920 dropped from memory (free 269189423)
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_95_piece0
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_95_piece0 of size 10907 dropped from memory (free 269200330)
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_95_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:41:01 INFO BlockManagerMaster: Updated info of block broadcast_95_piece0
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_95_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:41:01 INFO ContextCleaner: Cleaned broadcast 95
15/05/13 09:41:01 INFO BlockManager: Removing broadcast 94
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_94_piece0
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_94_piece0 of size 4226 dropped from memory (free 269204556)
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_94_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:41:01 INFO BlockManagerMaster: Updated info of block broadcast_94_piece0
15/05/13 09:41:01 INFO BlockManager: Removing block broadcast_94
15/05/13 09:41:01 INFO MemoryStore: Block broadcast_94 of size 6040 dropped from memory (free 269210596)
15/05/13 09:41:01 INFO BlockManagerInfo: Removed broadcast_94_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:41:01 INFO ContextCleaner: Cleaned broadcast 94
15/05/13 09:41:01 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 65) in 145 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:41:01 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool 
15/05/13 09:41:01 INFO DAGScheduler: Stage 65 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.146 s
15/05/13 09:41:01 INFO DAGScheduler: Job 67 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.239109 s
15/05/13 09:41:01 INFO JobScheduler: Finished job streaming job 1431524460000 ms.0 from job set of time 1431524460000 ms
15/05/13 09:41:01 INFO JobScheduler: Total delay: 1.264 s for time 1431524460000 ms (execution: 1.043 s)
15/05/13 09:41:01 INFO MapPartitionsRDD: Removing RDD 448 from persistence list
15/05/13 09:41:01 INFO BlockManager: Removing RDD 448
15/05/13 09:41:01 INFO UnionRDD: Removing RDD 447 from persistence list
15/05/13 09:41:01 INFO BlockManager: Removing RDD 447
15/05/13 09:41:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524400000 ms: 1431524340000 ms
15/05/13 09:41:01 INFO JobGenerator: Checkpointing graph for time 1431524460000 ms
15/05/13 09:41:01 INFO DStreamGraph: Updating checkpoint data for time 1431524460000 ms
15/05/13 09:41:01 INFO DStreamGraph: Updated checkpoint data for time 1431524460000 ms
15/05/13 09:41:01 INFO CheckpointWriter: Saving checkpoint for time 1431524460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000'
15/05/13 09:41:03 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82993): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:41:03 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82993): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:41:03 INFO CheckpointWriter: Saving checkpoint for time 1431524460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000'
15/05/13 09:41:03 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82994): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:41:03 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82994): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:41:03 INFO CheckpointWriter: Saving checkpoint for time 1431524460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000'
15/05/13 09:41:03 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524040000
15/05/13 09:41:03 INFO CheckpointWriter: Checkpoint for time 1431524460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000', took 7657 bytes and 1840 ms
15/05/13 09:41:03 INFO DStreamGraph: Clearing checkpoint data for time 1431524460000 ms
15/05/13 09:41:03 INFO DStreamGraph: Cleared checkpoint data for time 1431524460000 ms
15/05/13 09:41:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:42:00 INFO FileInputDStream: Finding new files took 47 ms
15/05/13 09:42:00 INFO FileInputDStream: New files at time 1431524520000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524406796.json
15/05/13 09:42:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=9091960, maxMem=278302556
15/05/13 09:42:00 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 232.9 KB, free 256.5 MB)
15/05/13 09:42:01 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=9330492, maxMem=278302556
15/05/13 09:42:01 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 34.9 KB, free 256.5 MB)
15/05/13 09:42:01 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:42:01 INFO BlockManagerMaster: Updated info of block broadcast_99_piece0
15/05/13 09:42:01 INFO SparkContext: Created broadcast 99 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:42:01 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:42:01 INFO JobScheduler: Added jobs for time 1431524520000 ms
15/05/13 09:42:01 INFO JobGenerator: Checkpointing graph for time 1431524520000 ms
15/05/13 09:42:01 INFO JobScheduler: Starting job streaming job 1431524520000 ms.0 from job set of time 1431524520000 ms
15/05/13 09:42:01 INFO DStreamGraph: Updating checkpoint data for time 1431524520000 ms
15/05/13 09:42:01 INFO DStreamGraph: Updated checkpoint data for time 1431524520000 ms
15/05/13 09:42:01 INFO CheckpointWriter: Saving checkpoint for time 1431524520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000'
15/05/13 09:42:01 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:42:01 INFO DAGScheduler: Got job 68 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:42:01 INFO DAGScheduler: Final stage: Stage 66(reduce at JsonRDD.scala:51)
15/05/13 09:42:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:42:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:42:01 INFO DAGScheduler: Submitting Stage 66 (MapPartitionsRDD[480] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:42:01 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=9366200, maxMem=278302556
15/05/13 09:42:01 INFO MemoryStore: Block broadcast_100 stored as values in memory (estimated size 5.9 KB, free 256.5 MB)
15/05/13 09:42:01 INFO MemoryStore: ensureFreeSpace(4229) called with curMem=9372240, maxMem=278302556
15/05/13 09:42:01 INFO MemoryStore: Block broadcast_100_piece0 stored as bytes in memory (estimated size 4.1 KB, free 256.5 MB)
15/05/13 09:42:01 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:42:01 INFO BlockManagerMaster: Updated info of block broadcast_100_piece0
15/05/13 09:42:01 INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:839
15/05/13 09:42:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 66 (MapPartitionsRDD[480] at map at JsonRDD.scala:51)
15/05/13 09:42:01 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks
15/05/13 09:42:01 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 66, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:42:02 INFO BlockManagerInfo: Added broadcast_100_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.6 MB)
15/05/13 09:42:02 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:42:02 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 66) in 678 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:42:02 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool 
15/05/13 09:42:02 INFO DAGScheduler: Stage 66 (reduce at JsonRDD.scala:51) finished in 0.691 s
15/05/13 09:42:02 INFO DAGScheduler: Job 68 finished: reduce at JsonRDD.scala:51, took 0.737268 s
15/05/13 09:42:02 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:42:02 INFO DAGScheduler: Got job 69 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:42:02 INFO DAGScheduler: Final stage: Stage 67(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:42:02 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:42:02 INFO DAGScheduler: Missing parents: List()
15/05/13 09:42:02 INFO DAGScheduler: Submitting Stage 67 (MapPartitionsRDD[487] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:42:02 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=9376469, maxMem=278302556
15/05/13 09:42:02 INFO MemoryStore: Block broadcast_101 stored as values in memory (estimated size 20.0 KB, free 256.4 MB)
15/05/13 09:42:02 INFO MemoryStore: ensureFreeSpace(10765) called with curMem=9396957, maxMem=278302556
15/05/13 09:42:02 INFO MemoryStore: Block broadcast_101_piece0 stored as bytes in memory (estimated size 10.5 KB, free 256.4 MB)
15/05/13 09:42:02 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 264.2 MB)
15/05/13 09:42:02 INFO BlockManagerMaster: Updated info of block broadcast_101_piece0
15/05/13 09:42:02 INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:839
15/05/13 09:42:02 INFO DAGScheduler: Submitting 1 missing tasks from Stage 67 (MapPartitionsRDD[487] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:42:02 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 tasks
15/05/13 09:42:02 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 67, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:42:02 INFO BlockManagerInfo: Added broadcast_101_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 265.0 MB)
15/05/13 09:42:03 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:42:03 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 67) in 455 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:42:03 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool 
15/05/13 09:42:03 INFO DAGScheduler: Stage 67 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.458 s
15/05/13 09:42:03 INFO DAGScheduler: Job 69 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.505362 s
15/05/13 09:42:03 INFO JobScheduler: Finished job streaming job 1431524520000 ms.0 from job set of time 1431524520000 ms
15/05/13 09:42:03 INFO JobScheduler: Total delay: 3.418 s for time 1431524520000 ms (execution: 1.577 s)
15/05/13 09:42:03 INFO MapPartitionsRDD: Removing RDD 462 from persistence list
15/05/13 09:42:03 INFO BlockManager: Removing RDD 462
15/05/13 09:42:03 INFO UnionRDD: Removing RDD 461 from persistence list
15/05/13 09:42:03 INFO BlockManager: Removing RDD 461
15/05/13 09:42:03 INFO FileInputDStream: Cleared 1 old files that were older than 1431524460000 ms: 1431524400000 ms
15/05/13 09:42:03 INFO JobGenerator: Checkpointing graph for time 1431524520000 ms
15/05/13 09:42:03 INFO DStreamGraph: Updating checkpoint data for time 1431524520000 ms
15/05/13 09:42:03 INFO DStreamGraph: Updated checkpoint data for time 1431524520000 ms
15/05/13 09:42:06 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83001): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:42:06 INFO CheckpointWriter: Saving checkpoint for time 1431524520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000'
15/05/13 09:42:06 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524160000
15/05/13 09:42:06 INFO CheckpointWriter: Checkpoint for time 1431524520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000', took 7666 bytes and 4317 ms
15/05/13 09:42:06 INFO CheckpointWriter: Saving checkpoint for time 1431524520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000'
15/05/13 09:42:06 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000
15/05/13 09:42:06 INFO CheckpointWriter: Checkpoint for time 1431524520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524520000', took 7657 bytes and 302 ms
15/05/13 09:42:06 INFO DStreamGraph: Clearing checkpoint data for time 1431524520000 ms
15/05/13 09:42:06 INFO DStreamGraph: Cleared checkpoint data for time 1431524520000 ms
15/05/13 09:42:06 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:43:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 09:43:00 INFO FileInputDStream: New files at time 1431524580000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524467941.json
15/05/13 09:43:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=9407722, maxMem=278302556
15/05/13 09:43:00 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 232.9 KB, free 256.2 MB)
15/05/13 09:43:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=9646254, maxMem=278302556
15/05/13 09:43:00 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 34.9 KB, free 256.2 MB)
15/05/13 09:43:00 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:43:00 INFO BlockManagerMaster: Updated info of block broadcast_102_piece0
15/05/13 09:43:00 INFO SparkContext: Created broadcast 102 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:43:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:43:00 INFO JobScheduler: Added jobs for time 1431524580000 ms
15/05/13 09:43:00 INFO JobGenerator: Checkpointing graph for time 1431524580000 ms
15/05/13 09:43:00 INFO JobScheduler: Starting job streaming job 1431524580000 ms.0 from job set of time 1431524580000 ms
15/05/13 09:43:00 INFO DStreamGraph: Updating checkpoint data for time 1431524580000 ms
15/05/13 09:43:00 INFO DStreamGraph: Updated checkpoint data for time 1431524580000 ms
15/05/13 09:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:43:00 INFO DAGScheduler: Got job 70 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:43:00 INFO DAGScheduler: Final stage: Stage 68(reduce at JsonRDD.scala:51)
15/05/13 09:43:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:43:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:43:00 INFO DAGScheduler: Submitting Stage 68 (MapPartitionsRDD[494] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:43:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=9681962, maxMem=278302556
15/05/13 09:43:00 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 5.9 KB, free 256.2 MB)
15/05/13 09:43:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=9688002, maxMem=278302556
15/05/13 09:43:00 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 4.1 KB, free 256.2 MB)
15/05/13 09:43:00 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:43:00 INFO BlockManagerMaster: Updated info of block broadcast_103_piece0
15/05/13 09:43:00 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:839
15/05/13 09:43:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 68 (MapPartitionsRDD[494] at map at JsonRDD.scala:51)
15/05/13 09:43:00 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks
15/05/13 09:43:00 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 68, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:43:00 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:43:00 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:43:00 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 68) in 599 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:43:00 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool 
15/05/13 09:43:00 INFO DAGScheduler: Stage 68 (reduce at JsonRDD.scala:51) finished in 0.617 s
15/05/13 09:43:00 INFO DAGScheduler: Job 70 finished: reduce at JsonRDD.scala:51, took 0.638349 s
15/05/13 09:43:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:43:01 INFO DAGScheduler: Got job 71 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:43:01 INFO DAGScheduler: Final stage: Stage 69(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:43:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:43:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:43:01 INFO DAGScheduler: Submitting Stage 69 (MapPartitionsRDD[501] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:43:01 INFO MemoryStore: ensureFreeSpace(21656) called with curMem=9692228, maxMem=278302556
15/05/13 09:43:01 INFO MemoryStore: Block broadcast_104 stored as values in memory (estimated size 21.1 KB, free 256.1 MB)
15/05/13 09:43:01 INFO MemoryStore: ensureFreeSpace(11239) called with curMem=9713884, maxMem=278302556
15/05/13 09:43:01 INFO MemoryStore: Block broadcast_104_piece0 stored as bytes in memory (estimated size 11.0 KB, free 256.1 MB)
15/05/13 09:43:01 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 264.2 MB)
15/05/13 09:43:01 INFO BlockManagerMaster: Updated info of block broadcast_104_piece0
15/05/13 09:43:01 INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:839
15/05/13 09:43:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 69 (MapPartitionsRDD[501] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:43:01 INFO TaskSchedulerImpl: Adding task set 69.0 with 1 tasks
15/05/13 09:43:01 INFO TaskSetManager: Starting task 0.0 in stage 69.0 (TID 69, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:43:01 INFO BlockManagerInfo: Added broadcast_104_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 264.5 MB)
15/05/13 09:43:01 INFO TaskSetManager: Finished task 0.0 in stage 69.0 (TID 69) in 202 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:43:01 INFO TaskSchedulerImpl: Removed TaskSet 69.0, whose tasks have all completed, from pool 
15/05/13 09:43:01 INFO DAGScheduler: Stage 69 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.203 s
15/05/13 09:43:01 INFO DAGScheduler: Job 71 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.232809 s
15/05/13 09:43:01 INFO JobScheduler: Finished job streaming job 1431524580000 ms.0 from job set of time 1431524580000 ms
15/05/13 09:43:01 INFO JobScheduler: Total delay: 1.363 s for time 1431524580000 ms (execution: 1.237 s)
15/05/13 09:43:01 INFO MapPartitionsRDD: Removing RDD 476 from persistence list
15/05/13 09:43:01 INFO BlockManager: Removing RDD 476
15/05/13 09:43:01 INFO UnionRDD: Removing RDD 475 from persistence list
15/05/13 09:43:01 INFO BlockManager: Removing RDD 475
15/05/13 09:43:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524520000 ms: 1431524460000 ms
15/05/13 09:43:01 INFO JobGenerator: Checkpointing graph for time 1431524580000 ms
15/05/13 09:43:01 INFO DStreamGraph: Updating checkpoint data for time 1431524580000 ms
15/05/13 09:43:01 INFO DStreamGraph: Updated checkpoint data for time 1431524580000 ms
15/05/13 09:43:03 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83008): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:03 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83008): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:03 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:05 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83010): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:05 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83010): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:05 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:07 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83012): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:07 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83012): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:43:07 WARN CheckpointWriter: Could not write checkpoint for time 1431524580000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:07 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:07 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83014): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:43:07 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:07 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83016): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:43:07 INFO CheckpointWriter: Saving checkpoint for time 1431524580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:43:07 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83018): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:43:07 WARN CheckpointWriter: Could not write checkpoint for time 1431524580000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524580000'
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 98
15/05/13 09:44:00 INFO FileInputDStream: Finding new files took 76 ms
15/05/13 09:44:00 INFO FileInputDStream: New files at time 1431524640000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524528434.json
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_98_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_98_piece0 of size 10843 dropped from memory (free 268588276)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_98_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_98_piece0
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_98
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_98 of size 20920 dropped from memory (free 268609196)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_98_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 265.0 MB)
15/05/13 09:44:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=9693360, maxMem=278302556
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_105 stored as values in memory (estimated size 232.9 KB, free 255.9 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 98
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 101
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_101
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_101 of size 20488 dropped from memory (free 268391152)
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_101_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_101_piece0 of size 10765 dropped from memory (free 268401917)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_101_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_101_piece0
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_101_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 264.9 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 101
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 100
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_100
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_100 of size 6040 dropped from memory (free 268407957)
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_100_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_100_piece0 of size 4229 dropped from memory (free 268412186)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_100_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_100_piece0
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_100_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 100
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 99
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_99_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_99_piece0 of size 35708 dropped from memory (free 268447894)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_99_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_99_piece0
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_99
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_99 of size 238532 dropped from memory (free 268686426)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_99_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_99_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 99
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 104
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_104_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_104_piece0 of size 11239 dropped from memory (free 268697665)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_104_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_104_piece0
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_104
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_104 of size 21656 dropped from memory (free 268719321)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_104_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 264.5 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 104
15/05/13 09:44:00 INFO BlockManager: Removing broadcast 103
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_103
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_103 of size 6040 dropped from memory (free 268725361)
15/05/13 09:44:00 INFO BlockManager: Removing block broadcast_103_piece0
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_103_piece0 of size 4226 dropped from memory (free 268729587)
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_103_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_103_piece0
15/05/13 09:44:00 INFO BlockManagerInfo: Removed broadcast_103_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:44:00 INFO ContextCleaner: Cleaned broadcast 103
15/05/13 09:44:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=9572969, maxMem=278302556
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_105_piece0 stored as bytes in memory (estimated size 34.9 KB, free 256.2 MB)
15/05/13 09:44:00 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_105_piece0
15/05/13 09:44:00 INFO SparkContext: Created broadcast 105 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:44:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:44:00 INFO JobScheduler: Added jobs for time 1431524640000 ms
15/05/13 09:44:00 INFO JobScheduler: Starting job streaming job 1431524640000 ms.0 from job set of time 1431524640000 ms
15/05/13 09:44:00 INFO JobGenerator: Checkpointing graph for time 1431524640000 ms
15/05/13 09:44:00 INFO DStreamGraph: Updating checkpoint data for time 1431524640000 ms
15/05/13 09:44:00 INFO DStreamGraph: Updated checkpoint data for time 1431524640000 ms
15/05/13 09:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431524640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524640000'
15/05/13 09:44:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:44:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524220000
15/05/13 09:44:00 INFO DAGScheduler: Got job 72 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:44:00 INFO DAGScheduler: Final stage: Stage 70(reduce at JsonRDD.scala:51)
15/05/13 09:44:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:44:00 INFO CheckpointWriter: Checkpoint for time 1431524640000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524640000', took 7666 bytes and 68 ms
15/05/13 09:44:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:44:00 INFO DAGScheduler: Submitting Stage 70 (MapPartitionsRDD[508] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:44:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=9608677, maxMem=278302556
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 5.9 KB, free 256.2 MB)
15/05/13 09:44:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=9614717, maxMem=278302556
15/05/13 09:44:00 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 4.1 KB, free 256.2 MB)
15/05/13 09:44:00 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:44:00 INFO BlockManagerMaster: Updated info of block broadcast_106_piece0
15/05/13 09:44:00 INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:839
15/05/13 09:44:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 70 (MapPartitionsRDD[508] at map at JsonRDD.scala:51)
15/05/13 09:44:00 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks
15/05/13 09:44:00 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 70, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:44:00 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:44:00 INFO BlockManagerInfo: Added broadcast_105_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:44:00 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 70) in 568 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:44:00 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool 
15/05/13 09:44:00 INFO DAGScheduler: Stage 70 (reduce at JsonRDD.scala:51) finished in 0.592 s
15/05/13 09:44:00 INFO DAGScheduler: Job 72 finished: reduce at JsonRDD.scala:51, took 0.621412 s
15/05/13 09:44:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:44:01 INFO DAGScheduler: Got job 73 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:44:01 INFO DAGScheduler: Final stage: Stage 71(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:44:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:44:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:44:01 INFO DAGScheduler: Submitting Stage 71 (MapPartitionsRDD[515] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:44:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=9618943, maxMem=278302556
15/05/13 09:44:01 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 20.4 KB, free 256.2 MB)
15/05/13 09:44:01 INFO MemoryStore: ensureFreeSpace(10847) called with curMem=9639863, maxMem=278302556
15/05/13 09:44:01 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 10.6 KB, free 256.2 MB)
15/05/13 09:44:01 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:44:01 INFO BlockManagerMaster: Updated info of block broadcast_107_piece0
15/05/13 09:44:01 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:839
15/05/13 09:44:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 71 (MapPartitionsRDD[515] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:44:01 INFO TaskSchedulerImpl: Adding task set 71.0 with 1 tasks
15/05/13 09:44:01 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 71, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:44:01 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.9 MB)
15/05/13 09:44:01 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 71) in 213 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:44:01 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool 
15/05/13 09:44:01 INFO DAGScheduler: Stage 71 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.216 s
15/05/13 09:44:01 INFO DAGScheduler: Job 73 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.236754 s
15/05/13 09:44:01 INFO JobScheduler: Finished job streaming job 1431524640000 ms.0 from job set of time 1431524640000 ms
15/05/13 09:44:01 INFO JobScheduler: Total delay: 1.474 s for time 1431524640000 ms (execution: 1.210 s)
15/05/13 09:44:01 INFO MapPartitionsRDD: Removing RDD 490 from persistence list
15/05/13 09:44:01 INFO BlockManager: Removing RDD 490
15/05/13 09:44:01 INFO UnionRDD: Removing RDD 489 from persistence list
15/05/13 09:44:01 INFO BlockManager: Removing RDD 489
15/05/13 09:44:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524580000 ms: 1431524520000 ms
15/05/13 09:44:01 INFO JobGenerator: Checkpointing graph for time 1431524640000 ms
15/05/13 09:44:01 INFO DStreamGraph: Updating checkpoint data for time 1431524640000 ms
15/05/13 09:44:01 INFO DStreamGraph: Updated checkpoint data for time 1431524640000 ms
15/05/13 09:44:01 INFO CheckpointWriter: Saving checkpoint for time 1431524640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524640000'
15/05/13 09:44:01 INFO CheckpointWriter: Checkpoint for time 1431524640000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524640000', took 7657 bytes and 50 ms
15/05/13 09:44:01 INFO DStreamGraph: Clearing checkpoint data for time 1431524640000 ms
15/05/13 09:44:01 INFO DStreamGraph: Cleared checkpoint data for time 1431524640000 ms
15/05/13 09:44:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:45:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 09:45:00 INFO FileInputDStream: New files at time 1431524700000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524589329.json
15/05/13 09:45:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=9650710, maxMem=278302556
15/05/13 09:45:00 INFO MemoryStore: Block broadcast_108 stored as values in memory (estimated size 232.9 KB, free 256.0 MB)
15/05/13 09:45:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=9889242, maxMem=278302556
15/05/13 09:45:00 INFO MemoryStore: Block broadcast_108_piece0 stored as bytes in memory (estimated size 34.9 KB, free 255.9 MB)
15/05/13 09:45:00 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:45:00 INFO BlockManagerMaster: Updated info of block broadcast_108_piece0
15/05/13 09:45:00 INFO SparkContext: Created broadcast 108 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:45:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:45:00 INFO JobScheduler: Added jobs for time 1431524700000 ms
15/05/13 09:45:00 INFO JobGenerator: Checkpointing graph for time 1431524700000 ms
15/05/13 09:45:00 INFO DStreamGraph: Updating checkpoint data for time 1431524700000 ms
15/05/13 09:45:00 INFO DStreamGraph: Updated checkpoint data for time 1431524700000 ms
15/05/13 09:45:00 INFO JobScheduler: Starting job streaming job 1431524700000 ms.0 from job set of time 1431524700000 ms
15/05/13 09:45:00 INFO CheckpointWriter: Saving checkpoint for time 1431524700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000'
15/05/13 09:45:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83027): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:45:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83027): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:45:00 INFO CheckpointWriter: Saving checkpoint for time 1431524700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000'
15/05/13 09:45:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:45:00 INFO DAGScheduler: Got job 74 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:45:00 INFO DAGScheduler: Final stage: Stage 72(reduce at JsonRDD.scala:51)
15/05/13 09:45:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:45:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:45:00 INFO DAGScheduler: Submitting Stage 72 (MapPartitionsRDD[522] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:45:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=9924950, maxMem=278302556
15/05/13 09:45:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000
15/05/13 09:45:00 INFO MemoryStore: Block broadcast_109 stored as values in memory (estimated size 5.9 KB, free 255.9 MB)
15/05/13 09:45:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=9930990, maxMem=278302556
15/05/13 09:45:00 INFO MemoryStore: Block broadcast_109_piece0 stored as bytes in memory (estimated size 4.1 KB, free 255.9 MB)
15/05/13 09:45:00 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:45:00 INFO BlockManagerMaster: Updated info of block broadcast_109_piece0
15/05/13 09:45:00 INFO CheckpointWriter: Checkpoint for time 1431524700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000', took 7665 bytes and 164 ms
15/05/13 09:45:00 INFO SparkContext: Created broadcast 109 from broadcast at DAGScheduler.scala:839
15/05/13 09:45:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 72 (MapPartitionsRDD[522] at map at JsonRDD.scala:51)
15/05/13 09:45:00 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks
15/05/13 09:45:00 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 72, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:45:00 INFO BlockManagerInfo: Added broadcast_109_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:45:00 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:45:00 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 72) in 579 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:45:00 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
15/05/13 09:45:00 INFO DAGScheduler: Stage 72 (reduce at JsonRDD.scala:51) finished in 0.596 s
15/05/13 09:45:00 INFO DAGScheduler: Job 74 finished: reduce at JsonRDD.scala:51, took 0.652394 s
15/05/13 09:45:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:45:01 INFO DAGScheduler: Got job 75 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:45:01 INFO DAGScheduler: Final stage: Stage 73(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:45:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:45:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:45:01 INFO DAGScheduler: Submitting Stage 73 (MapPartitionsRDD[529] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:45:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=9935216, maxMem=278302556
15/05/13 09:45:01 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 20.6 KB, free 255.9 MB)
15/05/13 09:45:01 INFO MemoryStore: ensureFreeSpace(10945) called with curMem=9956312, maxMem=278302556
15/05/13 09:45:01 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 10.7 KB, free 255.9 MB)
15/05/13 09:45:01 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.2 MB)
15/05/13 09:45:01 INFO BlockManagerMaster: Updated info of block broadcast_110_piece0
15/05/13 09:45:01 INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:839
15/05/13 09:45:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 73 (MapPartitionsRDD[529] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:45:01 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks
15/05/13 09:45:01 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 73, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:45:01 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:45:01 INFO BlockManagerInfo: Added broadcast_108_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:45:01 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 73) in 226 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:45:01 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool 
15/05/13 09:45:01 INFO DAGScheduler: Stage 73 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.229 s
15/05/13 09:45:01 INFO DAGScheduler: Job 75 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.259343 s
15/05/13 09:45:01 INFO JobScheduler: Finished job streaming job 1431524700000 ms.0 from job set of time 1431524700000 ms
15/05/13 09:45:01 INFO MapPartitionsRDD: Removing RDD 504 from persistence list
15/05/13 09:45:01 INFO JobScheduler: Total delay: 1.478 s for time 1431524700000 ms (execution: 1.314 s)
15/05/13 09:45:01 INFO BlockManager: Removing RDD 504
15/05/13 09:45:01 INFO UnionRDD: Removing RDD 503 from persistence list
15/05/13 09:45:01 INFO BlockManager: Removing RDD 503
15/05/13 09:45:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524640000 ms: 1431524580000 ms
15/05/13 09:45:01 INFO JobGenerator: Checkpointing graph for time 1431524700000 ms
15/05/13 09:45:01 INFO DStreamGraph: Updating checkpoint data for time 1431524700000 ms
15/05/13 09:45:01 INFO DStreamGraph: Updated checkpoint data for time 1431524700000 ms
15/05/13 09:45:01 INFO CheckpointWriter: Saving checkpoint for time 1431524700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000'
15/05/13 09:45:01 INFO CheckpointWriter: Checkpoint for time 1431524700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000', took 7656 bytes and 81 ms
15/05/13 09:45:01 INFO DStreamGraph: Clearing checkpoint data for time 1431524700000 ms
15/05/13 09:45:01 INFO DStreamGraph: Cleared checkpoint data for time 1431524700000 ms
15/05/13 09:45:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:46:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 09:46:00 INFO FileInputDStream: New files at time 1431524760000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524650062.json
15/05/13 09:46:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=9967257, maxMem=278302556
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 232.9 KB, free 255.7 MB)
15/05/13 09:46:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=10205789, maxMem=278302556
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 34.9 KB, free 255.6 MB)
15/05/13 09:46:00 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_111_piece0
15/05/13 09:46:00 INFO SparkContext: Created broadcast 111 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:46:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:46:00 INFO JobScheduler: Added jobs for time 1431524760000 ms
15/05/13 09:46:00 INFO JobGenerator: Checkpointing graph for time 1431524760000 ms
15/05/13 09:46:00 INFO DStreamGraph: Updating checkpoint data for time 1431524760000 ms
15/05/13 09:46:00 INFO DStreamGraph: Updated checkpoint data for time 1431524760000 ms
15/05/13 09:46:00 INFO JobScheduler: Starting job streaming job 1431524760000 ms.0 from job set of time 1431524760000 ms
15/05/13 09:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431524760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524760000'
15/05/13 09:46:00 INFO BlockManager: Removing broadcast 107
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_107
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_107 of size 20920 dropped from memory (free 268081979)
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_107_piece0
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_107_piece0 of size 10847 dropped from memory (free 268092826)
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_107_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_107_piece0
15/05/13 09:46:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524760000
15/05/13 09:46:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:46:00 INFO DAGScheduler: Got job 76 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:46:00 INFO DAGScheduler: Final stage: Stage 74(reduce at JsonRDD.scala:51)
15/05/13 09:46:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_107_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.9 MB)
15/05/13 09:46:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:46:00 INFO DAGScheduler: Submitting Stage 74 (MapPartitionsRDD[536] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:46:00 INFO ContextCleaner: Cleaned broadcast 107
15/05/13 09:46:00 INFO BlockManager: Removing broadcast 106
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_106
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_106 of size 6040 dropped from memory (free 268098866)
15/05/13 09:46:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=10209730, maxMem=278302556
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_106_piece0
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_106_piece0 of size 4226 dropped from memory (free 268097052)
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 5.9 KB, free 255.7 MB)
15/05/13 09:46:00 INFO CheckpointWriter: Checkpoint for time 1431524760000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524760000', took 7662 bytes and 131 ms
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_106_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_106_piece0
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_106_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:46:00 INFO ContextCleaner: Cleaned broadcast 106
15/05/13 09:46:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=10205504, maxMem=278302556
15/05/13 09:46:00 INFO BlockManager: Removing broadcast 110
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 4.1 KB, free 255.7 MB)
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_110_piece0
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_110_piece0 of size 10945 dropped from memory (free 268103771)
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_110_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_110_piece0
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_110
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_110 of size 21096 dropped from memory (free 268124867)
15/05/13 09:46:00 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_112_piece0
15/05/13 09:46:00 INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:839
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_110_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 265.0 MB)
15/05/13 09:46:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 74 (MapPartitionsRDD[536] at map at JsonRDD.scala:51)
15/05/13 09:46:00 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks
15/05/13 09:46:00 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 74, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:46:00 INFO ContextCleaner: Cleaned broadcast 110
15/05/13 09:46:00 INFO BlockManager: Removing broadcast 109
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_109
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_109 of size 6040 dropped from memory (free 268130907)
15/05/13 09:46:00 INFO BlockManager: Removing block broadcast_109_piece0
15/05/13 09:46:00 INFO MemoryStore: Block broadcast_109_piece0 of size 4226 dropped from memory (free 268135133)
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_109_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:46:00 INFO BlockManagerMaster: Updated info of block broadcast_109_piece0
15/05/13 09:46:00 INFO BlockManagerInfo: Removed broadcast_109_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:46:00 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.5 MB)
15/05/13 09:46:00 INFO ContextCleaner: Cleaned broadcast 109
15/05/13 09:46:00 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.5 MB)
15/05/13 09:46:00 INFO DAGScheduler: Stage 74 (reduce at JsonRDD.scala:51) finished in 0.430 s
15/05/13 09:46:00 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 74) in 413 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:46:00 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool 
15/05/13 09:46:00 INFO DAGScheduler: Job 76 finished: reduce at JsonRDD.scala:51, took 0.475210 s
15/05/13 09:46:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:46:01 INFO DAGScheduler: Got job 77 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:46:01 INFO DAGScheduler: Final stage: Stage 75(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:46:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:46:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:46:01 INFO DAGScheduler: Submitting Stage 75 (MapPartitionsRDD[543] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:46:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=10167423, maxMem=278302556
15/05/13 09:46:01 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 21.0 KB, free 255.7 MB)
15/05/13 09:46:01 INFO MemoryStore: ensureFreeSpace(11162) called with curMem=10188903, maxMem=278302556
15/05/13 09:46:01 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 10.9 KB, free 255.7 MB)
15/05/13 09:46:01 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 264.1 MB)
15/05/13 09:46:01 INFO BlockManagerMaster: Updated info of block broadcast_113_piece0
15/05/13 09:46:01 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:839
15/05/13 09:46:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 75 (MapPartitionsRDD[543] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:46:01 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks
15/05/13 09:46:01 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 75, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:46:01 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 264.9 MB)
15/05/13 09:46:01 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:46:01 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 75) in 349 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:46:01 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool 
15/05/13 09:46:01 INFO DAGScheduler: Stage 75 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.351 s
15/05/13 09:46:01 INFO DAGScheduler: Job 77 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.389410 s
15/05/13 09:46:01 INFO JobScheduler: Finished job streaming job 1431524760000 ms.0 from job set of time 1431524760000 ms
15/05/13 09:46:01 INFO JobScheduler: Total delay: 1.492 s for time 1431524760000 ms (execution: 1.254 s)
15/05/13 09:46:01 INFO MapPartitionsRDD: Removing RDD 518 from persistence list
15/05/13 09:46:01 INFO BlockManager: Removing RDD 518
15/05/13 09:46:01 INFO UnionRDD: Removing RDD 517 from persistence list
15/05/13 09:46:01 INFO BlockManager: Removing RDD 517
15/05/13 09:46:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524700000 ms: 1431524640000 ms
15/05/13 09:46:01 INFO JobGenerator: Checkpointing graph for time 1431524760000 ms
15/05/13 09:46:01 INFO DStreamGraph: Updating checkpoint data for time 1431524760000 ms
15/05/13 09:46:01 INFO DStreamGraph: Updated checkpoint data for time 1431524760000 ms
15/05/13 09:46:01 INFO CheckpointWriter: Saving checkpoint for time 1431524760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524760000'
15/05/13 09:46:01 INFO CheckpointWriter: Checkpoint for time 1431524760000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524760000', took 7655 bytes and 56 ms
15/05/13 09:46:01 INFO DStreamGraph: Clearing checkpoint data for time 1431524760000 ms
15/05/13 09:46:01 INFO DStreamGraph: Cleared checkpoint data for time 1431524760000 ms
15/05/13 09:46:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:47:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 09:47:00 INFO FileInputDStream: New files at time 1431524820000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524711888.json
15/05/13 09:47:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=10200065, maxMem=278302556
15/05/13 09:47:00 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 232.9 KB, free 255.5 MB)
15/05/13 09:47:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=10438597, maxMem=278302556
15/05/13 09:47:00 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 34.9 KB, free 255.4 MB)
15/05/13 09:47:00 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.1 MB)
15/05/13 09:47:00 INFO BlockManagerMaster: Updated info of block broadcast_114_piece0
15/05/13 09:47:00 INFO SparkContext: Created broadcast 114 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:47:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:47:00 INFO JobScheduler: Added jobs for time 1431524820000 ms
15/05/13 09:47:00 INFO JobScheduler: Starting job streaming job 1431524820000 ms.0 from job set of time 1431524820000 ms
15/05/13 09:47:00 INFO JobGenerator: Checkpointing graph for time 1431524820000 ms
15/05/13 09:47:00 INFO DStreamGraph: Updating checkpoint data for time 1431524820000 ms
15/05/13 09:47:00 INFO DStreamGraph: Updated checkpoint data for time 1431524820000 ms
15/05/13 09:47:00 INFO CheckpointWriter: Saving checkpoint for time 1431524820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000'
15/05/13 09:47:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524460000
15/05/13 09:47:00 INFO CheckpointWriter: Checkpoint for time 1431524820000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000', took 7666 bytes and 79 ms
15/05/13 09:47:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:47:00 INFO DAGScheduler: Got job 78 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:47:00 INFO DAGScheduler: Final stage: Stage 76(reduce at JsonRDD.scala:51)
15/05/13 09:47:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:47:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:47:00 INFO DAGScheduler: Submitting Stage 76 (MapPartitionsRDD[550] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:47:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=10474305, maxMem=278302556
15/05/13 09:47:00 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 5.9 KB, free 255.4 MB)
15/05/13 09:47:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=10480345, maxMem=278302556
15/05/13 09:47:00 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 4.1 KB, free 255.4 MB)
15/05/13 09:47:00 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:47:00 INFO BlockManagerMaster: Updated info of block broadcast_115_piece0
15/05/13 09:47:00 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:839
15/05/13 09:47:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 76 (MapPartitionsRDD[550] at map at JsonRDD.scala:51)
15/05/13 09:47:00 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks
15/05/13 09:47:00 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 76, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:47:00 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:47:00 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.4 MB)
15/05/13 09:47:00 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 76) in 482 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:47:00 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool 
15/05/13 09:47:00 INFO DAGScheduler: Stage 76 (reduce at JsonRDD.scala:51) finished in 0.504 s
15/05/13 09:47:00 INFO DAGScheduler: Job 78 finished: reduce at JsonRDD.scala:51, took 0.541348 s
15/05/13 09:47:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:47:01 INFO DAGScheduler: Got job 79 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:47:01 INFO DAGScheduler: Final stage: Stage 77(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:47:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:47:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:47:01 INFO DAGScheduler: Submitting Stage 77 (MapPartitionsRDD[557] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:47:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=10484572, maxMem=278302556
15/05/13 09:47:01 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 21.0 KB, free 255.4 MB)
15/05/13 09:47:01 INFO MemoryStore: ensureFreeSpace(11130) called with curMem=10506052, maxMem=278302556
15/05/13 09:47:01 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 10.9 KB, free 255.4 MB)
15/05/13 09:47:01 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 264.1 MB)
15/05/13 09:47:01 INFO BlockManagerMaster: Updated info of block broadcast_116_piece0
15/05/13 09:47:01 INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:839
15/05/13 09:47:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 77 (MapPartitionsRDD[557] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:47:01 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks
15/05/13 09:47:01 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 77, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:47:01 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 265.0 MB)
15/05/13 09:47:01 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 265.0 MB)
15/05/13 09:47:01 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 77) in 250 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:47:01 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool 
15/05/13 09:47:01 INFO DAGScheduler: Stage 77 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.252 s
15/05/13 09:47:01 INFO DAGScheduler: Job 79 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.273117 s
15/05/13 09:47:01 INFO JobScheduler: Finished job streaming job 1431524820000 ms.0 from job set of time 1431524820000 ms
15/05/13 09:47:01 INFO JobScheduler: Total delay: 1.378 s for time 1431524820000 ms (execution: 1.174 s)
15/05/13 09:47:01 INFO MapPartitionsRDD: Removing RDD 532 from persistence list
15/05/13 09:47:01 INFO UnionRDD: Removing RDD 531 from persistence list
15/05/13 09:47:01 INFO BlockManager: Removing RDD 532
15/05/13 09:47:01 INFO BlockManager: Removing RDD 531
15/05/13 09:47:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524760000 ms: 1431524700000 ms
15/05/13 09:47:01 INFO JobGenerator: Checkpointing graph for time 1431524820000 ms
15/05/13 09:47:01 INFO DStreamGraph: Updating checkpoint data for time 1431524820000 ms
15/05/13 09:47:01 INFO DStreamGraph: Updated checkpoint data for time 1431524820000 ms
15/05/13 09:47:01 INFO CheckpointWriter: Saving checkpoint for time 1431524820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000'
15/05/13 09:47:07 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83051): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:47:07 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83051): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:47:07 INFO CheckpointWriter: Saving checkpoint for time 1431524820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000'
15/05/13 09:47:09 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83052): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:47:09 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83052): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:47:09 INFO CheckpointWriter: Saving checkpoint for time 1431524820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000'
15/05/13 09:47:09 INFO CheckpointWriter: Checkpoint for time 1431524820000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000', took 7657 bytes and 7783 ms
15/05/13 09:47:09 INFO DStreamGraph: Clearing checkpoint data for time 1431524820000 ms
15/05/13 09:47:09 INFO DStreamGraph: Cleared checkpoint data for time 1431524820000 ms
15/05/13 09:47:09 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:48:03 INFO FileInputDStream: Finding new files took 3758 ms
15/05/13 09:48:03 INFO FileInputDStream: New files at time 1431524880000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524775010.json
15/05/13 09:48:03 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=10517182, maxMem=278302556
15/05/13 09:48:03 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 232.9 KB, free 255.2 MB)
15/05/13 09:48:03 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=10755714, maxMem=278302556
15/05/13 09:48:03 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 34.9 KB, free 255.1 MB)
15/05/13 09:48:03 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.0 MB)
15/05/13 09:48:03 INFO BlockManagerMaster: Updated info of block broadcast_117_piece0
15/05/13 09:48:03 INFO SparkContext: Created broadcast 117 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:48:04 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:48:04 INFO JobScheduler: Added jobs for time 1431524880000 ms
15/05/13 09:48:04 INFO JobGenerator: Checkpointing graph for time 1431524880000 ms
15/05/13 09:48:04 INFO DStreamGraph: Updating checkpoint data for time 1431524880000 ms
15/05/13 09:48:04 INFO JobScheduler: Starting job streaming job 1431524880000 ms.0 from job set of time 1431524880000 ms
15/05/13 09:48:04 INFO DStreamGraph: Updated checkpoint data for time 1431524880000 ms
15/05/13 09:48:04 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:04 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:48:04 INFO DAGScheduler: Got job 80 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:48:04 INFO DAGScheduler: Final stage: Stage 78(reduce at JsonRDD.scala:51)
15/05/13 09:48:04 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:48:04 INFO DAGScheduler: Missing parents: List()
15/05/13 09:48:04 INFO DAGScheduler: Submitting Stage 78 (MapPartitionsRDD[564] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:48:04 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=10791422, maxMem=278302556
15/05/13 09:48:04 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 5.9 KB, free 255.1 MB)
15/05/13 09:48:04 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=10797462, maxMem=278302556
15/05/13 09:48:04 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 4.1 KB, free 255.1 MB)
15/05/13 09:48:05 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:48:05 INFO BlockManagerMaster: Updated info of block broadcast_118_piece0
15/05/13 09:48:05 INFO SparkContext: Created broadcast 118 from broadcast at DAGScheduler.scala:839
15/05/13 09:48:05 INFO DAGScheduler: Submitting 1 missing tasks from Stage 78 (MapPartitionsRDD[564] at map at JsonRDD.scala:51)
15/05/13 09:48:05 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks
15/05/13 09:48:05 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 78, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:48:06 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 78) in 429 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:48:06 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool 
15/05/13 09:48:06 INFO DAGScheduler: Stage 78 (reduce at JsonRDD.scala:51) finished in 0.446 s
15/05/13 09:48:06 INFO DAGScheduler: Job 80 finished: reduce at JsonRDD.scala:51, took 1.935764 s
15/05/13 09:48:06 INFO BlockManager: Removing broadcast 113
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_113_piece0
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_113_piece0 of size 11162 dropped from memory (free 267512032)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_113_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_113_piece0
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_113
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_113 of size 21480 dropped from memory (free 267533512)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_113_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 264.9 MB)
15/05/13 09:48:06 INFO ContextCleaner: Cleaned broadcast 113
15/05/13 09:48:06 INFO BlockManager: Removing broadcast 112
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_112_piece0
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_112_piece0 of size 4226 dropped from memory (free 267537738)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_112_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_112_piece0
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_112
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_112 of size 6040 dropped from memory (free 267543778)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_112_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO ContextCleaner: Cleaned broadcast 112
15/05/13 09:48:06 INFO BlockManager: Removing broadcast 116
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_116_piece0
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_116_piece0 of size 11130 dropped from memory (free 267554908)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_116_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_116_piece0
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_116
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_116 of size 21480 dropped from memory (free 267576388)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_116_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 265.0 MB)
15/05/13 09:48:06 INFO ContextCleaner: Cleaned broadcast 116
15/05/13 09:48:06 INFO BlockManager: Removing broadcast 115
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_115
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_115 of size 6040 dropped from memory (free 267582428)
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_115_piece0
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_115_piece0 of size 4227 dropped from memory (free 267586655)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_115_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_115_piece0
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_115_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO ContextCleaner: Cleaned broadcast 115
15/05/13 09:48:06 INFO BlockManager: Removing broadcast 118
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_118_piece0
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_118_piece0 of size 4224 dropped from memory (free 267590879)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_118_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_118_piece0
15/05/13 09:48:06 INFO BlockManager: Removing block broadcast_118
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_118 of size 6040 dropped from memory (free 267596919)
15/05/13 09:48:06 INFO BlockManagerInfo: Removed broadcast_118_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO ContextCleaner: Cleaned broadcast 118
15/05/13 09:48:06 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:48:06 INFO DAGScheduler: Got job 81 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:48:06 INFO DAGScheduler: Final stage: Stage 79(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:48:06 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:48:06 INFO DAGScheduler: Missing parents: List()
15/05/13 09:48:06 INFO DAGScheduler: Submitting Stage 79 (MapPartitionsRDD[571] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:48:06 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=10705637, maxMem=278302556
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 20.4 KB, free 255.2 MB)
15/05/13 09:48:06 INFO MemoryStore: ensureFreeSpace(10910) called with curMem=10726557, maxMem=278302556
15/05/13 09:48:06 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 10.7 KB, free 255.2 MB)
15/05/13 09:48:06 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 264.1 MB)
15/05/13 09:48:06 INFO BlockManagerMaster: Updated info of block broadcast_119_piece0
15/05/13 09:48:06 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:839
15/05/13 09:48:06 INFO DAGScheduler: Submitting 1 missing tasks from Stage 79 (MapPartitionsRDD[571] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:48:06 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks
15/05/13 09:48:06 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 79, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:48:06 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 264.4 MB)
15/05/13 09:48:06 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 79) in 107 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:48:06 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool 
15/05/13 09:48:06 INFO DAGScheduler: Stage 79 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.105 s
15/05/13 09:48:06 INFO DAGScheduler: Job 81 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.139089 s
15/05/13 09:48:06 INFO JobScheduler: Finished job streaming job 1431524880000 ms.0 from job set of time 1431524880000 ms
15/05/13 09:48:06 INFO JobScheduler: Total delay: 6.900 s for time 1431524880000 ms (execution: 2.534 s)
15/05/13 09:48:06 INFO MapPartitionsRDD: Removing RDD 546 from persistence list
15/05/13 09:48:06 INFO BlockManager: Removing RDD 546
15/05/13 09:48:06 INFO UnionRDD: Removing RDD 545 from persistence list
15/05/13 09:48:06 INFO BlockManager: Removing RDD 545
15/05/13 09:48:06 INFO FileInputDStream: Cleared 1 old files that were older than 1431524820000 ms: 1431524760000 ms
15/05/13 09:48:06 INFO JobGenerator: Checkpointing graph for time 1431524880000 ms
15/05/13 09:48:06 INFO DStreamGraph: Updating checkpoint data for time 1431524880000 ms
15/05/13 09:48:06 INFO DStreamGraph: Updated checkpoint data for time 1431524880000 ms
15/05/13 09:48:07 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83057): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:48:07 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83057): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:48:07 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:09 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83058): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:48:09 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83058): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:48:09 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:09 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
15/05/13 09:48:09 INFO CheckpointWriter: Checkpoint for time 1431524880000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000', took 7667 bytes and 4985 ms
15/05/13 09:48:09 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:09 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83062): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:48:09 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:09 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83064): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:48:09 INFO CheckpointWriter: Saving checkpoint for time 1431524880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:48:09 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83066): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:48:09 WARN CheckpointWriter: Could not write checkpoint for time 1431524880000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000'
15/05/13 09:49:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 09:49:00 INFO FileInputDStream: New files at time 1431524940000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524838461.json
15/05/13 09:49:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=10737467, maxMem=278302556
15/05/13 09:49:00 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 232.9 KB, free 254.9 MB)
15/05/13 09:49:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=10975999, maxMem=278302556
15/05/13 09:49:00 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 34.9 KB, free 254.9 MB)
15/05/13 09:49:00 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.0 MB)
15/05/13 09:49:00 INFO BlockManagerMaster: Updated info of block broadcast_120_piece0
15/05/13 09:49:00 INFO SparkContext: Created broadcast 120 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:49:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:49:00 INFO JobScheduler: Added jobs for time 1431524940000 ms
15/05/13 09:49:00 INFO JobGenerator: Checkpointing graph for time 1431524940000 ms
15/05/13 09:49:00 INFO DStreamGraph: Updating checkpoint data for time 1431524940000 ms
15/05/13 09:49:00 INFO DStreamGraph: Updated checkpoint data for time 1431524940000 ms
15/05/13 09:49:00 INFO JobScheduler: Starting job streaming job 1431524940000 ms.0 from job set of time 1431524940000 ms
15/05/13 09:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431524940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000'
15/05/13 09:49:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:49:00 INFO DAGScheduler: Got job 82 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:49:00 INFO DAGScheduler: Final stage: Stage 80(reduce at JsonRDD.scala:51)
15/05/13 09:49:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:49:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:49:00 INFO DAGScheduler: Submitting Stage 80 (MapPartitionsRDD[578] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:49:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=11011709, maxMem=278302556
15/05/13 09:49:00 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 5.9 KB, free 254.9 MB)
15/05/13 09:49:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=11017749, maxMem=278302556
15/05/13 09:49:00 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 4.1 KB, free 254.9 MB)
15/05/13 09:49:00 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:49:00 INFO BlockManagerMaster: Updated info of block broadcast_121_piece0
15/05/13 09:49:00 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:839
15/05/13 09:49:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 80 (MapPartitionsRDD[578] at map at JsonRDD.scala:51)
15/05/13 09:49:00 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks
15/05/13 09:49:00 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 80, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:49:00 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:49:00 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:49:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83070): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:49:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83070): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431524940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000'
15/05/13 09:49:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83072): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431524940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000'
15/05/13 09:49:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83074): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:49:00 WARN CheckpointWriter: Could not write checkpoint for time 1431524940000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000'
15/05/13 09:49:00 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 80) in 560 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:49:00 INFO DAGScheduler: Stage 80 (reduce at JsonRDD.scala:51) finished in 0.569 s
15/05/13 09:49:00 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool 
15/05/13 09:49:00 INFO DAGScheduler: Job 82 finished: reduce at JsonRDD.scala:51, took 0.606434 s
15/05/13 09:49:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:49:01 INFO DAGScheduler: Got job 83 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:49:01 INFO DAGScheduler: Final stage: Stage 81(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:49:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:49:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:49:01 INFO DAGScheduler: Submitting Stage 81 (MapPartitionsRDD[585] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:49:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=11021973, maxMem=278302556
15/05/13 09:49:01 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 21.0 KB, free 254.9 MB)
15/05/13 09:49:01 INFO MemoryStore: ensureFreeSpace(11140) called with curMem=11043453, maxMem=278302556
15/05/13 09:49:01 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 10.9 KB, free 254.9 MB)
15/05/13 09:49:01 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 264.0 MB)
15/05/13 09:49:01 INFO BlockManagerMaster: Updated info of block broadcast_122_piece0
15/05/13 09:49:01 INFO SparkContext: Created broadcast 122 from broadcast at DAGScheduler.scala:839
15/05/13 09:49:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 81 (MapPartitionsRDD[585] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:49:01 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks
15/05/13 09:49:01 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 81, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:49:01 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 264.4 MB)
15/05/13 09:49:01 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:49:01 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 81) in 353 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:49:01 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool 
15/05/13 09:49:01 INFO DAGScheduler: Stage 81 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.353 s
15/05/13 09:49:01 INFO DAGScheduler: Job 83 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.383942 s
15/05/13 09:49:01 INFO JobScheduler: Finished job streaming job 1431524940000 ms.0 from job set of time 1431524940000 ms
15/05/13 09:49:01 INFO JobScheduler: Total delay: 1.473 s for time 1431524940000 ms (execution: 1.278 s)
15/05/13 09:49:01 INFO MapPartitionsRDD: Removing RDD 560 from persistence list
15/05/13 09:49:01 INFO UnionRDD: Removing RDD 559 from persistence list
15/05/13 09:49:01 INFO BlockManager: Removing RDD 560
15/05/13 09:49:01 INFO BlockManager: Removing RDD 559
15/05/13 09:49:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524880000 ms: 1431524820000 ms
15/05/13 09:49:01 INFO JobGenerator: Checkpointing graph for time 1431524940000 ms
15/05/13 09:49:01 INFO DStreamGraph: Updating checkpoint data for time 1431524940000 ms
15/05/13 09:49:01 INFO DStreamGraph: Updated checkpoint data for time 1431524940000 ms
15/05/13 09:49:01 INFO CheckpointWriter: Saving checkpoint for time 1431524940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000'
15/05/13 09:49:01 INFO CheckpointWriter: Checkpoint for time 1431524940000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524940000', took 7655 bytes and 58 ms
15/05/13 09:49:01 INFO DStreamGraph: Clearing checkpoint data for time 1431524940000 ms
15/05/13 09:49:01 INFO DStreamGraph: Cleared checkpoint data for time 1431524940000 ms
15/05/13 09:49:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:50:00 INFO FileInputDStream: Finding new files took 45 ms
15/05/13 09:50:00 INFO FileInputDStream: New files at time 1431525000000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524902284.json
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=11054593, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 232.9 KB, free 254.6 MB)
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=11293125, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 34.9 KB, free 254.6 MB)
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_123_piece0
15/05/13 09:50:00 INFO SparkContext: Created broadcast 123 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:50:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:50:00 INFO JobScheduler: Added jobs for time 1431525000000 ms
15/05/13 09:50:00 INFO JobScheduler: Starting job streaming job 1431525000000 ms.0 from job set of time 1431525000000 ms
15/05/13 09:50:00 INFO JobGenerator: Checkpointing graph for time 1431525000000 ms
15/05/13 09:50:00 INFO DStreamGraph: Updating checkpoint data for time 1431525000000 ms
15/05/13 09:50:00 INFO DStreamGraph: Updated checkpoint data for time 1431525000000 ms
15/05/13 09:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431525000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000'
15/05/13 09:50:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:50:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83081): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431525000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000'
15/05/13 09:50:00 INFO DAGScheduler: Got job 84 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:50:00 INFO DAGScheduler: Final stage: Stage 82(reduce at JsonRDD.scala:51)
15/05/13 09:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:50:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:50:00 INFO DAGScheduler: Submitting Stage 82 (MapPartitionsRDD[592] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=11328833, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 5.9 KB, free 254.6 MB)
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=11334873, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 4.1 KB, free 254.6 MB)
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_124_piece0
15/05/13 09:50:00 INFO SparkContext: Created broadcast 124 from broadcast at DAGScheduler.scala:839
15/05/13 09:50:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 82 (MapPartitionsRDD[592] at map at JsonRDD.scala:51)
15/05/13 09:50:00 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks
15/05/13 09:50:00 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 82, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:50:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83083): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431525000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000'
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 265.0 MB)
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:50:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524640000
15/05/13 09:50:00 INFO CheckpointWriter: Checkpoint for time 1431525000000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000', took 7666 bytes and 234 ms
15/05/13 09:50:00 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 82) in 292 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:50:00 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool 
15/05/13 09:50:00 INFO DAGScheduler: Stage 82 (reduce at JsonRDD.scala:51) finished in 0.304 s
15/05/13 09:50:00 INFO DAGScheduler: Job 84 finished: reduce at JsonRDD.scala:51, took 0.346133 s
15/05/13 09:50:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:50:00 INFO DAGScheduler: Got job 85 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:50:00 INFO DAGScheduler: Final stage: Stage 83(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:50:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:50:00 INFO DAGScheduler: Submitting Stage 83 (MapPartitionsRDD[599] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(21320) called with curMem=11339099, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 20.8 KB, free 254.6 MB)
15/05/13 09:50:00 INFO MemoryStore: ensureFreeSpace(11040) called with curMem=11360419, maxMem=278302556
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 10.8 KB, free 254.6 MB)
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManager: Removing broadcast 119
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_119_piece0
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_119_piece0 of size 10910 dropped from memory (free 266942007)
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_119_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_119_piece0
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_119
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_119 of size 20920 dropped from memory (free 266962927)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_125_piece0
15/05/13 09:50:00 INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:839
15/05/13 09:50:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 83 (MapPartitionsRDD[599] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:50:00 INFO TaskSchedulerImpl: Adding task set 83.0 with 1 tasks
15/05/13 09:50:00 INFO TaskSetManager: Starting task 0.0 in stage 83.0 (TID 83, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_119_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 264.3 MB)
15/05/13 09:50:00 INFO ContextCleaner: Cleaned broadcast 119
15/05/13 09:50:00 INFO BlockManager: Removing broadcast 122
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_122
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_122 of size 21480 dropped from memory (free 266984407)
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_122_piece0
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_122_piece0 of size 11140 dropped from memory (free 266995547)
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_122_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_122_piece0
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_122_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 264.4 MB)
15/05/13 09:50:00 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.8 KB, free: 264.9 MB)
15/05/13 09:50:00 INFO ContextCleaner: Cleaned broadcast 122
15/05/13 09:50:00 INFO BlockManager: Removing broadcast 121
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_121
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_121 of size 6040 dropped from memory (free 267001587)
15/05/13 09:50:00 INFO BlockManager: Removing block broadcast_121_piece0
15/05/13 09:50:00 INFO MemoryStore: Block broadcast_121_piece0 of size 4224 dropped from memory (free 267005811)
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_121_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:50:00 INFO BlockManagerMaster: Updated info of block broadcast_121_piece0
15/05/13 09:50:00 INFO BlockManagerInfo: Removed broadcast_121_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:50:01 INFO ContextCleaner: Cleaned broadcast 121
15/05/13 09:50:01 INFO BlockManager: Removing broadcast 124
15/05/13 09:50:01 INFO BlockManager: Removing block broadcast_124
15/05/13 09:50:01 INFO MemoryStore: Block broadcast_124 of size 6040 dropped from memory (free 267011851)
15/05/13 09:50:01 INFO BlockManager: Removing block broadcast_124_piece0
15/05/13 09:50:01 INFO MemoryStore: Block broadcast_124_piece0 of size 4226 dropped from memory (free 267016077)
15/05/13 09:50:01 INFO BlockManagerInfo: Removed broadcast_124_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:50:01 INFO BlockManagerMaster: Updated info of block broadcast_124_piece0
15/05/13 09:50:01 INFO BlockManagerInfo: Removed broadcast_124_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:50:01 INFO ContextCleaner: Cleaned broadcast 124
15/05/13 09:50:01 INFO TaskSetManager: Finished task 0.0 in stage 83.0 (TID 83) in 106 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:50:01 INFO TaskSchedulerImpl: Removed TaskSet 83.0, whose tasks have all completed, from pool 
15/05/13 09:50:01 INFO DAGScheduler: Stage 83 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.106 s
15/05/13 09:50:01 INFO DAGScheduler: Job 85 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.195229 s
15/05/13 09:50:01 INFO JobScheduler: Finished job streaming job 1431525000000 ms.0 from job set of time 1431525000000 ms
15/05/13 09:50:01 INFO JobScheduler: Total delay: 1.118 s for time 1431525000000 ms (execution: 0.945 s)
15/05/13 09:50:01 INFO MapPartitionsRDD: Removing RDD 574 from persistence list
15/05/13 09:50:01 INFO BlockManager: Removing RDD 574
15/05/13 09:50:01 INFO UnionRDD: Removing RDD 573 from persistence list
15/05/13 09:50:01 INFO BlockManager: Removing RDD 573
15/05/13 09:50:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431524940000 ms: 1431524880000 ms
15/05/13 09:50:01 INFO JobGenerator: Checkpointing graph for time 1431525000000 ms
15/05/13 09:50:01 INFO DStreamGraph: Updating checkpoint data for time 1431525000000 ms
15/05/13 09:50:01 INFO DStreamGraph: Updated checkpoint data for time 1431525000000 ms
15/05/13 09:50:01 INFO CheckpointWriter: Saving checkpoint for time 1431525000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000'
15/05/13 09:50:01 INFO CheckpointWriter: Checkpoint for time 1431525000000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525000000', took 7657 bytes and 64 ms
15/05/13 09:50:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525000000 ms
15/05/13 09:50:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525000000 ms
15/05/13 09:50:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:51:00 INFO FileInputDStream: Finding new files took 484 ms
15/05/13 09:51:00 INFO FileInputDStream: New files at time 1431525060000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431524968465.json
15/05/13 09:51:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=11286479, maxMem=278302556
15/05/13 09:51:00 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 232.9 KB, free 254.4 MB)
15/05/13 09:51:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=11525011, maxMem=278302556
15/05/13 09:51:00 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 34.9 KB, free 254.4 MB)
15/05/13 09:51:00 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 264.0 MB)
15/05/13 09:51:00 INFO BlockManagerMaster: Updated info of block broadcast_126_piece0
15/05/13 09:51:00 INFO SparkContext: Created broadcast 126 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:51:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:51:00 INFO JobScheduler: Added jobs for time 1431525060000 ms
15/05/13 09:51:00 INFO JobGenerator: Checkpointing graph for time 1431525060000 ms
15/05/13 09:51:00 INFO JobScheduler: Starting job streaming job 1431525060000 ms.0 from job set of time 1431525060000 ms
15/05/13 09:51:00 INFO DStreamGraph: Updating checkpoint data for time 1431525060000 ms
15/05/13 09:51:00 INFO DStreamGraph: Updated checkpoint data for time 1431525060000 ms
15/05/13 09:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431525060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000'
15/05/13 09:51:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:51:00 INFO DAGScheduler: Got job 86 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:51:00 INFO DAGScheduler: Final stage: Stage 84(reduce at JsonRDD.scala:51)
15/05/13 09:51:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:51:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:51:00 INFO DAGScheduler: Submitting Stage 84 (MapPartitionsRDD[606] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:51:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=11560719, maxMem=278302556
15/05/13 09:51:00 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 5.9 KB, free 254.4 MB)
15/05/13 09:51:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=11566759, maxMem=278302556
15/05/13 09:51:00 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 4.1 KB, free 254.4 MB)
15/05/13 09:51:00 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 264.0 MB)
15/05/13 09:51:00 INFO BlockManagerMaster: Updated info of block broadcast_127_piece0
15/05/13 09:51:00 INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:839
15/05/13 09:51:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 84 (MapPartitionsRDD[606] at map at JsonRDD.scala:51)
15/05/13 09:51:00 INFO TaskSchedulerImpl: Adding task set 84.0 with 1 tasks
15/05/13 09:51:00 INFO TaskSetManager: Starting task 0.0 in stage 84.0 (TID 84, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:51:00 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.4 MB)
15/05/13 09:51:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83096): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431525060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000'
15/05/13 09:51:00 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:51:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83098): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:51:01 INFO CheckpointWriter: Saving checkpoint for time 1431525060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000'
15/05/13 09:51:01 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524700000
15/05/13 09:51:01 INFO CheckpointWriter: Checkpoint for time 1431525060000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000', took 7665 bytes and 546 ms
15/05/13 09:51:01 INFO TaskSetManager: Finished task 0.0 in stage 84.0 (TID 84) in 658 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:51:01 INFO TaskSchedulerImpl: Removed TaskSet 84.0, whose tasks have all completed, from pool 
15/05/13 09:51:01 INFO DAGScheduler: Stage 84 (reduce at JsonRDD.scala:51) finished in 0.673 s
15/05/13 09:51:01 INFO DAGScheduler: Job 86 finished: reduce at JsonRDD.scala:51, took 0.703817 s
15/05/13 09:51:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:51:01 INFO DAGScheduler: Got job 87 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:51:01 INFO DAGScheduler: Final stage: Stage 85(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:51:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:51:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:51:01 INFO DAGScheduler: Submitting Stage 85 (MapPartitionsRDD[613] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:51:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=11570985, maxMem=278302556
15/05/13 09:51:01 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 20.6 KB, free 254.4 MB)
15/05/13 09:51:01 INFO MemoryStore: ensureFreeSpace(11022) called with curMem=11592081, maxMem=278302556
15/05/13 09:51:01 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 10.8 KB, free 254.3 MB)
15/05/13 09:51:01 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 264.0 MB)
15/05/13 09:51:01 INFO BlockManagerMaster: Updated info of block broadcast_128_piece0
15/05/13 09:51:01 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:839
15/05/13 09:51:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 85 (MapPartitionsRDD[613] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:51:01 INFO TaskSchedulerImpl: Adding task set 85.0 with 1 tasks
15/05/13 09:51:01 INFO TaskSetManager: Starting task 0.0 in stage 85.0 (TID 85, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:51:01 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.8 KB, free: 264.9 MB)
15/05/13 09:51:01 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:51:01 INFO TaskSetManager: Finished task 0.0 in stage 85.0 (TID 85) in 245 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:51:01 INFO TaskSchedulerImpl: Removed TaskSet 85.0, whose tasks have all completed, from pool 
15/05/13 09:51:02 INFO DAGScheduler: Stage 85 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.247 s
15/05/13 09:51:02 INFO DAGScheduler: Job 87 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.280418 s
15/05/13 09:51:02 INFO JobScheduler: Finished job streaming job 1431525060000 ms.0 from job set of time 1431525060000 ms
15/05/13 09:51:02 INFO JobScheduler: Total delay: 2.049 s for time 1431525060000 ms (execution: 1.376 s)
15/05/13 09:51:02 INFO MapPartitionsRDD: Removing RDD 588 from persistence list
15/05/13 09:51:02 INFO BlockManager: Removing RDD 588
15/05/13 09:51:02 INFO UnionRDD: Removing RDD 587 from persistence list
15/05/13 09:51:02 INFO BlockManager: Removing RDD 587
15/05/13 09:51:02 INFO FileInputDStream: Cleared 1 old files that were older than 1431525000000 ms: 1431524940000 ms
15/05/13 09:51:02 INFO JobGenerator: Checkpointing graph for time 1431525060000 ms
15/05/13 09:51:02 INFO DStreamGraph: Updating checkpoint data for time 1431525060000 ms
15/05/13 09:51:02 INFO DStreamGraph: Updated checkpoint data for time 1431525060000 ms
15/05/13 09:51:02 INFO CheckpointWriter: Saving checkpoint for time 1431525060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000'
15/05/13 09:51:02 INFO CheckpointWriter: Checkpoint for time 1431525060000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000', took 7654 bytes and 73 ms
15/05/13 09:51:02 INFO DStreamGraph: Clearing checkpoint data for time 1431525060000 ms
15/05/13 09:51:02 INFO DStreamGraph: Cleared checkpoint data for time 1431525060000 ms
15/05/13 09:51:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:52:00 INFO FileInputDStream: Finding new files took 55 ms
15/05/13 09:52:00 INFO FileInputDStream: New files at time 1431525120000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525032650.json
15/05/13 09:52:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=11603103, maxMem=278302556
15/05/13 09:52:00 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 232.9 KB, free 254.1 MB)
15/05/13 09:52:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=11841635, maxMem=278302556
15/05/13 09:52:00 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 34.9 KB, free 254.1 MB)
15/05/13 09:52:00 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.9 MB)
15/05/13 09:52:00 INFO BlockManagerMaster: Updated info of block broadcast_129_piece0
15/05/13 09:52:00 INFO SparkContext: Created broadcast 129 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:52:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:52:00 INFO JobScheduler: Added jobs for time 1431525120000 ms
15/05/13 09:52:00 INFO JobGenerator: Checkpointing graph for time 1431525120000 ms
15/05/13 09:52:00 INFO DStreamGraph: Updating checkpoint data for time 1431525120000 ms
15/05/13 09:52:00 INFO DStreamGraph: Updated checkpoint data for time 1431525120000 ms
15/05/13 09:52:00 INFO JobScheduler: Starting job streaming job 1431525120000 ms.0 from job set of time 1431525120000 ms
15/05/13 09:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431525120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525120000'
15/05/13 09:52:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524820000.bk
15/05/13 09:52:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:52:00 INFO DAGScheduler: Got job 88 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:52:00 INFO DAGScheduler: Final stage: Stage 86(reduce at JsonRDD.scala:51)
15/05/13 09:52:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:52:00 INFO CheckpointWriter: Checkpoint for time 1431525120000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525120000', took 7662 bytes and 68 ms
15/05/13 09:52:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:52:00 INFO DAGScheduler: Submitting Stage 86 (MapPartitionsRDD[620] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:52:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=11877343, maxMem=278302556
15/05/13 09:52:00 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 5.9 KB, free 254.1 MB)
15/05/13 09:52:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=11883383, maxMem=278302556
15/05/13 09:52:00 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 4.1 KB, free 254.1 MB)
15/05/13 09:52:00 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:52:00 INFO BlockManagerMaster: Updated info of block broadcast_130_piece0
15/05/13 09:52:00 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:839
15/05/13 09:52:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 86 (MapPartitionsRDD[620] at map at JsonRDD.scala:51)
15/05/13 09:52:00 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks
15/05/13 09:52:00 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 86, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:52:00 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:52:00 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.3 MB)
15/05/13 09:52:00 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 86) in 523 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:52:00 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool 
15/05/13 09:52:00 INFO DAGScheduler: Stage 86 (reduce at JsonRDD.scala:51) finished in 0.542 s
15/05/13 09:52:00 INFO DAGScheduler: Job 88 finished: reduce at JsonRDD.scala:51, took 0.584496 s
15/05/13 09:52:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:52:01 INFO DAGScheduler: Got job 89 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:52:01 INFO DAGScheduler: Final stage: Stage 87(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:52:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:52:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:52:01 INFO DAGScheduler: Submitting Stage 87 (MapPartitionsRDD[627] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:52:01 INFO MemoryStore: ensureFreeSpace(22064) called with curMem=11887609, maxMem=278302556
15/05/13 09:52:01 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 21.5 KB, free 254.1 MB)
15/05/13 09:52:01 INFO MemoryStore: ensureFreeSpace(11387) called with curMem=11909673, maxMem=278302556
15/05/13 09:52:01 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 11.1 KB, free 254.0 MB)
15/05/13 09:52:01 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.1 KB, free: 263.9 MB)
15/05/13 09:52:01 INFO BlockManagerMaster: Updated info of block broadcast_131_piece0
15/05/13 09:52:01 INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:839
15/05/13 09:52:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 87 (MapPartitionsRDD[627] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:52:01 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks
15/05/13 09:52:01 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 87, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:52:01 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.1 KB, free: 264.3 MB)
15/05/13 09:52:01 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 87) in 135 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:52:01 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool 
15/05/13 09:52:01 INFO DAGScheduler: Stage 87 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.137 s
15/05/13 09:52:01 INFO DAGScheduler: Job 89 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.173827 s
15/05/13 09:52:01 INFO JobScheduler: Finished job streaming job 1431525120000 ms.0 from job set of time 1431525120000 ms
15/05/13 09:52:01 INFO JobScheduler: Total delay: 1.368 s for time 1431525120000 ms (execution: 1.140 s)
15/05/13 09:52:01 INFO MapPartitionsRDD: Removing RDD 602 from persistence list
15/05/13 09:52:01 INFO BlockManager: Removing RDD 602
15/05/13 09:52:01 INFO UnionRDD: Removing RDD 601 from persistence list
15/05/13 09:52:01 INFO BlockManager: Removing RDD 601
15/05/13 09:52:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525060000 ms: 1431525000000 ms
15/05/13 09:52:01 INFO JobGenerator: Checkpointing graph for time 1431525120000 ms
15/05/13 09:52:01 INFO DStreamGraph: Updating checkpoint data for time 1431525120000 ms
15/05/13 09:52:01 INFO DStreamGraph: Updated checkpoint data for time 1431525120000 ms
15/05/13 09:52:01 INFO CheckpointWriter: Saving checkpoint for time 1431525120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525120000'
15/05/13 09:52:01 INFO CheckpointWriter: Checkpoint for time 1431525120000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525120000', took 7656 bytes and 72 ms
15/05/13 09:52:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525120000 ms
15/05/13 09:52:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525120000 ms
15/05/13 09:52:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:53:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 09:53:00 INFO FileInputDStream: New files at time 1431525180000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525097184.json
15/05/13 09:53:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=11921060, maxMem=278302556
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 232.9 KB, free 253.8 MB)
15/05/13 09:53:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=12159592, maxMem=278302556
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 34.9 KB, free 253.8 MB)
15/05/13 09:53:00 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_132_piece0
15/05/13 09:53:00 INFO SparkContext: Created broadcast 132 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:53:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:53:00 INFO JobScheduler: Starting job streaming job 1431525180000 ms.0 from job set of time 1431525180000 ms
15/05/13 09:53:00 INFO JobScheduler: Added jobs for time 1431525180000 ms
15/05/13 09:53:00 INFO JobGenerator: Checkpointing graph for time 1431525180000 ms
15/05/13 09:53:00 INFO DStreamGraph: Updating checkpoint data for time 1431525180000 ms
15/05/13 09:53:00 INFO DStreamGraph: Updated checkpoint data for time 1431525180000 ms
15/05/13 09:53:00 INFO CheckpointWriter: Saving checkpoint for time 1431525180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525180000'
15/05/13 09:53:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:53:00 INFO DAGScheduler: Got job 90 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:53:00 INFO DAGScheduler: Final stage: Stage 88(reduce at JsonRDD.scala:51)
15/05/13 09:53:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:53:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:53:00 INFO DAGScheduler: Submitting Stage 88 (MapPartitionsRDD[634] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:53:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12195300, maxMem=278302556
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_133 stored as values in memory (estimated size 5.9 KB, free 253.8 MB)
15/05/13 09:53:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=12201340, maxMem=278302556
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_133_piece0 stored as bytes in memory (estimated size 4.1 KB, free 253.8 MB)
15/05/13 09:53:00 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_133_piece0
15/05/13 09:53:00 INFO SparkContext: Created broadcast 133 from broadcast at DAGScheduler.scala:839
15/05/13 09:53:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 88 (MapPartitionsRDD[634] at map at JsonRDD.scala:51)
15/05/13 09:53:00 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks
15/05/13 09:53:00 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 88, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:53:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431524880000
15/05/13 09:53:00 INFO CheckpointWriter: Checkpoint for time 1431525180000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525180000', took 7669 bytes and 87 ms
15/05/13 09:53:00 INFO BlockManagerInfo: Added broadcast_133_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:53:00 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:53:00 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 88) in 445 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:53:00 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool 
15/05/13 09:53:00 INFO DAGScheduler: Stage 88 (reduce at JsonRDD.scala:51) finished in 0.465 s
15/05/13 09:53:00 INFO DAGScheduler: Job 90 finished: reduce at JsonRDD.scala:51, took 0.493054 s
15/05/13 09:53:00 INFO BlockManager: Removing broadcast 125
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_125
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_125 of size 21320 dropped from memory (free 266118310)
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_125_piece0
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_125_piece0 of size 11040 dropped from memory (free 266129350)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_125_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_125_piece0
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_125_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.8 KB, free: 264.9 MB)
15/05/13 09:53:00 INFO ContextCleaner: Cleaned broadcast 125
15/05/13 09:53:00 INFO BlockManager: Removing broadcast 128
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_128_piece0
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_128_piece0 of size 11022 dropped from memory (free 266140372)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_128_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_128_piece0
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_128
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_128 of size 21096 dropped from memory (free 266161468)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_128_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.8 KB, free: 264.9 MB)
15/05/13 09:53:00 INFO ContextCleaner: Cleaned broadcast 128
15/05/13 09:53:00 INFO BlockManager: Removing broadcast 127
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_127
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_127 of size 6040 dropped from memory (free 266167508)
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_127_piece0
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_127_piece0 of size 4226 dropped from memory (free 266171734)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_127_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_127_piece0
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_127_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:53:00 INFO ContextCleaner: Cleaned broadcast 127
15/05/13 09:53:00 INFO BlockManager: Removing broadcast 133
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_133_piece0
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_133_piece0 of size 4226 dropped from memory (free 266175960)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_133_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_133_piece0
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_133
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_133 of size 6040 dropped from memory (free 266182000)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_133_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:53:00 INFO ContextCleaner: Cleaned broadcast 133
15/05/13 09:53:00 INFO BlockManager: Removing broadcast 131
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_131_piece0
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_131_piece0 of size 11387 dropped from memory (free 266193387)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_131_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.1 KB, free: 263.9 MB)
15/05/13 09:53:00 INFO BlockManagerMaster: Updated info of block broadcast_131_piece0
15/05/13 09:53:00 INFO BlockManager: Removing block broadcast_131
15/05/13 09:53:00 INFO MemoryStore: Block broadcast_131 of size 22064 dropped from memory (free 266215451)
15/05/13 09:53:00 INFO BlockManagerInfo: Removed broadcast_131_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.1 KB, free: 264.2 MB)
15/05/13 09:53:01 INFO ContextCleaner: Cleaned broadcast 131
15/05/13 09:53:01 INFO BlockManager: Removing broadcast 130
15/05/13 09:53:01 INFO BlockManager: Removing block broadcast_130
15/05/13 09:53:01 INFO MemoryStore: Block broadcast_130 of size 6040 dropped from memory (free 266221491)
15/05/13 09:53:01 INFO BlockManager: Removing block broadcast_130_piece0
15/05/13 09:53:01 INFO MemoryStore: Block broadcast_130_piece0 of size 4226 dropped from memory (free 266225717)
15/05/13 09:53:01 INFO BlockManagerInfo: Removed broadcast_130_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:53:01 INFO BlockManagerMaster: Updated info of block broadcast_130_piece0
15/05/13 09:53:01 INFO BlockManagerInfo: Removed broadcast_130_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 09:53:01 INFO ContextCleaner: Cleaned broadcast 130
15/05/13 09:53:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:53:01 INFO DAGScheduler: Got job 91 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:53:01 INFO DAGScheduler: Final stage: Stage 89(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:53:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:53:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:53:01 INFO DAGScheduler: Submitting Stage 89 (MapPartitionsRDD[641] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:53:01 INFO MemoryStore: ensureFreeSpace(21856) called with curMem=12076839, maxMem=278302556
15/05/13 09:53:01 INFO MemoryStore: Block broadcast_134 stored as values in memory (estimated size 21.3 KB, free 253.9 MB)
15/05/13 09:53:01 INFO MemoryStore: ensureFreeSpace(11119) called with curMem=12098695, maxMem=278302556
15/05/13 09:53:01 INFO MemoryStore: Block broadcast_134_piece0 stored as bytes in memory (estimated size 10.9 KB, free 253.9 MB)
15/05/13 09:53:01 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 263.9 MB)
15/05/13 09:53:01 INFO BlockManagerMaster: Updated info of block broadcast_134_piece0
15/05/13 09:53:01 INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:839
15/05/13 09:53:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 89 (MapPartitionsRDD[641] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:53:01 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks
15/05/13 09:53:01 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 89, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:53:01 INFO BlockManagerInfo: Added broadcast_134_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.9 MB)
15/05/13 09:53:01 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.9 MB)
15/05/13 09:53:01 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 89) in 212 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 09:53:01 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool 
15/05/13 09:53:01 INFO DAGScheduler: Stage 89 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.213 s
15/05/13 09:53:01 INFO DAGScheduler: Job 91 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.231939 s
15/05/13 09:53:01 INFO JobScheduler: Finished job streaming job 1431525180000 ms.0 from job set of time 1431525180000 ms
15/05/13 09:53:01 INFO JobScheduler: Total delay: 1.290 s for time 1431525180000 ms (execution: 1.081 s)
15/05/13 09:53:01 INFO MapPartitionsRDD: Removing RDD 616 from persistence list
15/05/13 09:53:01 INFO BlockManager: Removing RDD 616
15/05/13 09:53:01 INFO UnionRDD: Removing RDD 615 from persistence list
15/05/13 09:53:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525120000 ms: 1431525060000 ms
15/05/13 09:53:01 INFO BlockManager: Removing RDD 615
15/05/13 09:53:01 INFO JobGenerator: Checkpointing graph for time 1431525180000 ms
15/05/13 09:53:01 INFO DStreamGraph: Updating checkpoint data for time 1431525180000 ms
15/05/13 09:53:01 INFO DStreamGraph: Updated checkpoint data for time 1431525180000 ms
15/05/13 09:53:01 INFO CheckpointWriter: Saving checkpoint for time 1431525180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525180000'
15/05/13 09:53:01 INFO CheckpointWriter: Checkpoint for time 1431525180000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525180000', took 7659 bytes and 54 ms
15/05/13 09:53:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525180000 ms
15/05/13 09:53:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525180000 ms
15/05/13 09:53:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:54:00 INFO FileInputDStream: Finding new files took 48 ms
15/05/13 09:54:00 INFO FileInputDStream: New files at time 1431525240000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525159741.json
15/05/13 09:54:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=12109814, maxMem=278302556
15/05/13 09:54:00 INFO MemoryStore: Block broadcast_135 stored as values in memory (estimated size 232.9 KB, free 253.6 MB)
15/05/13 09:54:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=12348346, maxMem=278302556
15/05/13 09:54:00 INFO MemoryStore: Block broadcast_135_piece0 stored as bytes in memory (estimated size 34.9 KB, free 253.6 MB)
15/05/13 09:54:00 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.9 MB)
15/05/13 09:54:00 INFO BlockManagerMaster: Updated info of block broadcast_135_piece0
15/05/13 09:54:00 INFO SparkContext: Created broadcast 135 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:54:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:54:00 INFO JobScheduler: Starting job streaming job 1431525240000 ms.0 from job set of time 1431525240000 ms
15/05/13 09:54:00 INFO JobScheduler: Added jobs for time 1431525240000 ms
15/05/13 09:54:00 INFO JobGenerator: Checkpointing graph for time 1431525240000 ms
15/05/13 09:54:00 INFO DStreamGraph: Updating checkpoint data for time 1431525240000 ms
15/05/13 09:54:00 INFO DStreamGraph: Updated checkpoint data for time 1431525240000 ms
15/05/13 09:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431525240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000'
15/05/13 09:54:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83116): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431525240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000'
15/05/13 09:54:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:54:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431525240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000'
15/05/13 09:54:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:54:00 INFO DAGScheduler: Got job 92 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:54:00 INFO DAGScheduler: Final stage: Stage 90(reduce at JsonRDD.scala:51)
15/05/13 09:54:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:54:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:54:00 INFO DAGScheduler: Submitting Stage 90 (MapPartitionsRDD[648] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:54:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12384054, maxMem=278302556
15/05/13 09:54:00 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 5.9 KB, free 253.6 MB)
15/05/13 09:54:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83120): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:54:00 WARN CheckpointWriter: Could not write checkpoint for time 1431525240000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000'
15/05/13 09:54:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=12390094, maxMem=278302556
15/05/13 09:54:00 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 4.1 KB, free 253.6 MB)
15/05/13 09:54:00 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.9 MB)
15/05/13 09:54:00 INFO BlockManagerMaster: Updated info of block broadcast_136_piece0
15/05/13 09:54:00 INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:839
15/05/13 09:54:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 90 (MapPartitionsRDD[648] at map at JsonRDD.scala:51)
15/05/13 09:54:00 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks
15/05/13 09:54:00 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 90, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:54:00 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:54:00 INFO BlockManagerInfo: Added broadcast_135_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:54:00 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 90) in 590 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:54:00 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool 
15/05/13 09:54:00 INFO DAGScheduler: Stage 90 (reduce at JsonRDD.scala:51) finished in 0.601 s
15/05/13 09:54:00 INFO DAGScheduler: Job 92 finished: reduce at JsonRDD.scala:51, took 0.653755 s
15/05/13 09:54:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:54:01 INFO DAGScheduler: Got job 93 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:54:01 INFO DAGScheduler: Final stage: Stage 91(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:54:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:54:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:54:01 INFO DAGScheduler: Submitting Stage 91 (MapPartitionsRDD[655] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:54:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=12394320, maxMem=278302556
15/05/13 09:54:01 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 20.4 KB, free 253.6 MB)
15/05/13 09:54:01 INFO MemoryStore: ensureFreeSpace(10830) called with curMem=12415240, maxMem=278302556
15/05/13 09:54:01 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 10.6 KB, free 253.6 MB)
15/05/13 09:54:01 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.8 MB)
15/05/13 09:54:01 INFO BlockManagerMaster: Updated info of block broadcast_137_piece0
15/05/13 09:54:01 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:839
15/05/13 09:54:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 91 (MapPartitionsRDD[655] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:54:01 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks
15/05/13 09:54:01 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 91, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:54:01 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:54:01 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 91) in 146 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:54:01 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool 
15/05/13 09:54:01 INFO DAGScheduler: Stage 91 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.148 s
15/05/13 09:54:01 INFO DAGScheduler: Job 93 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.181113 s
15/05/13 09:54:01 INFO JobScheduler: Finished job streaming job 1431525240000 ms.0 from job set of time 1431525240000 ms
15/05/13 09:54:01 INFO JobScheduler: Total delay: 1.402 s for time 1431525240000 ms (execution: 1.205 s)
15/05/13 09:54:01 INFO MapPartitionsRDD: Removing RDD 630 from persistence list
15/05/13 09:54:01 INFO BlockManager: Removing RDD 630
15/05/13 09:54:01 INFO UnionRDD: Removing RDD 629 from persistence list
15/05/13 09:54:01 INFO BlockManager: Removing RDD 629
15/05/13 09:54:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525180000 ms: 1431525120000 ms
15/05/13 09:54:01 INFO JobGenerator: Checkpointing graph for time 1431525240000 ms
15/05/13 09:54:01 INFO DStreamGraph: Updating checkpoint data for time 1431525240000 ms
15/05/13 09:54:01 INFO DStreamGraph: Updated checkpoint data for time 1431525240000 ms
15/05/13 09:54:01 INFO CheckpointWriter: Saving checkpoint for time 1431525240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000'
15/05/13 09:54:01 INFO CheckpointWriter: Checkpoint for time 1431525240000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000', took 7657 bytes and 87 ms
15/05/13 09:54:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525240000 ms
15/05/13 09:54:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525240000 ms
15/05/13 09:54:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:55:00 INFO FileInputDStream: Finding new files took 326 ms
15/05/13 09:55:00 INFO FileInputDStream: New files at time 1431525300000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525222226.json
15/05/13 09:55:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=12426070, maxMem=278302556
15/05/13 09:55:00 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 232.9 KB, free 253.3 MB)
15/05/13 09:55:00 INFO MemoryStore: ensureFreeSpace(35707) called with curMem=12664602, maxMem=278302556
15/05/13 09:55:00 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 34.9 KB, free 253.3 MB)
15/05/13 09:55:00 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.8 MB)
15/05/13 09:55:00 INFO BlockManagerMaster: Updated info of block broadcast_138_piece0
15/05/13 09:55:00 INFO SparkContext: Created broadcast 138 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:55:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:55:00 INFO JobScheduler: Added jobs for time 1431525300000 ms
15/05/13 09:55:00 INFO JobGenerator: Checkpointing graph for time 1431525300000 ms
15/05/13 09:55:00 INFO JobScheduler: Starting job streaming job 1431525300000 ms.0 from job set of time 1431525300000 ms
15/05/13 09:55:00 INFO DStreamGraph: Updating checkpoint data for time 1431525300000 ms
15/05/13 09:55:00 INFO DStreamGraph: Updated checkpoint data for time 1431525300000 ms
15/05/13 09:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431525300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000'
15/05/13 09:55:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83126): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:55:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83126): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431525300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000'
15/05/13 09:55:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:55:00 INFO DAGScheduler: Got job 94 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:55:00 INFO DAGScheduler: Final stage: Stage 92(reduce at JsonRDD.scala:51)
15/05/13 09:55:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:55:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:55:00 INFO DAGScheduler: Submitting Stage 92 (MapPartitionsRDD[662] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:55:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12700309, maxMem=278302556
15/05/13 09:55:00 INFO MemoryStore: Block broadcast_139 stored as values in memory (estimated size 5.9 KB, free 253.3 MB)
15/05/13 09:55:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=12706349, maxMem=278302556
15/05/13 09:55:00 INFO MemoryStore: Block broadcast_139_piece0 stored as bytes in memory (estimated size 4.1 KB, free 253.3 MB)
15/05/13 09:55:00 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.8 MB)
15/05/13 09:55:00 INFO BlockManagerMaster: Updated info of block broadcast_139_piece0
15/05/13 09:55:00 INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:839
15/05/13 09:55:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 92 (MapPartitionsRDD[662] at map at JsonRDD.scala:51)
15/05/13 09:55:00 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks
15/05/13 09:55:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83128): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431525300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000'
15/05/13 09:55:00 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 92, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:55:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83130): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:55:00 WARN CheckpointWriter: Could not write checkpoint for time 1431525300000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000'
15/05/13 09:55:00 INFO BlockManagerInfo: Added broadcast_139_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.9 MB)
15/05/13 09:55:00 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:55:01 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 92) in 411 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:55:01 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool 
15/05/13 09:55:01 INFO DAGScheduler: Stage 92 (reduce at JsonRDD.scala:51) finished in 0.423 s
15/05/13 09:55:01 INFO DAGScheduler: Job 94 finished: reduce at JsonRDD.scala:51, took 0.483551 s
15/05/13 09:55:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:55:01 INFO DAGScheduler: Got job 95 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:55:01 INFO DAGScheduler: Final stage: Stage 93(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:55:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:55:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:55:01 INFO DAGScheduler: Submitting Stage 93 (MapPartitionsRDD[669] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:55:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=12710575, maxMem=278302556
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_140 stored as values in memory (estimated size 20.0 KB, free 253.3 MB)
15/05/13 09:55:01 INFO MemoryStore: ensureFreeSpace(10751) called with curMem=12731063, maxMem=278302556
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_140_piece0 stored as bytes in memory (estimated size 10.5 KB, free 253.3 MB)
15/05/13 09:55:01 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.8 MB)
15/05/13 09:55:01 INFO BlockManagerMaster: Updated info of block broadcast_140_piece0
15/05/13 09:55:01 INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:839
15/05/13 09:55:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 93 (MapPartitionsRDD[669] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:55:01 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks
15/05/13 09:55:01 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 93, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:55:01 INFO BlockManagerInfo: Added broadcast_140_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 264.8 MB)
15/05/13 09:55:01 INFO BlockManager: Removing broadcast 134
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_134
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_134 of size 21856 dropped from memory (free 265582598)
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_134_piece0
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_134_piece0 of size 11119 dropped from memory (free 265593717)
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_134_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 263.8 MB)
15/05/13 09:55:01 INFO BlockManagerMaster: Updated info of block broadcast_134_piece0
15/05/13 09:55:01 INFO DAGScheduler: Stage 93 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.124 s
15/05/13 09:55:01 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 93) in 124 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:55:01 INFO DAGScheduler: Job 95 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.177606 s
15/05/13 09:55:01 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool 
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_134_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.9 MB)
15/05/13 09:55:01 INFO ContextCleaner: Cleaned broadcast 134
15/05/13 09:55:01 INFO BlockManager: Removing broadcast 137
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_137_piece0
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_137_piece0 of size 10830 dropped from memory (free 265604547)
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_137_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.8 MB)
15/05/13 09:55:01 INFO BlockManagerMaster: Updated info of block broadcast_137_piece0
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_137
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_137 of size 20920 dropped from memory (free 265625467)
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_137_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:55:01 INFO ContextCleaner: Cleaned broadcast 137
15/05/13 09:55:01 INFO BlockManager: Removing broadcast 136
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_136
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_136 of size 6040 dropped from memory (free 265631507)
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_136_piece0
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_136_piece0 of size 4226 dropped from memory (free 265635733)
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_136_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 09:55:01 INFO BlockManagerMaster: Updated info of block broadcast_136_piece0
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_136_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:55:01 INFO JobScheduler: Finished job streaming job 1431525300000 ms.0 from job set of time 1431525300000 ms
15/05/13 09:55:01 INFO JobScheduler: Total delay: 1.593 s for time 1431525300000 ms (execution: 1.101 s)
15/05/13 09:55:01 INFO MapPartitionsRDD: Removing RDD 644 from persistence list
15/05/13 09:55:01 INFO ContextCleaner: Cleaned broadcast 136
15/05/13 09:55:01 INFO UnionRDD: Removing RDD 643 from persistence list
15/05/13 09:55:01 INFO BlockManager: Removing RDD 644
15/05/13 09:55:01 INFO BlockManager: Removing broadcast 139
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_139_piece0
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_139_piece0 of size 4226 dropped from memory (free 265639959)
15/05/13 09:55:01 INFO BlockManager: Removing RDD 643
15/05/13 09:55:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525240000 ms: 1431525180000 ms
15/05/13 09:55:01 INFO JobGenerator: Checkpointing graph for time 1431525300000 ms
15/05/13 09:55:01 INFO DStreamGraph: Updating checkpoint data for time 1431525300000 ms
15/05/13 09:55:01 INFO DStreamGraph: Updated checkpoint data for time 1431525300000 ms
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_139_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 09:55:01 INFO BlockManagerMaster: Updated info of block broadcast_139_piece0
15/05/13 09:55:01 INFO BlockManager: Removing block broadcast_139
15/05/13 09:55:01 INFO MemoryStore: Block broadcast_139 of size 6040 dropped from memory (free 265645999)
15/05/13 09:55:01 INFO BlockManagerInfo: Removed broadcast_139_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:55:01 INFO CheckpointWriter: Saving checkpoint for time 1431525300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000'
15/05/13 09:55:01 INFO ContextCleaner: Cleaned broadcast 139
15/05/13 09:55:01 INFO CheckpointWriter: Checkpoint for time 1431525300000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000', took 7656 bytes and 70 ms
15/05/13 09:55:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525300000 ms
15/05/13 09:55:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525300000 ms
15/05/13 09:55:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:56:00 INFO FileInputDStream: Finding new files took 39 ms
15/05/13 09:56:00 INFO FileInputDStream: New files at time 1431525360000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525282500.json
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=12656557, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_141 stored as values in memory (estimated size 232.9 KB, free 253.1 MB)
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=12895089, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_141_piece0 stored as bytes in memory (estimated size 34.9 KB, free 253.1 MB)
15/05/13 09:56:00 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.8 MB)
15/05/13 09:56:00 INFO BlockManagerMaster: Updated info of block broadcast_141_piece0
15/05/13 09:56:00 INFO SparkContext: Created broadcast 141 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:56:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:56:00 INFO JobScheduler: Added jobs for time 1431525360000 ms
15/05/13 09:56:00 INFO JobGenerator: Checkpointing graph for time 1431525360000 ms
15/05/13 09:56:00 INFO DStreamGraph: Updating checkpoint data for time 1431525360000 ms
15/05/13 09:56:00 INFO JobScheduler: Starting job streaming job 1431525360000 ms.0 from job set of time 1431525360000 ms
15/05/13 09:56:00 INFO DStreamGraph: Updated checkpoint data for time 1431525360000 ms
15/05/13 09:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431525360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000'
15/05/13 09:56:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:56:00 INFO DAGScheduler: Got job 96 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:56:00 INFO DAGScheduler: Final stage: Stage 94(reduce at JsonRDD.scala:51)
15/05/13 09:56:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:56:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:56:00 INFO DAGScheduler: Submitting Stage 94 (MapPartitionsRDD[676] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=12930797, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_142 stored as values in memory (estimated size 5.9 KB, free 253.1 MB)
15/05/13 09:56:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525060000.bk
15/05/13 09:56:00 INFO CheckpointWriter: Checkpoint for time 1431525360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000', took 7665 bytes and 70 ms
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=12936837, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_142_piece0 stored as bytes in memory (estimated size 4.1 KB, free 253.1 MB)
15/05/13 09:56:00 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.8 MB)
15/05/13 09:56:00 INFO BlockManagerMaster: Updated info of block broadcast_142_piece0
15/05/13 09:56:00 INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:839
15/05/13 09:56:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 94 (MapPartitionsRDD[676] at map at JsonRDD.scala:51)
15/05/13 09:56:00 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks
15/05/13 09:56:00 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 94, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:56:00 INFO BlockManagerInfo: Added broadcast_142_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.8 MB)
15/05/13 09:56:00 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.8 MB)
15/05/13 09:56:00 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 94) in 527 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:56:00 INFO DAGScheduler: Stage 94 (reduce at JsonRDD.scala:51) finished in 0.535 s
15/05/13 09:56:00 INFO DAGScheduler: Job 96 finished: reduce at JsonRDD.scala:51, took 0.564709 s
15/05/13 09:56:00 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool 
15/05/13 09:56:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:56:00 INFO DAGScheduler: Got job 97 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:56:00 INFO DAGScheduler: Final stage: Stage 95(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:56:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:56:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:56:00 INFO DAGScheduler: Submitting Stage 95 (MapPartitionsRDD[683] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=12941063, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_143 stored as values in memory (estimated size 20.4 KB, free 253.0 MB)
15/05/13 09:56:00 INFO MemoryStore: ensureFreeSpace(10851) called with curMem=12961983, maxMem=278302556
15/05/13 09:56:00 INFO MemoryStore: Block broadcast_143_piece0 stored as bytes in memory (estimated size 10.6 KB, free 253.0 MB)
15/05/13 09:56:00 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.8 MB)
15/05/13 09:56:00 INFO BlockManagerMaster: Updated info of block broadcast_143_piece0
15/05/13 09:56:00 INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:839
15/05/13 09:56:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 95 (MapPartitionsRDD[683] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:56:00 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks
15/05/13 09:56:00 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 95, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:56:01 INFO BlockManagerInfo: Added broadcast_143_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 264.2 MB)
15/05/13 09:56:01 INFO BlockManagerInfo: Added broadcast_141_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.2 MB)
15/05/13 09:56:01 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 95) in 303 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:56:01 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool 
15/05/13 09:56:01 INFO DAGScheduler: Stage 95 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.305 s
15/05/13 09:56:01 INFO DAGScheduler: Job 97 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.336831 s
15/05/13 09:56:01 INFO JobScheduler: Finished job streaming job 1431525360000 ms.0 from job set of time 1431525360000 ms
15/05/13 09:56:01 INFO JobScheduler: Total delay: 1.316 s for time 1431525360000 ms (execution: 1.182 s)
15/05/13 09:56:01 INFO MapPartitionsRDD: Removing RDD 658 from persistence list
15/05/13 09:56:01 INFO BlockManager: Removing RDD 658
15/05/13 09:56:01 INFO UnionRDD: Removing RDD 657 from persistence list
15/05/13 09:56:01 INFO BlockManager: Removing RDD 657
15/05/13 09:56:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525300000 ms: 1431525240000 ms
15/05/13 09:56:01 INFO JobGenerator: Checkpointing graph for time 1431525360000 ms
15/05/13 09:56:01 INFO DStreamGraph: Updating checkpoint data for time 1431525360000 ms
15/05/13 09:56:01 INFO DStreamGraph: Updated checkpoint data for time 1431525360000 ms
15/05/13 09:56:01 INFO CheckpointWriter: Saving checkpoint for time 1431525360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000'
15/05/13 09:56:03 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83146): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:56:03 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83146): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:56:03 INFO CheckpointWriter: Saving checkpoint for time 1431525360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000'
15/05/13 09:56:03 INFO CheckpointWriter: Checkpoint for time 1431525360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000', took 7655 bytes and 2413 ms
15/05/13 09:56:03 INFO DStreamGraph: Clearing checkpoint data for time 1431525360000 ms
15/05/13 09:56:03 INFO DStreamGraph: Cleared checkpoint data for time 1431525360000 ms
15/05/13 09:56:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:57:00 INFO FileInputDStream: Finding new files took 65 ms
15/05/13 09:57:00 INFO FileInputDStream: New files at time 1431525420000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525344385.json
15/05/13 09:57:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=12972834, maxMem=278302556
15/05/13 09:57:00 INFO MemoryStore: Block broadcast_144 stored as values in memory (estimated size 232.9 KB, free 252.8 MB)
15/05/13 09:57:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=13211366, maxMem=278302556
15/05/13 09:57:00 INFO MemoryStore: Block broadcast_144_piece0 stored as bytes in memory (estimated size 34.9 KB, free 252.8 MB)
15/05/13 09:57:00 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.7 MB)
15/05/13 09:57:00 INFO BlockManagerMaster: Updated info of block broadcast_144_piece0
15/05/13 09:57:00 INFO SparkContext: Created broadcast 144 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:57:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:57:00 INFO JobScheduler: Added jobs for time 1431525420000 ms
15/05/13 09:57:00 INFO JobGenerator: Checkpointing graph for time 1431525420000 ms
15/05/13 09:57:00 INFO JobScheduler: Starting job streaming job 1431525420000 ms.0 from job set of time 1431525420000 ms
15/05/13 09:57:00 INFO DStreamGraph: Updating checkpoint data for time 1431525420000 ms
15/05/13 09:57:00 INFO DStreamGraph: Updated checkpoint data for time 1431525420000 ms
15/05/13 09:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431525420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000'
15/05/13 09:57:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83151): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:57:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83151): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431525420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000'
15/05/13 09:57:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83153): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:57:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83153): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431525420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000'
15/05/13 09:57:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:57:00 INFO DAGScheduler: Got job 98 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:57:00 INFO DAGScheduler: Final stage: Stage 96(reduce at JsonRDD.scala:51)
15/05/13 09:57:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:57:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:57:00 INFO DAGScheduler: Submitting Stage 96 (MapPartitionsRDD[690] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:57:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=13247074, maxMem=278302556
15/05/13 09:57:00 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 5.9 KB, free 252.8 MB)
15/05/13 09:57:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=13253114, maxMem=278302556
15/05/13 09:57:00 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 4.1 KB, free 252.8 MB)
15/05/13 09:57:00 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:57:00 INFO BlockManagerMaster: Updated info of block broadcast_145_piece0
15/05/13 09:57:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83155): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:57:00 WARN CheckpointWriter: Could not write checkpoint for time 1431525420000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000'
15/05/13 09:57:00 INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:839
15/05/13 09:57:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 96 (MapPartitionsRDD[690] at map at JsonRDD.scala:51)
15/05/13 09:57:00 INFO TaskSchedulerImpl: Adding task set 96.0 with 1 tasks
15/05/13 09:57:00 INFO TaskSetManager: Starting task 0.0 in stage 96.0 (TID 96, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:57:00 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.2 MB)
15/05/13 09:57:00 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.1 MB)
15/05/13 09:57:00 INFO TaskSetManager: Finished task 0.0 in stage 96.0 (TID 96) in 454 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:57:00 INFO TaskSchedulerImpl: Removed TaskSet 96.0, whose tasks have all completed, from pool 
15/05/13 09:57:00 INFO DAGScheduler: Stage 96 (reduce at JsonRDD.scala:51) finished in 0.468 s
15/05/13 09:57:00 INFO DAGScheduler: Job 98 finished: reduce at JsonRDD.scala:51, took 0.506539 s
15/05/13 09:57:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:57:01 INFO DAGScheduler: Got job 99 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:57:01 INFO DAGScheduler: Final stage: Stage 97(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:57:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:57:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:57:01 INFO DAGScheduler: Submitting Stage 97 (MapPartitionsRDD[697] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:57:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=13257340, maxMem=278302556
15/05/13 09:57:01 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 20.4 KB, free 252.7 MB)
15/05/13 09:57:01 INFO MemoryStore: ensureFreeSpace(10908) called with curMem=13278260, maxMem=278302556
15/05/13 09:57:01 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 10.7 KB, free 252.7 MB)
15/05/13 09:57:01 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.7 MB)
15/05/13 09:57:01 INFO BlockManagerMaster: Updated info of block broadcast_146_piece0
15/05/13 09:57:01 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:839
15/05/13 09:57:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 97 (MapPartitionsRDD[697] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:57:01 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks
15/05/13 09:57:01 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 97, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:57:01 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 264.8 MB)
15/05/13 09:57:01 INFO BlockManagerInfo: Added broadcast_144_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.7 MB)
15/05/13 09:57:01 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 97) in 353 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 09:57:01 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool 
15/05/13 09:57:01 INFO DAGScheduler: Stage 97 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.356 s
15/05/13 09:57:01 INFO DAGScheduler: Job 99 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.395351 s
15/05/13 09:57:01 INFO JobScheduler: Finished job streaming job 1431525420000 ms.0 from job set of time 1431525420000 ms
15/05/13 09:57:01 INFO JobScheduler: Total delay: 1.500 s for time 1431525420000 ms (execution: 1.295 s)
15/05/13 09:57:01 INFO MapPartitionsRDD: Removing RDD 672 from persistence list
15/05/13 09:57:01 INFO BlockManager: Removing RDD 672
15/05/13 09:57:01 INFO UnionRDD: Removing RDD 671 from persistence list
15/05/13 09:57:01 INFO BlockManager: Removing RDD 671
15/05/13 09:57:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525360000 ms: 1431525300000 ms
15/05/13 09:57:01 INFO JobGenerator: Checkpointing graph for time 1431525420000 ms
15/05/13 09:57:01 INFO DStreamGraph: Updating checkpoint data for time 1431525420000 ms
15/05/13 09:57:01 INFO DStreamGraph: Updated checkpoint data for time 1431525420000 ms
15/05/13 09:57:01 INFO CheckpointWriter: Saving checkpoint for time 1431525420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000'
15/05/13 09:57:01 INFO CheckpointWriter: Checkpoint for time 1431525420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525420000', took 7655 bytes and 65 ms
15/05/13 09:57:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525420000 ms
15/05/13 09:57:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525420000 ms
15/05/13 09:57:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:58:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 09:58:00 INFO FileInputDStream: New files at time 1431525480000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525404689.json
15/05/13 09:58:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=13289168, maxMem=278302556
15/05/13 09:58:00 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 232.9 KB, free 252.5 MB)
15/05/13 09:58:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=13527700, maxMem=278302556
15/05/13 09:58:00 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 34.9 KB, free 252.5 MB)
15/05/13 09:58:00 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.7 MB)
15/05/13 09:58:00 INFO BlockManagerMaster: Updated info of block broadcast_147_piece0
15/05/13 09:58:00 INFO SparkContext: Created broadcast 147 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:58:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:58:00 INFO JobScheduler: Added jobs for time 1431525480000 ms
15/05/13 09:58:00 INFO JobGenerator: Checkpointing graph for time 1431525480000 ms
15/05/13 09:58:00 INFO JobScheduler: Starting job streaming job 1431525480000 ms.0 from job set of time 1431525480000 ms
15/05/13 09:58:00 INFO DStreamGraph: Updating checkpoint data for time 1431525480000 ms
15/05/13 09:58:00 INFO DStreamGraph: Updated checkpoint data for time 1431525480000 ms
15/05/13 09:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431525480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000'
15/05/13 09:58:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:58:00 INFO DAGScheduler: Got job 100 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:58:00 INFO DAGScheduler: Final stage: Stage 98(reduce at JsonRDD.scala:51)
15/05/13 09:58:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:58:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:58:00 INFO DAGScheduler: Submitting Stage 98 (MapPartitionsRDD[704] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:58:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=13563408, maxMem=278302556
15/05/13 09:58:00 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 5.9 KB, free 252.5 MB)
15/05/13 09:58:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=13569448, maxMem=278302556
15/05/13 09:58:00 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 4.1 KB, free 252.5 MB)
15/05/13 09:58:00 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:58:00 INFO BlockManagerMaster: Updated info of block broadcast_148_piece0
15/05/13 09:58:00 INFO SparkContext: Created broadcast 148 from broadcast at DAGScheduler.scala:839
15/05/13 09:58:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 98 (MapPartitionsRDD[704] at map at JsonRDD.scala:51)
15/05/13 09:58:00 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks
15/05/13 09:58:00 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 98, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:58:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83160): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:58:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83160): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:58:01 INFO CheckpointWriter: Saving checkpoint for time 1431525480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000'
15/05/13 09:58:01 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:58:01 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.1 MB)
15/05/13 09:58:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83161): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:58:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83161): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 09:58:01 INFO CheckpointWriter: Saving checkpoint for time 1431525480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000'
15/05/13 09:58:01 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83163): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 09:58:01 WARN CheckpointWriter: Could not write checkpoint for time 1431525480000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000'
15/05/13 09:58:01 INFO DAGScheduler: Stage 98 (reduce at JsonRDD.scala:51) finished in 1.099 s
15/05/13 09:58:01 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 98) in 1088 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:58:01 INFO DAGScheduler: Job 100 finished: reduce at JsonRDD.scala:51, took 1.122700 s
15/05/13 09:58:01 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool 
15/05/13 09:58:01 INFO BlockManager: Removing broadcast 142
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_142_piece0
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_142_piece0 of size 4226 dropped from memory (free 264733108)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_142_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_142_piece0
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_142
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_142 of size 6040 dropped from memory (free 264739148)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_142_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 09:58:01 INFO ContextCleaner: Cleaned broadcast 142
15/05/13 09:58:01 INFO BlockManager: Removing broadcast 143
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_143_piece0
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_143_piece0 of size 10851 dropped from memory (free 264749999)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_143_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_143_piece0
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_143
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_143 of size 20920 dropped from memory (free 264770919)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_143_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 264.1 MB)
15/05/13 09:58:01 INFO ContextCleaner: Cleaned broadcast 143
15/05/13 09:58:01 INFO BlockManager: Removing broadcast 146
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_146_piece0
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_146_piece0 of size 10908 dropped from memory (free 264781827)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_146_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_146_piece0
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_146
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_146 of size 20920 dropped from memory (free 264802747)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_146_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 264.7 MB)
15/05/13 09:58:01 INFO ContextCleaner: Cleaned broadcast 146
15/05/13 09:58:01 INFO BlockManager: Removing broadcast 145
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_145
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_145 of size 6040 dropped from memory (free 264808787)
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_145_piece0
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_145_piece0 of size 4226 dropped from memory (free 264813013)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_145_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_145_piece0
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_145_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:58:01 INFO ContextCleaner: Cleaned broadcast 145
15/05/13 09:58:01 INFO BlockManager: Removing broadcast 148
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_148_piece0
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_148_piece0 of size 4226 dropped from memory (free 264817239)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_148_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_148_piece0
15/05/13 09:58:01 INFO BlockManager: Removing block broadcast_148
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_148 of size 6040 dropped from memory (free 264823279)
15/05/13 09:58:01 INFO BlockManagerInfo: Removed broadcast_148_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:58:01 INFO ContextCleaner: Cleaned broadcast 148
15/05/13 09:58:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:58:01 INFO DAGScheduler: Got job 101 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:58:01 INFO DAGScheduler: Final stage: Stage 99(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:58:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:58:01 INFO DAGScheduler: Missing parents: List()
15/05/13 09:58:01 INFO DAGScheduler: Submitting Stage 99 (MapPartitionsRDD[711] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:58:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=13479277, maxMem=278302556
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 20.0 KB, free 252.5 MB)
15/05/13 09:58:01 INFO MemoryStore: ensureFreeSpace(10756) called with curMem=13499765, maxMem=278302556
15/05/13 09:58:01 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 10.5 KB, free 252.5 MB)
15/05/13 09:58:01 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.7 MB)
15/05/13 09:58:01 INFO BlockManagerMaster: Updated info of block broadcast_149_piece0
15/05/13 09:58:01 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:839
15/05/13 09:58:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 99 (MapPartitionsRDD[711] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:58:01 INFO TaskSchedulerImpl: Adding task set 99.0 with 1 tasks
15/05/13 09:58:01 INFO TaskSetManager: Starting task 0.0 in stage 99.0 (TID 99, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:58:01 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 264.1 MB)
15/05/13 09:58:01 INFO TaskSetManager: Finished task 0.0 in stage 99.0 (TID 99) in 137 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:58:01 INFO TaskSchedulerImpl: Removed TaskSet 99.0, whose tasks have all completed, from pool 
15/05/13 09:58:01 INFO DAGScheduler: Stage 99 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.140 s
15/05/13 09:58:01 INFO DAGScheduler: Job 101 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.177729 s
15/05/13 09:58:01 INFO JobScheduler: Finished job streaming job 1431525480000 ms.0 from job set of time 1431525480000 ms
15/05/13 09:58:01 INFO JobScheduler: Total delay: 1.885 s for time 1431525480000 ms (execution: 1.699 s)
15/05/13 09:58:01 INFO MapPartitionsRDD: Removing RDD 686 from persistence list
15/05/13 09:58:01 INFO BlockManager: Removing RDD 686
15/05/13 09:58:01 INFO UnionRDD: Removing RDD 685 from persistence list
15/05/13 09:58:01 INFO BlockManager: Removing RDD 685
15/05/13 09:58:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525420000 ms: 1431525360000 ms
15/05/13 09:58:01 INFO JobGenerator: Checkpointing graph for time 1431525480000 ms
15/05/13 09:58:01 INFO DStreamGraph: Updating checkpoint data for time 1431525480000 ms
15/05/13 09:58:01 INFO DStreamGraph: Updated checkpoint data for time 1431525480000 ms
15/05/13 09:58:01 INFO CheckpointWriter: Saving checkpoint for time 1431525480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000'
15/05/13 09:58:01 INFO CheckpointWriter: Checkpoint for time 1431525480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000', took 7657 bytes and 55 ms
15/05/13 09:58:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525480000 ms
15/05/13 09:58:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525480000 ms
15/05/13 09:58:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 09:59:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 09:59:00 INFO FileInputDStream: New files at time 1431525540000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525466020.json
15/05/13 09:59:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=13510521, maxMem=278302556
15/05/13 09:59:00 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 232.9 KB, free 252.3 MB)
15/05/13 09:59:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=13749053, maxMem=278302556
15/05/13 09:59:00 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 34.9 KB, free 252.3 MB)
15/05/13 09:59:00 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.7 MB)
15/05/13 09:59:00 INFO BlockManagerMaster: Updated info of block broadcast_150_piece0
15/05/13 09:59:00 INFO SparkContext: Created broadcast 150 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 09:59:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 09:59:00 INFO JobScheduler: Added jobs for time 1431525540000 ms
15/05/13 09:59:00 INFO JobGenerator: Checkpointing graph for time 1431525540000 ms
15/05/13 09:59:00 INFO JobScheduler: Starting job streaming job 1431525540000 ms.0 from job set of time 1431525540000 ms
15/05/13 09:59:00 INFO DStreamGraph: Updating checkpoint data for time 1431525540000 ms
15/05/13 09:59:00 INFO DStreamGraph: Updated checkpoint data for time 1431525540000 ms
15/05/13 09:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431525540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525540000'
15/05/13 09:59:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 09:59:00 INFO DAGScheduler: Got job 102 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 09:59:00 INFO DAGScheduler: Final stage: Stage 100(reduce at JsonRDD.scala:51)
15/05/13 09:59:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:59:00 INFO DAGScheduler: Missing parents: List()
15/05/13 09:59:00 INFO DAGScheduler: Submitting Stage 100 (MapPartitionsRDD[718] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 09:59:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=13784761, maxMem=278302556
15/05/13 09:59:00 INFO MemoryStore: Block broadcast_151 stored as values in memory (estimated size 5.9 KB, free 252.3 MB)
15/05/13 09:59:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=13790801, maxMem=278302556
15/05/13 09:59:00 INFO MemoryStore: Block broadcast_151_piece0 stored as bytes in memory (estimated size 4.1 KB, free 252.3 MB)
15/05/13 09:59:00 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.7 MB)
15/05/13 09:59:00 INFO BlockManagerMaster: Updated info of block broadcast_151_piece0
15/05/13 09:59:00 INFO SparkContext: Created broadcast 151 from broadcast at DAGScheduler.scala:839
15/05/13 09:59:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 100 (MapPartitionsRDD[718] at map at JsonRDD.scala:51)
15/05/13 09:59:00 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks
15/05/13 09:59:00 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 100, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:59:00 INFO BlockManagerInfo: Added broadcast_151_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.1 MB)
15/05/13 09:59:00 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.1 MB)
15/05/13 09:59:13 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525240000.bk
15/05/13 09:59:13 INFO CheckpointWriter: Checkpoint for time 1431525540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525540000', took 7663 bytes and 13368 ms
15/05/13 09:59:13 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 100) in 13616 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:59:13 INFO DAGScheduler: Stage 100 (reduce at JsonRDD.scala:51) finished in 13.656 s
15/05/13 09:59:13 INFO DAGScheduler: Job 102 finished: reduce at JsonRDD.scala:51, took 13.686970 s
15/05/13 09:59:13 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool 
15/05/13 09:59:14 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 09:59:14 INFO DAGScheduler: Got job 103 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 09:59:14 INFO DAGScheduler: Final stage: Stage 101(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:59:14 INFO DAGScheduler: Parents of final stage: List()
15/05/13 09:59:14 INFO DAGScheduler: Missing parents: List()
15/05/13 09:59:14 INFO DAGScheduler: Submitting Stage 101 (MapPartitionsRDD[725] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 09:59:14 INFO MemoryStore: ensureFreeSpace(21344) called with curMem=13795027, maxMem=278302556
15/05/13 09:59:14 INFO MemoryStore: Block broadcast_152 stored as values in memory (estimated size 20.8 KB, free 252.2 MB)
15/05/13 09:59:14 INFO MemoryStore: ensureFreeSpace(11213) called with curMem=13816371, maxMem=278302556
15/05/13 09:59:14 INFO MemoryStore: Block broadcast_152_piece0 stored as bytes in memory (estimated size 11.0 KB, free 252.2 MB)
15/05/13 09:59:14 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 263.7 MB)
15/05/13 09:59:14 INFO BlockManagerMaster: Updated info of block broadcast_152_piece0
15/05/13 09:59:14 INFO SparkContext: Created broadcast 152 from broadcast at DAGScheduler.scala:839
15/05/13 09:59:14 INFO DAGScheduler: Submitting 1 missing tasks from Stage 101 (MapPartitionsRDD[725] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 09:59:14 INFO TaskSchedulerImpl: Adding task set 101.0 with 1 tasks
15/05/13 09:59:14 INFO TaskSetManager: Starting task 0.0 in stage 101.0 (TID 101, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 09:59:14 INFO BlockManagerInfo: Added broadcast_152_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 264.1 MB)
15/05/13 09:59:14 INFO TaskSetManager: Finished task 0.0 in stage 101.0 (TID 101) in 201 ms on pti-base.insafanalytics.com (1/1)
15/05/13 09:59:14 INFO TaskSchedulerImpl: Removed TaskSet 101.0, whose tasks have all completed, from pool 
15/05/13 09:59:14 INFO DAGScheduler: Stage 101 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.203 s
15/05/13 09:59:14 INFO DAGScheduler: Job 103 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.248856 s
15/05/13 09:59:14 INFO JobScheduler: Finished job streaming job 1431525540000 ms.0 from job set of time 1431525540000 ms
15/05/13 09:59:14 INFO JobScheduler: Total delay: 14.628 s for time 1431525540000 ms (execution: 14.439 s)
15/05/13 09:59:14 INFO MapPartitionsRDD: Removing RDD 700 from persistence list
15/05/13 09:59:14 INFO BlockManager: Removing RDD 700
15/05/13 09:59:14 INFO UnionRDD: Removing RDD 699 from persistence list
15/05/13 09:59:14 INFO BlockManager: Removing RDD 699
15/05/13 09:59:14 INFO FileInputDStream: Cleared 1 old files that were older than 1431525480000 ms: 1431525420000 ms
15/05/13 09:59:14 INFO JobGenerator: Checkpointing graph for time 1431525540000 ms
15/05/13 09:59:14 INFO DStreamGraph: Updating checkpoint data for time 1431525540000 ms
15/05/13 09:59:14 INFO DStreamGraph: Updated checkpoint data for time 1431525540000 ms
15/05/13 09:59:14 INFO CheckpointWriter: Saving checkpoint for time 1431525540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525540000'
15/05/13 09:59:14 INFO CheckpointWriter: Checkpoint for time 1431525540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525540000', took 7654 bytes and 82 ms
15/05/13 09:59:14 INFO DStreamGraph: Clearing checkpoint data for time 1431525540000 ms
15/05/13 09:59:14 INFO DStreamGraph: Cleared checkpoint data for time 1431525540000 ms
15/05/13 09:59:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:00:00 INFO FileInputDStream: Finding new files took 66 ms
15/05/13 10:00:00 INFO FileInputDStream: New files at time 1431525600000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525534570.json
15/05/13 10:00:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=13827584, maxMem=278302556
15/05/13 10:00:00 INFO MemoryStore: Block broadcast_153 stored as values in memory (estimated size 232.9 KB, free 252.0 MB)
15/05/13 10:00:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=14066116, maxMem=278302556
15/05/13 10:00:00 INFO MemoryStore: Block broadcast_153_piece0 stored as bytes in memory (estimated size 34.9 KB, free 252.0 MB)
15/05/13 10:00:00 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:00:00 INFO BlockManagerMaster: Updated info of block broadcast_153_piece0
15/05/13 10:00:00 INFO SparkContext: Created broadcast 153 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:00:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:00:00 INFO JobScheduler: Added jobs for time 1431525600000 ms
15/05/13 10:00:00 INFO JobGenerator: Checkpointing graph for time 1431525600000 ms
15/05/13 10:00:00 INFO DStreamGraph: Updating checkpoint data for time 1431525600000 ms
15/05/13 10:00:00 INFO DStreamGraph: Updated checkpoint data for time 1431525600000 ms
15/05/13 10:00:00 INFO JobScheduler: Starting job streaming job 1431525600000 ms.0 from job set of time 1431525600000 ms
15/05/13 10:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431525600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000'
15/05/13 10:00:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:00:00 INFO DAGScheduler: Got job 104 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:00:00 INFO DAGScheduler: Final stage: Stage 102(reduce at JsonRDD.scala:51)
15/05/13 10:00:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:00:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83176): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431525600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000'
15/05/13 10:00:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:00:00 INFO DAGScheduler: Submitting Stage 102 (MapPartitionsRDD[732] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:00:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=14101824, maxMem=278302556
15/05/13 10:00:00 INFO MemoryStore: Block broadcast_154 stored as values in memory (estimated size 5.9 KB, free 252.0 MB)
15/05/13 10:00:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=14107864, maxMem=278302556
15/05/13 10:00:00 INFO MemoryStore: Block broadcast_154_piece0 stored as bytes in memory (estimated size 4.1 KB, free 252.0 MB)
15/05/13 10:00:00 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:00:00 INFO BlockManagerMaster: Updated info of block broadcast_154_piece0
15/05/13 10:00:00 INFO SparkContext: Created broadcast 154 from broadcast at DAGScheduler.scala:839
15/05/13 10:00:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 102 (MapPartitionsRDD[732] at map at JsonRDD.scala:51)
15/05/13 10:00:00 INFO TaskSchedulerImpl: Adding task set 102.0 with 1 tasks
15/05/13 10:00:00 INFO TaskSetManager: Starting task 0.0 in stage 102.0 (TID 102, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:00:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83178): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431525600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000'
15/05/13 10:00:00 INFO BlockManagerInfo: Added broadcast_154_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.9 MB)
15/05/13 10:00:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525300000.bk
15/05/13 10:00:00 INFO CheckpointWriter: Checkpoint for time 1431525600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000', took 7665 bytes and 219 ms
15/05/13 10:00:00 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.8 MB)
15/05/13 10:00:00 INFO TaskSetManager: Finished task 0.0 in stage 102.0 (TID 102) in 420 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:00:00 INFO TaskSchedulerImpl: Removed TaskSet 102.0, whose tasks have all completed, from pool 
15/05/13 10:00:00 INFO DAGScheduler: Stage 102 (reduce at JsonRDD.scala:51) finished in 0.435 s
15/05/13 10:00:00 INFO DAGScheduler: Job 104 finished: reduce at JsonRDD.scala:51, took 0.479430 s
15/05/13 10:00:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:00:01 INFO DAGScheduler: Got job 105 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:00:01 INFO DAGScheduler: Final stage: Stage 103(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:00:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:00:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:00:01 INFO DAGScheduler: Submitting Stage 103 (MapPartitionsRDD[739] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:00:01 INFO MemoryStore: ensureFreeSpace(21800) called with curMem=14112090, maxMem=278302556
15/05/13 10:00:01 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 21.3 KB, free 251.9 MB)
15/05/13 10:00:01 INFO MemoryStore: ensureFreeSpace(11312) called with curMem=14133890, maxMem=278302556
15/05/13 10:00:01 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 11.0 KB, free 251.9 MB)
15/05/13 10:00:01 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:00:01 INFO BlockManagerMaster: Updated info of block broadcast_155_piece0
15/05/13 10:00:01 INFO SparkContext: Created broadcast 155 from broadcast at DAGScheduler.scala:839
15/05/13 10:00:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 103 (MapPartitionsRDD[739] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:00:01 INFO TaskSchedulerImpl: Adding task set 103.0 with 1 tasks
15/05/13 10:00:01 INFO TaskSetManager: Starting task 0.0 in stage 103.0 (TID 103, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:00:01 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 264.0 MB)
15/05/13 10:00:01 INFO BlockManagerInfo: Added broadcast_153_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.0 MB)
15/05/13 10:00:01 INFO TaskSetManager: Finished task 0.0 in stage 103.0 (TID 103) in 378 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:00:01 INFO TaskSchedulerImpl: Removed TaskSet 103.0, whose tasks have all completed, from pool 
15/05/13 10:00:01 INFO DAGScheduler: Stage 103 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.381 s
15/05/13 10:00:01 INFO DAGScheduler: Job 105 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.419144 s
15/05/13 10:00:01 INFO JobScheduler: Finished job streaming job 1431525600000 ms.0 from job set of time 1431525600000 ms
15/05/13 10:00:01 INFO JobScheduler: Total delay: 1.504 s for time 1431525600000 ms (execution: 1.276 s)
15/05/13 10:00:01 INFO MapPartitionsRDD: Removing RDD 714 from persistence list
15/05/13 10:00:01 INFO BlockManager: Removing RDD 714
15/05/13 10:00:01 INFO UnionRDD: Removing RDD 713 from persistence list
15/05/13 10:00:01 INFO BlockManager: Removing RDD 713
15/05/13 10:00:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525540000 ms: 1431525480000 ms
15/05/13 10:00:01 INFO JobGenerator: Checkpointing graph for time 1431525600000 ms
15/05/13 10:00:01 INFO DStreamGraph: Updating checkpoint data for time 1431525600000 ms
15/05/13 10:00:01 INFO DStreamGraph: Updated checkpoint data for time 1431525600000 ms
15/05/13 10:00:01 INFO CheckpointWriter: Saving checkpoint for time 1431525600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000'
15/05/13 10:00:01 INFO CheckpointWriter: Checkpoint for time 1431525600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000', took 7656 bytes and 64 ms
15/05/13 10:00:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525600000 ms
15/05/13 10:00:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525600000 ms
15/05/13 10:00:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:01:00 INFO FileInputDStream: Finding new files took 41 ms
15/05/13 10:01:00 INFO FileInputDStream: New files at time 1431525660000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525598471.json
15/05/13 10:01:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=14145202, maxMem=278302556
15/05/13 10:01:00 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 232.9 KB, free 251.7 MB)
15/05/13 10:01:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=14383734, maxMem=278302556
15/05/13 10:01:00 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 34.9 KB, free 251.7 MB)
15/05/13 10:01:00 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:01:00 INFO BlockManagerMaster: Updated info of block broadcast_156_piece0
15/05/13 10:01:00 INFO SparkContext: Created broadcast 156 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:01:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:01:00 INFO JobScheduler: Added jobs for time 1431525660000 ms
15/05/13 10:01:00 INFO JobGenerator: Checkpointing graph for time 1431525660000 ms
15/05/13 10:01:00 INFO JobScheduler: Starting job streaming job 1431525660000 ms.0 from job set of time 1431525660000 ms
15/05/13 10:01:00 INFO DStreamGraph: Updating checkpoint data for time 1431525660000 ms
15/05/13 10:01:00 INFO DStreamGraph: Updated checkpoint data for time 1431525660000 ms
15/05/13 10:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431525660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000'
15/05/13 10:01:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83191): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431525660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000'
15/05/13 10:01:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:01:00 INFO DAGScheduler: Got job 106 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:01:00 INFO DAGScheduler: Final stage: Stage 104(reduce at JsonRDD.scala:51)
15/05/13 10:01:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:01:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:01:00 INFO DAGScheduler: Submitting Stage 104 (MapPartitionsRDD[746] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:01:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=14419442, maxMem=278302556
15/05/13 10:01:00 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 5.9 KB, free 251.7 MB)
15/05/13 10:01:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=14425482, maxMem=278302556
15/05/13 10:01:00 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 4.1 KB, free 251.6 MB)
15/05/13 10:01:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83193): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:01:00 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:01:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83193): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:01:00 INFO BlockManagerMaster: Updated info of block broadcast_157_piece0
15/05/13 10:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431525660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000'
15/05/13 10:01:00 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:839
15/05/13 10:01:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 104 (MapPartitionsRDD[746] at map at JsonRDD.scala:51)
15/05/13 10:01:00 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks
15/05/13 10:01:00 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 104, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:01:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525360000.bk
15/05/13 10:01:00 INFO CheckpointWriter: Checkpoint for time 1431525660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000', took 7664 bytes and 224 ms
15/05/13 10:01:00 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.8 MB)
15/05/13 10:01:00 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.8 MB)
15/05/13 10:01:00 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 104) in 467 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:01:00 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool 
15/05/13 10:01:00 INFO DAGScheduler: Stage 104 (reduce at JsonRDD.scala:51) finished in 0.489 s
15/05/13 10:01:00 INFO DAGScheduler: Job 106 finished: reduce at JsonRDD.scala:51, took 0.526577 s
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 157
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_157_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_157_piece0 of size 4228 dropped from memory (free 263877074)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_157_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_157_piece0
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_157
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_157 of size 6040 dropped from memory (free 263883114)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_157_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 157
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 152
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_152
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_152 of size 21344 dropped from memory (free 263904458)
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_152_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_152_piece0 of size 11213 dropped from memory (free 263915671)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_152_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_152_piece0
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_152_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 152
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 151
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_151_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_151_piece0 of size 4226 dropped from memory (free 263919897)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_151_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_151_piece0
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_151
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_151 of size 6040 dropped from memory (free 263925937)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_151_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 151
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 149
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_149_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_149_piece0 of size 10756 dropped from memory (free 263936693)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_149_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_149_piece0
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_149
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_149 of size 20488 dropped from memory (free 263957181)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_149_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 149
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 155
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_155
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_155 of size 21800 dropped from memory (free 263978981)
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_155_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_155_piece0 of size 11312 dropped from memory (free 263990293)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_155_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_155_piece0
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_155_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:01:01 INFO DAGScheduler: Got job 107 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:01:01 INFO DAGScheduler: Final stage: Stage 105(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:01:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 155
15/05/13 10:01:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:01:01 INFO BlockManager: Removing broadcast 154
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_154_piece0
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_154_piece0 of size 4226 dropped from memory (free 263994519)
15/05/13 10:01:01 INFO DAGScheduler: Submitting Stage 105 (MapPartitionsRDD[753] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_154_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_154_piece0
15/05/13 10:01:01 INFO BlockManager: Removing block broadcast_154
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_154 of size 6040 dropped from memory (free 264000559)
15/05/13 10:01:01 INFO MemoryStore: ensureFreeSpace(21344) called with curMem=14301997, maxMem=278302556
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 20.8 KB, free 251.8 MB)
15/05/13 10:01:01 INFO BlockManagerInfo: Removed broadcast_154_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.8 MB)
15/05/13 10:01:01 INFO MemoryStore: ensureFreeSpace(11172) called with curMem=14323341, maxMem=278302556
15/05/13 10:01:01 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 10.9 KB, free 251.7 MB)
15/05/13 10:01:01 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 263.6 MB)
15/05/13 10:01:01 INFO BlockManagerMaster: Updated info of block broadcast_158_piece0
15/05/13 10:01:01 INFO SparkContext: Created broadcast 158 from broadcast at DAGScheduler.scala:839
15/05/13 10:01:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 105 (MapPartitionsRDD[753] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:01:01 INFO TaskSchedulerImpl: Adding task set 105.0 with 1 tasks
15/05/13 10:01:01 INFO TaskSetManager: Starting task 0.0 in stage 105.0 (TID 105, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:01:01 INFO ContextCleaner: Cleaned broadcast 154
15/05/13 10:01:01 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.0 MB)
15/05/13 10:01:01 INFO TaskSetManager: Finished task 0.0 in stage 105.0 (TID 105) in 245 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:01:01 INFO TaskSchedulerImpl: Removed TaskSet 105.0, whose tasks have all completed, from pool 
15/05/13 10:01:01 INFO DAGScheduler: Stage 105 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.248 s
15/05/13 10:01:01 INFO DAGScheduler: Job 107 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.271423 s
15/05/13 10:01:01 INFO JobScheduler: Finished job streaming job 1431525660000 ms.0 from job set of time 1431525660000 ms
15/05/13 10:01:01 INFO JobScheduler: Total delay: 1.532 s for time 1431525660000 ms (execution: 1.309 s)
15/05/13 10:01:01 INFO MapPartitionsRDD: Removing RDD 728 from persistence list
15/05/13 10:01:01 INFO BlockManager: Removing RDD 728
15/05/13 10:01:01 INFO UnionRDD: Removing RDD 727 from persistence list
15/05/13 10:01:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525600000 ms: 1431525540000 ms
15/05/13 10:01:01 INFO BlockManager: Removing RDD 727
15/05/13 10:01:01 INFO JobGenerator: Checkpointing graph for time 1431525660000 ms
15/05/13 10:01:01 INFO DStreamGraph: Updating checkpoint data for time 1431525660000 ms
15/05/13 10:01:01 INFO DStreamGraph: Updated checkpoint data for time 1431525660000 ms
15/05/13 10:01:01 INFO CheckpointWriter: Saving checkpoint for time 1431525660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000'
15/05/13 10:01:01 INFO CheckpointWriter: Checkpoint for time 1431525660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525660000', took 7656 bytes and 50 ms
15/05/13 10:01:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525660000 ms
15/05/13 10:01:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525660000 ms
15/05/13 10:01:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:02:00 INFO FileInputDStream: Finding new files took 42 ms
15/05/13 10:02:00 INFO FileInputDStream: New files at time 1431525720000 ms:

15/05/13 10:02:00 INFO JobScheduler: Added jobs for time 1431525720000 ms
15/05/13 10:02:00 INFO JobGenerator: Checkpointing graph for time 1431525720000 ms
15/05/13 10:02:00 INFO DStreamGraph: Updating checkpoint data for time 1431525720000 ms
15/05/13 10:02:00 INFO JobScheduler: Starting job streaming job 1431525720000 ms.0 from job set of time 1431525720000 ms
15/05/13 10:02:00 INFO DStreamGraph: Updated checkpoint data for time 1431525720000 ms
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83199): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83199): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:02:00 INFO DAGScheduler: Job 108 finished: reduce at JsonRDD.scala:51, took 0.000164 s
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83201): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 10:02:00 INFO JobScheduler: Finished job streaming job 1431525720000 ms.0 from job set of time 1431525720000 ms
15/05/13 10:02:00 INFO JobScheduler: Total delay: 0.213 s for time 1431525720000 ms (execution: 0.149 s)
15/05/13 10:02:00 INFO MapPartitionsRDD: Removing RDD 742 from persistence list
15/05/13 10:02:00 INFO BlockManager: Removing RDD 742
15/05/13 10:02:00 INFO UnionRDD: Removing RDD 741 from persistence list
15/05/13 10:02:00 INFO BlockManager: Removing RDD 741
15/05/13 10:02:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431525660000 ms: 1431525600000 ms
15/05/13 10:02:00 INFO JobGenerator: Checkpointing graph for time 1431525720000 ms
15/05/13 10:02:00 INFO DStreamGraph: Updating checkpoint data for time 1431525720000 ms
15/05/13 10:02:00 INFO DStreamGraph: Updated checkpoint data for time 1431525720000 ms
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83203): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:02:00 WARN CheckpointWriter: Could not write checkpoint for time 1431525720000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83205): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83207): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83207): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431525720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:02:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83209): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:02:00 WARN CheckpointWriter: Could not write checkpoint for time 1431525720000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525720000'
15/05/13 10:03:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 10:03:00 INFO FileInputDStream: New files at time 1431525780000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525667274.json
15/05/13 10:03:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=14334513, maxMem=278302556
15/05/13 10:03:00 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 232.9 KB, free 251.5 MB)
15/05/13 10:03:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=14573045, maxMem=278302556
15/05/13 10:03:00 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 34.9 KB, free 251.5 MB)
15/05/13 10:03:00 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:03:00 INFO BlockManagerMaster: Updated info of block broadcast_159_piece0
15/05/13 10:03:00 INFO SparkContext: Created broadcast 159 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:03:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:03:00 INFO JobScheduler: Added jobs for time 1431525780000 ms
15/05/13 10:03:00 INFO JobGenerator: Checkpointing graph for time 1431525780000 ms
15/05/13 10:03:00 INFO DStreamGraph: Updating checkpoint data for time 1431525780000 ms
15/05/13 10:03:00 INFO JobScheduler: Starting job streaming job 1431525780000 ms.0 from job set of time 1431525780000 ms
15/05/13 10:03:00 INFO DStreamGraph: Updated checkpoint data for time 1431525780000 ms
15/05/13 10:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431525780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000'
15/05/13 10:03:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83213): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431525780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000'
15/05/13 10:03:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:03:00 INFO DAGScheduler: Got job 109 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:03:00 INFO DAGScheduler: Final stage: Stage 106(reduce at JsonRDD.scala:51)
15/05/13 10:03:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:03:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:03:00 INFO DAGScheduler: Submitting Stage 106 (MapPartitionsRDD[766] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:03:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=14608753, maxMem=278302556
15/05/13 10:03:00 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 5.9 KB, free 251.5 MB)
15/05/13 10:03:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000
15/05/13 10:03:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=14614793, maxMem=278302556
15/05/13 10:03:00 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 4.1 KB, free 251.5 MB)
15/05/13 10:03:00 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:03:00 INFO BlockManagerMaster: Updated info of block broadcast_160_piece0
15/05/13 10:03:00 INFO CheckpointWriter: Checkpoint for time 1431525780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000', took 7642 bytes and 139 ms
15/05/13 10:03:00 INFO SparkContext: Created broadcast 160 from broadcast at DAGScheduler.scala:839
15/05/13 10:03:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 106 (MapPartitionsRDD[766] at map at JsonRDD.scala:51)
15/05/13 10:03:00 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks
15/05/13 10:03:00 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 106, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:03:00 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.0 MB)
15/05/13 10:03:00 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 264.0 MB)
15/05/13 10:03:00 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 106) in 451 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:03:00 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool 
15/05/13 10:03:00 INFO DAGScheduler: Stage 106 (reduce at JsonRDD.scala:51) finished in 0.468 s
15/05/13 10:03:00 INFO DAGScheduler: Job 109 finished: reduce at JsonRDD.scala:51, took 0.539951 s
15/05/13 10:03:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:03:01 INFO DAGScheduler: Got job 110 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:03:01 INFO DAGScheduler: Final stage: Stage 107(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:03:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:03:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:03:01 INFO DAGScheduler: Submitting Stage 107 (MapPartitionsRDD[773] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:03:01 INFO MemoryStore: ensureFreeSpace(21168) called with curMem=14619019, maxMem=278302556
15/05/13 10:03:01 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 20.7 KB, free 251.4 MB)
15/05/13 10:03:01 INFO MemoryStore: ensureFreeSpace(11066) called with curMem=14640187, maxMem=278302556
15/05/13 10:03:01 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 10.8 KB, free 251.4 MB)
15/05/13 10:03:01 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 263.6 MB)
15/05/13 10:03:01 INFO BlockManagerMaster: Updated info of block broadcast_161_piece0
15/05/13 10:03:01 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:839
15/05/13 10:03:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 107 (MapPartitionsRDD[773] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:03:01 INFO TaskSchedulerImpl: Adding task set 107.0 with 1 tasks
15/05/13 10:03:01 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 107, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:03:01 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 264.0 MB)
15/05/13 10:03:01 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 107) in 155 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:03:01 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool 
15/05/13 10:03:01 INFO DAGScheduler: Stage 107 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.158 s
15/05/13 10:03:01 INFO DAGScheduler: Job 110 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.196800 s
15/05/13 10:03:01 INFO JobScheduler: Finished job streaming job 1431525780000 ms.0 from job set of time 1431525780000 ms
15/05/13 10:03:01 INFO JobScheduler: Total delay: 1.330 s for time 1431525780000 ms (execution: 1.156 s)
15/05/13 10:03:01 INFO MapPartitionsRDD: Removing RDD 755 from persistence list
15/05/13 10:03:01 INFO BlockManager: Removing RDD 755
15/05/13 10:03:01 INFO UnionRDD: Removing RDD 754 from persistence list
15/05/13 10:03:01 INFO BlockManager: Removing RDD 754
15/05/13 10:03:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525720000 ms: 1431525660000 ms
15/05/13 10:03:01 INFO JobGenerator: Checkpointing graph for time 1431525780000 ms
15/05/13 10:03:01 INFO DStreamGraph: Updating checkpoint data for time 1431525780000 ms
15/05/13 10:03:01 INFO DStreamGraph: Updated checkpoint data for time 1431525780000 ms
15/05/13 10:03:01 INFO CheckpointWriter: Saving checkpoint for time 1431525780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000'
15/05/13 10:03:01 INFO CheckpointWriter: Checkpoint for time 1431525780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000', took 7647 bytes and 61 ms
15/05/13 10:03:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525780000 ms
15/05/13 10:03:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525780000 ms
15/05/13 10:03:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:04:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 10:04:00 INFO FileInputDStream: New files at time 1431525840000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525727498.json
15/05/13 10:04:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=14651253, maxMem=278302556
15/05/13 10:04:00 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 232.9 KB, free 251.2 MB)
15/05/13 10:04:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=14889785, maxMem=278302556
15/05/13 10:04:00 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 34.9 KB, free 251.2 MB)
15/05/13 10:04:00 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:04:00 INFO BlockManagerMaster: Updated info of block broadcast_162_piece0
15/05/13 10:04:00 INFO SparkContext: Created broadcast 162 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:04:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:04:00 INFO JobScheduler: Added jobs for time 1431525840000 ms
15/05/13 10:04:00 INFO JobScheduler: Starting job streaming job 1431525840000 ms.0 from job set of time 1431525840000 ms
15/05/13 10:04:00 INFO JobGenerator: Checkpointing graph for time 1431525840000 ms
15/05/13 10:04:00 INFO DStreamGraph: Updating checkpoint data for time 1431525840000 ms
15/05/13 10:04:00 INFO DStreamGraph: Updated checkpoint data for time 1431525840000 ms
15/05/13 10:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431525840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000'
15/05/13 10:04:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83222): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:04:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83222): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431525840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000'
15/05/13 10:04:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83224): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431525840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000'
15/05/13 10:04:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:04:00 INFO DAGScheduler: Got job 111 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:04:00 INFO DAGScheduler: Final stage: Stage 108(reduce at JsonRDD.scala:51)
15/05/13 10:04:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:04:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:04:00 INFO DAGScheduler: Submitting Stage 108 (MapPartitionsRDD[780] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:04:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=14925493, maxMem=278302556
15/05/13 10:04:00 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 5.9 KB, free 251.2 MB)
15/05/13 10:04:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=14931533, maxMem=278302556
15/05/13 10:04:00 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 4.1 KB, free 251.2 MB)
15/05/13 10:04:00 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:04:00 INFO BlockManagerMaster: Updated info of block broadcast_163_piece0
15/05/13 10:04:00 INFO SparkContext: Created broadcast 163 from broadcast at DAGScheduler.scala:839
15/05/13 10:04:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 108 (MapPartitionsRDD[780] at map at JsonRDD.scala:51)
15/05/13 10:04:00 INFO TaskSchedulerImpl: Adding task set 108.0 with 1 tasks
15/05/13 10:04:00 INFO TaskSetManager: Starting task 0.0 in stage 108.0 (TID 108, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:04:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525480000
15/05/13 10:04:00 INFO CheckpointWriter: Checkpoint for time 1431525840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000', took 7641 bytes and 169 ms
15/05/13 10:04:00 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.0 MB)
15/05/13 10:04:00 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.9 MB)
15/05/13 10:04:00 INFO TaskSetManager: Finished task 0.0 in stage 108.0 (TID 108) in 469 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:04:00 INFO TaskSchedulerImpl: Removed TaskSet 108.0, whose tasks have all completed, from pool 
15/05/13 10:04:00 INFO DAGScheduler: Stage 108 (reduce at JsonRDD.scala:51) finished in 0.481 s
15/05/13 10:04:00 INFO DAGScheduler: Job 111 finished: reduce at JsonRDD.scala:51, took 0.516732 s
15/05/13 10:04:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:04:01 INFO DAGScheduler: Got job 112 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:04:01 INFO DAGScheduler: Final stage: Stage 109(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:04:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:04:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:04:01 INFO DAGScheduler: Submitting Stage 109 (MapPartitionsRDD[787] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:04:01 INFO MemoryStore: ensureFreeSpace(20984) called with curMem=14935759, maxMem=278302556
15/05/13 10:04:01 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 20.5 KB, free 251.1 MB)
15/05/13 10:04:01 INFO MemoryStore: ensureFreeSpace(10942) called with curMem=14956743, maxMem=278302556
15/05/13 10:04:01 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 10.7 KB, free 251.1 MB)
15/05/13 10:04:01 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.5 MB)
15/05/13 10:04:01 INFO BlockManagerMaster: Updated info of block broadcast_164_piece0
15/05/13 10:04:01 INFO SparkContext: Created broadcast 164 from broadcast at DAGScheduler.scala:839
15/05/13 10:04:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 109 (MapPartitionsRDD[787] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:04:01 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks
15/05/13 10:04:01 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 109, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:04:01 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.9 MB)
15/05/13 10:04:01 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 109) in 138 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:04:01 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool 
15/05/13 10:04:01 INFO DAGScheduler: Stage 109 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.141 s
15/05/13 10:04:01 INFO DAGScheduler: Job 112 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.180773 s
15/05/13 10:04:01 INFO JobScheduler: Finished job streaming job 1431525840000 ms.0 from job set of time 1431525840000 ms
15/05/13 10:04:01 INFO JobScheduler: Total delay: 1.256 s for time 1431525840000 ms (execution: 1.101 s)
15/05/13 10:04:01 INFO MapPartitionsRDD: Removing RDD 762 from persistence list
15/05/13 10:04:01 INFO BlockManager: Removing RDD 762
15/05/13 10:04:01 INFO UnionRDD: Removing RDD 761 from persistence list
15/05/13 10:04:01 INFO BlockManager: Removing RDD 761
15/05/13 10:04:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525780000 ms: 1431525720000 ms
15/05/13 10:04:01 INFO JobGenerator: Checkpointing graph for time 1431525840000 ms
15/05/13 10:04:01 INFO DStreamGraph: Updating checkpoint data for time 1431525840000 ms
15/05/13 10:04:01 INFO DStreamGraph: Updated checkpoint data for time 1431525840000 ms
15/05/13 10:04:01 INFO CheckpointWriter: Saving checkpoint for time 1431525840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000'
15/05/13 10:04:01 INFO CheckpointWriter: Checkpoint for time 1431525840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000', took 7653 bytes and 65 ms
15/05/13 10:04:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525840000 ms
15/05/13 10:04:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525840000 ms
15/05/13 10:04:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:05:00 INFO FileInputDStream: Finding new files took 39 ms
15/05/13 10:05:00 INFO FileInputDStream: New files at time 1431525900000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525789374.json
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=14967685, maxMem=278302556
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 232.9 KB, free 250.9 MB)
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 160
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=15206217, maxMem=278302556
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_160
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_160 of size 6040 dropped from memory (free 263066671)
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 34.9 KB, free 250.9 MB)
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_160_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_160_piece0 of size 4226 dropped from memory (free 263070897)
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_165_piece0
15/05/13 10:05:00 INFO SparkContext: Created broadcast 165 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_160_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_160_piece0
15/05/13 10:05:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_160_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 10:05:00 INFO JobScheduler: Starting job streaming job 1431525900000 ms.0 from job set of time 1431525900000 ms
15/05/13 10:05:00 INFO JobScheduler: Added jobs for time 1431525900000 ms
15/05/13 10:05:00 INFO JobGenerator: Checkpointing graph for time 1431525900000 ms
15/05/13 10:05:00 INFO DStreamGraph: Updating checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO DStreamGraph: Updated checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 160
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 159
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_159
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_159 of size 238532 dropped from memory (free 263309429)
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_159_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_159_piece0 of size 35708 dropped from memory (free 263345137)
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_159_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_159_piece0
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_159_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.9 MB)
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 159
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 158
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_158
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_158 of size 21344 dropped from memory (free 263366481)
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_158_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_158_piece0 of size 11172 dropped from memory (free 263377653)
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_158_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_158_piece0
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_158_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 264.0 MB)
15/05/13 10:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431525900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000'
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 158
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 164
15/05/13 10:05:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83231): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_164
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_164 of size 20984 dropped from memory (free 263398637)
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_164_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_164_piece0 of size 10942 dropped from memory (free 263409579)
15/05/13 10:05:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83231): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431525900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000'
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_164_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_164_piece0
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_164_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 264.0 MB)
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 164
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 163
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_163_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_163_piece0 of size 4226 dropped from memory (free 263413805)
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_163_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_163_piece0
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_163
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_163 of size 6040 dropped from memory (free 263419845)
15/05/13 10:05:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:05:00 INFO DAGScheduler: Got job 113 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:05:00 INFO DAGScheduler: Final stage: Stage 110(reduce at JsonRDD.scala:51)
15/05/13 10:05:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_163_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 10:05:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:05:00 INFO DAGScheduler: Submitting Stage 110 (MapPartitionsRDD[794] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=14882711, maxMem=278302556
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 5.9 KB, free 251.2 MB)
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 163
15/05/13 10:05:00 INFO BlockManager: Removing broadcast 161
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_161_piece0
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_161_piece0 of size 11066 dropped from memory (free 263424871)
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_161_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 263.6 MB)
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=14877685, maxMem=278302556
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_161_piece0
15/05/13 10:05:00 INFO BlockManager: Removing block broadcast_161
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 4.1 KB, free 251.2 MB)
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_161 of size 21168 dropped from memory (free 263441813)
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_166_piece0
15/05/13 10:05:00 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:839
15/05/13 10:05:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 110 (MapPartitionsRDD[794] at map at JsonRDD.scala:51)
15/05/13 10:05:00 INFO TaskSchedulerImpl: Adding task set 110.0 with 1 tasks
15/05/13 10:05:00 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 110, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:05:00 INFO BlockManagerInfo: Removed broadcast_161_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 264.0 MB)
15/05/13 10:05:00 INFO ContextCleaner: Cleaned broadcast 161
15/05/13 10:05:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.8 MB)
15/05/13 10:05:00 INFO CheckpointWriter: Checkpoint for time 1431525900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000', took 7660 bytes and 99 ms
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.8 MB)
15/05/13 10:05:00 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 110) in 233 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:05:00 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool 
15/05/13 10:05:00 INFO DAGScheduler: Stage 110 (reduce at JsonRDD.scala:51) finished in 0.243 s
15/05/13 10:05:00 INFO DAGScheduler: Job 113 finished: reduce at JsonRDD.scala:51, took 0.261041 s
15/05/13 10:05:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:05:00 INFO DAGScheduler: Got job 114 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:05:00 INFO DAGScheduler: Final stage: Stage 111(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:05:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:05:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:05:00 INFO DAGScheduler: Submitting Stage 111 (MapPartitionsRDD[801] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=14860743, maxMem=278302556
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 20.4 KB, free 251.2 MB)
15/05/13 10:05:00 INFO MemoryStore: ensureFreeSpace(10828) called with curMem=14881663, maxMem=278302556
15/05/13 10:05:00 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 10.6 KB, free 251.2 MB)
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.5 MB)
15/05/13 10:05:00 INFO BlockManagerMaster: Updated info of block broadcast_167_piece0
15/05/13 10:05:00 INFO SparkContext: Created broadcast 167 from broadcast at DAGScheduler.scala:839
15/05/13 10:05:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 111 (MapPartitionsRDD[801] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:05:00 INFO TaskSchedulerImpl: Adding task set 111.0 with 1 tasks
15/05/13 10:05:00 INFO TaskSetManager: Starting task 0.0 in stage 111.0 (TID 111, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:05:00 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:05:00 INFO TaskSetManager: Finished task 0.0 in stage 111.0 (TID 111) in 97 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:05:00 INFO TaskSchedulerImpl: Removed TaskSet 111.0, whose tasks have all completed, from pool 
15/05/13 10:05:00 INFO DAGScheduler: Stage 111 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.097 s
15/05/13 10:05:00 INFO DAGScheduler: Job 114 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.114331 s
15/05/13 10:05:00 INFO JobScheduler: Finished job streaming job 1431525900000 ms.0 from job set of time 1431525900000 ms
15/05/13 10:05:00 INFO JobScheduler: Total delay: 0.730 s for time 1431525900000 ms (execution: 0.601 s)
15/05/13 10:05:00 INFO MapPartitionsRDD: Removing RDD 776 from persistence list
15/05/13 10:05:00 INFO BlockManager: Removing RDD 776
15/05/13 10:05:00 INFO UnionRDD: Removing RDD 775 from persistence list
15/05/13 10:05:00 INFO BlockManager: Removing RDD 775
15/05/13 10:05:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431525840000 ms: 1431525780000 ms
15/05/13 10:05:00 INFO JobGenerator: Checkpointing graph for time 1431525900000 ms
15/05/13 10:05:00 INFO DStreamGraph: Updating checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO DStreamGraph: Updated checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431525900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000'
15/05/13 10:05:00 INFO CheckpointWriter: Checkpoint for time 1431525900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000', took 7654 bytes and 46 ms
15/05/13 10:05:00 INFO DStreamGraph: Clearing checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO DStreamGraph: Cleared checkpoint data for time 1431525900000 ms
15/05/13 10:05:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:06:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 10:06:00 INFO FileInputDStream: New files at time 1431525960000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525850276.json
15/05/13 10:06:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=14892491, maxMem=278302556
15/05/13 10:06:00 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 232.9 KB, free 251.0 MB)
15/05/13 10:06:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=15131023, maxMem=278302556
15/05/13 10:06:00 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 34.9 KB, free 250.9 MB)
15/05/13 10:06:00 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:06:00 INFO BlockManagerMaster: Updated info of block broadcast_168_piece0
15/05/13 10:06:00 INFO SparkContext: Created broadcast 168 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:06:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:06:00 INFO JobScheduler: Added jobs for time 1431525960000 ms
15/05/13 10:06:00 INFO JobGenerator: Checkpointing graph for time 1431525960000 ms
15/05/13 10:06:00 INFO DStreamGraph: Updating checkpoint data for time 1431525960000 ms
15/05/13 10:06:00 INFO JobScheduler: Starting job streaming job 1431525960000 ms.0 from job set of time 1431525960000 ms
15/05/13 10:06:00 INFO DStreamGraph: Updated checkpoint data for time 1431525960000 ms
15/05/13 10:06:00 INFO CheckpointWriter: Saving checkpoint for time 1431525960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525960000'
15/05/13 10:06:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:06:00 INFO DAGScheduler: Got job 115 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:06:00 INFO DAGScheduler: Final stage: Stage 112(reduce at JsonRDD.scala:51)
15/05/13 10:06:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:06:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:06:00 INFO DAGScheduler: Submitting Stage 112 (MapPartitionsRDD[808] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:06:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525600000
15/05/13 10:06:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=15166731, maxMem=278302556
15/05/13 10:06:00 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 5.9 KB, free 250.9 MB)
15/05/13 10:06:00 INFO CheckpointWriter: Checkpoint for time 1431525960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525960000', took 7666 bytes and 93 ms
15/05/13 10:06:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=15172771, maxMem=278302556
15/05/13 10:06:00 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 4.1 KB, free 250.9 MB)
15/05/13 10:06:00 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:06:00 INFO BlockManagerMaster: Updated info of block broadcast_169_piece0
15/05/13 10:06:00 INFO SparkContext: Created broadcast 169 from broadcast at DAGScheduler.scala:839
15/05/13 10:06:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 112 (MapPartitionsRDD[808] at map at JsonRDD.scala:51)
15/05/13 10:06:00 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks
15/05/13 10:06:00 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 112, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:06:00 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 264.0 MB)
15/05/13 10:06:00 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.9 MB)
15/05/13 10:06:00 INFO DAGScheduler: Stage 112 (reduce at JsonRDD.scala:51) finished in 0.621 s
15/05/13 10:06:00 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 112) in 605 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:06:00 INFO DAGScheduler: Job 115 finished: reduce at JsonRDD.scala:51, took 0.659059 s
15/05/13 10:06:00 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool 
15/05/13 10:06:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:06:01 INFO DAGScheduler: Got job 116 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:06:01 INFO DAGScheduler: Final stage: Stage 113(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:06:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:06:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:06:01 INFO DAGScheduler: Submitting Stage 113 (MapPartitionsRDD[815] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:06:01 INFO MemoryStore: ensureFreeSpace(20664) called with curMem=15176999, maxMem=278302556
15/05/13 10:06:01 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 20.2 KB, free 250.9 MB)
15/05/13 10:06:01 INFO MemoryStore: ensureFreeSpace(10813) called with curMem=15197663, maxMem=278302556
15/05/13 10:06:01 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 10.6 KB, free 250.9 MB)
15/05/13 10:06:01 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.5 MB)
15/05/13 10:06:01 INFO BlockManagerMaster: Updated info of block broadcast_170_piece0
15/05/13 10:06:01 INFO SparkContext: Created broadcast 170 from broadcast at DAGScheduler.scala:839
15/05/13 10:06:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 113 (MapPartitionsRDD[815] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:06:01 INFO TaskSchedulerImpl: Adding task set 113.0 with 1 tasks
15/05/13 10:06:01 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 113, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:06:01 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:06:01 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:06:01 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 113) in 441 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:06:01 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool 
15/05/13 10:06:01 INFO DAGScheduler: Stage 113 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.443 s
15/05/13 10:06:01 INFO DAGScheduler: Job 116 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.486112 s
15/05/13 10:06:01 INFO JobScheduler: Finished job streaming job 1431525960000 ms.0 from job set of time 1431525960000 ms
15/05/13 10:06:01 INFO JobScheduler: Total delay: 1.669 s for time 1431525960000 ms (execution: 1.523 s)
15/05/13 10:06:01 INFO MapPartitionsRDD: Removing RDD 790 from persistence list
15/05/13 10:06:01 INFO BlockManager: Removing RDD 790
15/05/13 10:06:01 INFO UnionRDD: Removing RDD 789 from persistence list
15/05/13 10:06:01 INFO BlockManager: Removing RDD 789
15/05/13 10:06:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525900000 ms: 1431525840000 ms
15/05/13 10:06:01 INFO JobGenerator: Checkpointing graph for time 1431525960000 ms
15/05/13 10:06:01 INFO DStreamGraph: Updating checkpoint data for time 1431525960000 ms
15/05/13 10:06:01 INFO DStreamGraph: Updated checkpoint data for time 1431525960000 ms
15/05/13 10:06:01 INFO CheckpointWriter: Saving checkpoint for time 1431525960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525960000'
15/05/13 10:06:01 INFO CheckpointWriter: Checkpoint for time 1431525960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525960000', took 7657 bytes and 57 ms
15/05/13 10:06:01 INFO DStreamGraph: Clearing checkpoint data for time 1431525960000 ms
15/05/13 10:06:01 INFO DStreamGraph: Cleared checkpoint data for time 1431525960000 ms
15/05/13 10:06:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:07:00 INFO FileInputDStream: Finding new files took 42 ms
15/05/13 10:07:00 INFO FileInputDStream: New files at time 1431526020000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525910699.json
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=15208476, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 232.9 KB, free 250.7 MB)
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=15447008, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 34.9 KB, free 250.6 MB)
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:07:00 INFO BlockManagerMaster: Updated info of block broadcast_171_piece0
15/05/13 10:07:00 INFO SparkContext: Created broadcast 171 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:07:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:07:00 INFO JobScheduler: Starting job streaming job 1431526020000 ms.0 from job set of time 1431526020000 ms
15/05/13 10:07:00 INFO JobScheduler: Added jobs for time 1431526020000 ms
15/05/13 10:07:00 INFO JobGenerator: Checkpointing graph for time 1431526020000 ms
15/05/13 10:07:00 INFO DStreamGraph: Updating checkpoint data for time 1431526020000 ms
15/05/13 10:07:00 INFO DStreamGraph: Updated checkpoint data for time 1431526020000 ms
15/05/13 10:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431526020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000'
15/05/13 10:07:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83251): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431526020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000'
15/05/13 10:07:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:07:00 INFO DAGScheduler: Got job 117 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:07:00 INFO DAGScheduler: Final stage: Stage 114(reduce at JsonRDD.scala:51)
15/05/13 10:07:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:07:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:07:00 INFO DAGScheduler: Submitting Stage 114 (MapPartitionsRDD[822] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=15482716, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 5.9 KB, free 250.6 MB)
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=15488756, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 4.1 KB, free 250.6 MB)
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:07:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83253): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:07:00 INFO BlockManagerMaster: Updated info of block broadcast_172_piece0
15/05/13 10:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431526020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000'
15/05/13 10:07:00 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:839
15/05/13 10:07:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 114 (MapPartitionsRDD[822] at map at JsonRDD.scala:51)
15/05/13 10:07:00 INFO TaskSchedulerImpl: Adding task set 114.0 with 1 tasks
15/05/13 10:07:00 INFO TaskSetManager: Starting task 0.0 in stage 114.0 (TID 114, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:07:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83255): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:07:00 WARN CheckpointWriter: Could not write checkpoint for time 1431526020000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000'
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:07:00 INFO TaskSetManager: Finished task 0.0 in stage 114.0 (TID 114) in 308 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:07:00 INFO TaskSchedulerImpl: Removed TaskSet 114.0, whose tasks have all completed, from pool 
15/05/13 10:07:00 INFO DAGScheduler: Stage 114 (reduce at JsonRDD.scala:51) finished in 0.322 s
15/05/13 10:07:00 INFO DAGScheduler: Job 117 finished: reduce at JsonRDD.scala:51, took 0.357726 s
15/05/13 10:07:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:07:00 INFO DAGScheduler: Got job 118 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:07:00 INFO DAGScheduler: Final stage: Stage 115(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:07:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:07:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:07:00 INFO DAGScheduler: Submitting Stage 115 (MapPartitionsRDD[829] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=15492981, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 20.0 KB, free 250.6 MB)
15/05/13 10:07:00 INFO MemoryStore: ensureFreeSpace(10756) called with curMem=15513469, maxMem=278302556
15/05/13 10:07:00 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 10.5 KB, free 250.6 MB)
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.4 MB)
15/05/13 10:07:00 INFO BlockManagerMaster: Updated info of block broadcast_173_piece0
15/05/13 10:07:00 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:839
15/05/13 10:07:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 115 (MapPartitionsRDD[829] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:07:00 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks
15/05/13 10:07:00 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 115, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:07:00 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.5 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 115) in 93 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:07:01 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool 
15/05/13 10:07:01 INFO DAGScheduler: Stage 115 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.096 s
15/05/13 10:07:01 INFO DAGScheduler: Job 118 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.137998 s
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 166
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_166_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_166_piece0 of size 4226 dropped from memory (free 262782557)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_166_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_166_piece0
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_166
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_166 of size 6040 dropped from memory (free 262788597)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_166_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 166
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 169
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_169_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_169_piece0 of size 4228 dropped from memory (free 262792825)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_169_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_169_piece0
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_169
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_169 of size 6040 dropped from memory (free 262798865)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_169_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 169
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 167
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_167_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_167_piece0 of size 10828 dropped from memory (free 262809693)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_167_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.5 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_167_piece0
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_167
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_167 of size 20920 dropped from memory (free 262830613)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_167_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO JobScheduler: Finished job streaming job 1431526020000 ms.0 from job set of time 1431526020000 ms
15/05/13 10:07:01 INFO JobScheduler: Total delay: 1.127 s for time 1431526020000 ms (execution: 0.941 s)
15/05/13 10:07:01 INFO MapPartitionsRDD: Removing RDD 804 from persistence list
15/05/13 10:07:01 INFO BlockManager: Removing RDD 804
15/05/13 10:07:01 INFO UnionRDD: Removing RDD 803 from persistence list
15/05/13 10:07:01 INFO BlockManager: Removing RDD 803
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 167
15/05/13 10:07:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431525960000 ms: 1431525900000 ms
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 173
15/05/13 10:07:01 INFO JobGenerator: Checkpointing graph for time 1431526020000 ms
15/05/13 10:07:01 INFO DStreamGraph: Updating checkpoint data for time 1431526020000 ms
15/05/13 10:07:01 INFO DStreamGraph: Updated checkpoint data for time 1431526020000 ms
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_173
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_173 of size 20488 dropped from memory (free 262851101)
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_173_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_173_piece0 of size 10756 dropped from memory (free 262861857)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_173_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.5 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_173_piece0
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_173_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.5 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO CheckpointWriter: Saving checkpoint for time 1431526020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000'
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 173
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 172
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_172_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_172_piece0 of size 4225 dropped from memory (free 262866082)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_172_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_172_piece0
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_172
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_172 of size 6040 dropped from memory (free 262872122)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_172_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 172
15/05/13 10:07:01 INFO BlockManager: Removing broadcast 170
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_170_piece0
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_170_piece0 of size 10813 dropped from memory (free 262882935)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_170_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.5 MB)
15/05/13 10:07:01 INFO BlockManagerMaster: Updated info of block broadcast_170_piece0
15/05/13 10:07:01 INFO BlockManager: Removing block broadcast_170
15/05/13 10:07:01 INFO MemoryStore: Block broadcast_170 of size 20664 dropped from memory (free 262903599)
15/05/13 10:07:01 INFO BlockManagerInfo: Removed broadcast_170_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:07:01 INFO ContextCleaner: Cleaned broadcast 170
15/05/13 10:07:01 INFO CheckpointWriter: Checkpoint for time 1431526020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526020000', took 7657 bytes and 65 ms
15/05/13 10:07:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526020000 ms
15/05/13 10:07:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526020000 ms
15/05/13 10:07:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:08:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 10:08:00 INFO FileInputDStream: New files at time 1431526080000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431525971046.json
15/05/13 10:08:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=15398957, maxMem=278302556
15/05/13 10:08:00 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 232.9 KB, free 250.5 MB)
15/05/13 10:08:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=15637489, maxMem=278302556
15/05/13 10:08:00 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 34.9 KB, free 250.5 MB)
15/05/13 10:08:00 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:08:00 INFO BlockManagerMaster: Updated info of block broadcast_174_piece0
15/05/13 10:08:00 INFO SparkContext: Created broadcast 174 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:08:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:08:00 INFO JobScheduler: Added jobs for time 1431526080000 ms
15/05/13 10:08:00 INFO JobGenerator: Checkpointing graph for time 1431526080000 ms
15/05/13 10:08:00 INFO JobScheduler: Starting job streaming job 1431526080000 ms.0 from job set of time 1431526080000 ms
15/05/13 10:08:00 INFO DStreamGraph: Updating checkpoint data for time 1431526080000 ms
15/05/13 10:08:00 INFO DStreamGraph: Updated checkpoint data for time 1431526080000 ms
15/05/13 10:08:00 INFO CheckpointWriter: Saving checkpoint for time 1431526080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526080000'
15/05/13 10:08:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525780000
15/05/13 10:08:00 INFO CheckpointWriter: Checkpoint for time 1431526080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526080000', took 7664 bytes and 82 ms
15/05/13 10:08:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:08:00 INFO DAGScheduler: Got job 119 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:08:00 INFO DAGScheduler: Final stage: Stage 116(reduce at JsonRDD.scala:51)
15/05/13 10:08:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:08:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:08:00 INFO DAGScheduler: Submitting Stage 116 (MapPartitionsRDD[836] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:08:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=15673197, maxMem=278302556
15/05/13 10:08:00 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 5.9 KB, free 250.5 MB)
15/05/13 10:08:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=15679237, maxMem=278302556
15/05/13 10:08:00 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 4.1 KB, free 250.5 MB)
15/05/13 10:08:00 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:08:00 INFO BlockManagerMaster: Updated info of block broadcast_175_piece0
15/05/13 10:08:00 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:839
15/05/13 10:08:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 116 (MapPartitionsRDD[836] at map at JsonRDD.scala:51)
15/05/13 10:08:00 INFO TaskSchedulerImpl: Adding task set 116.0 with 1 tasks
15/05/13 10:08:00 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 116, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:08:00 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:08:00 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:08:00 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 116) in 520 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:08:00 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool 
15/05/13 10:08:00 INFO DAGScheduler: Stage 116 (reduce at JsonRDD.scala:51) finished in 0.536 s
15/05/13 10:08:00 INFO DAGScheduler: Job 119 finished: reduce at JsonRDD.scala:51, took 0.571811 s
15/05/13 10:08:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:08:01 INFO DAGScheduler: Got job 120 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:08:01 INFO DAGScheduler: Final stage: Stage 117(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:08:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:08:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:08:01 INFO DAGScheduler: Submitting Stage 117 (MapPartitionsRDD[843] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:08:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=15683463, maxMem=278302556
15/05/13 10:08:01 INFO MemoryStore: Block broadcast_176 stored as values in memory (estimated size 20.4 KB, free 250.4 MB)
15/05/13 10:08:01 INFO MemoryStore: ensureFreeSpace(10845) called with curMem=15704383, maxMem=278302556
15/05/13 10:08:01 INFO MemoryStore: Block broadcast_176_piece0 stored as bytes in memory (estimated size 10.6 KB, free 250.4 MB)
15/05/13 10:08:01 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.4 MB)
15/05/13 10:08:01 INFO BlockManagerMaster: Updated info of block broadcast_176_piece0
15/05/13 10:08:01 INFO SparkContext: Created broadcast 176 from broadcast at DAGScheduler.scala:839
15/05/13 10:08:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 117 (MapPartitionsRDD[843] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:08:01 INFO TaskSchedulerImpl: Adding task set 117.0 with 1 tasks
15/05/13 10:08:01 INFO TaskSetManager: Starting task 0.0 in stage 117.0 (TID 117, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:08:01 INFO BlockManagerInfo: Added broadcast_176_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:08:01 INFO TaskSetManager: Finished task 0.0 in stage 117.0 (TID 117) in 155 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:08:01 INFO TaskSchedulerImpl: Removed TaskSet 117.0, whose tasks have all completed, from pool 
15/05/13 10:08:01 INFO DAGScheduler: Stage 117 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.158 s
15/05/13 10:08:01 INFO DAGScheduler: Job 120 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.198727 s
15/05/13 10:08:01 INFO JobScheduler: Finished job streaming job 1431526080000 ms.0 from job set of time 1431526080000 ms
15/05/13 10:08:01 INFO JobScheduler: Total delay: 1.369 s for time 1431526080000 ms (execution: 1.160 s)
15/05/13 10:08:01 INFO MapPartitionsRDD: Removing RDD 818 from persistence list
15/05/13 10:08:01 INFO BlockManager: Removing RDD 818
15/05/13 10:08:01 INFO UnionRDD: Removing RDD 817 from persistence list
15/05/13 10:08:01 INFO BlockManager: Removing RDD 817
15/05/13 10:08:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526020000 ms: 1431525960000 ms
15/05/13 10:08:01 INFO JobGenerator: Checkpointing graph for time 1431526080000 ms
15/05/13 10:08:01 INFO DStreamGraph: Updating checkpoint data for time 1431526080000 ms
15/05/13 10:08:01 INFO DStreamGraph: Updated checkpoint data for time 1431526080000 ms
15/05/13 10:08:01 INFO CheckpointWriter: Saving checkpoint for time 1431526080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526080000'
15/05/13 10:08:01 INFO CheckpointWriter: Checkpoint for time 1431526080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526080000', took 7656 bytes and 61 ms
15/05/13 10:08:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526080000 ms
15/05/13 10:08:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526080000 ms
15/05/13 10:08:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:09:00 INFO FileInputDStream: Finding new files took 28 ms
15/05/13 10:09:00 INFO FileInputDStream: New files at time 1431526140000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526036462.json
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=15715228, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_177 stored as values in memory (estimated size 232.9 KB, free 250.2 MB)
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=15953760, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_177_piece0 stored as bytes in memory (estimated size 34.9 KB, free 250.2 MB)
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:09:00 INFO BlockManagerMaster: Updated info of block broadcast_177_piece0
15/05/13 10:09:00 INFO SparkContext: Created broadcast 177 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:09:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:09:00 INFO JobScheduler: Added jobs for time 1431526140000 ms
15/05/13 10:09:00 INFO JobGenerator: Checkpointing graph for time 1431526140000 ms
15/05/13 10:09:00 INFO DStreamGraph: Updating checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO JobScheduler: Starting job streaming job 1431526140000 ms.0 from job set of time 1431526140000 ms
15/05/13 10:09:00 INFO DStreamGraph: Updated checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431526140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000'
15/05/13 10:09:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83267): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:09:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83267): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431526140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000'
15/05/13 10:09:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:09:00 INFO DAGScheduler: Got job 121 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:09:00 INFO DAGScheduler: Final stage: Stage 118(reduce at JsonRDD.scala:51)
15/05/13 10:09:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:09:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:09:00 INFO DAGScheduler: Submitting Stage 118 (MapPartitionsRDD[850] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=15989468, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_178 stored as values in memory (estimated size 5.9 KB, free 250.2 MB)
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=15995508, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_178_piece0 stored as bytes in memory (estimated size 4.1 KB, free 250.2 MB)
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:09:00 INFO BlockManagerMaster: Updated info of block broadcast_178_piece0
15/05/13 10:09:00 INFO SparkContext: Created broadcast 178 from broadcast at DAGScheduler.scala:839
15/05/13 10:09:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 118 (MapPartitionsRDD[850] at map at JsonRDD.scala:51)
15/05/13 10:09:00 INFO TaskSchedulerImpl: Adding task set 118.0 with 1 tasks
15/05/13 10:09:00 INFO TaskSetManager: Starting task 0.0 in stage 118.0 (TID 118, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:09:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525840000.bk
15/05/13 10:09:00 INFO CheckpointWriter: Checkpoint for time 1431526140000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000', took 7666 bytes and 86 ms
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_178_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.9 MB)
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_177_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.9 MB)
15/05/13 10:09:00 INFO TaskSetManager: Finished task 0.0 in stage 118.0 (TID 118) in 288 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:09:00 INFO TaskSchedulerImpl: Removed TaskSet 118.0, whose tasks have all completed, from pool 
15/05/13 10:09:00 INFO DAGScheduler: Stage 118 (reduce at JsonRDD.scala:51) finished in 0.300 s
15/05/13 10:09:00 INFO DAGScheduler: Job 121 finished: reduce at JsonRDD.scala:51, took 0.318031 s
15/05/13 10:09:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:09:00 INFO DAGScheduler: Got job 122 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:09:00 INFO DAGScheduler: Final stage: Stage 119(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:09:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:09:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:09:00 INFO DAGScheduler: Submitting Stage 119 (MapPartitionsRDD[857] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=15999734, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_179 stored as values in memory (estimated size 20.4 KB, free 250.1 MB)
15/05/13 10:09:00 INFO MemoryStore: ensureFreeSpace(10910) called with curMem=16020654, maxMem=278302556
15/05/13 10:09:00 INFO MemoryStore: Block broadcast_179_piece0 stored as bytes in memory (estimated size 10.7 KB, free 250.1 MB)
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.4 MB)
15/05/13 10:09:00 INFO BlockManagerMaster: Updated info of block broadcast_179_piece0
15/05/13 10:09:00 INFO SparkContext: Created broadcast 179 from broadcast at DAGScheduler.scala:839
15/05/13 10:09:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 119 (MapPartitionsRDD[857] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:09:00 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks
15/05/13 10:09:00 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 119, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:09:00 INFO BlockManagerInfo: Added broadcast_179_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.9 MB)
15/05/13 10:09:00 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 119) in 108 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:09:00 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool 
15/05/13 10:09:00 INFO DAGScheduler: Stage 119 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.110 s
15/05/13 10:09:00 INFO DAGScheduler: Job 122 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.136881 s
15/05/13 10:09:00 INFO JobScheduler: Finished job streaming job 1431526140000 ms.0 from job set of time 1431526140000 ms
15/05/13 10:09:00 INFO JobScheduler: Total delay: 0.836 s for time 1431526140000 ms (execution: 0.741 s)
15/05/13 10:09:00 INFO MapPartitionsRDD: Removing RDD 832 from persistence list
15/05/13 10:09:00 INFO BlockManager: Removing RDD 832
15/05/13 10:09:00 INFO UnionRDD: Removing RDD 831 from persistence list
15/05/13 10:09:00 INFO BlockManager: Removing RDD 831
15/05/13 10:09:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431526080000 ms: 1431526020000 ms
15/05/13 10:09:00 INFO JobGenerator: Checkpointing graph for time 1431526140000 ms
15/05/13 10:09:00 INFO DStreamGraph: Updating checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO DStreamGraph: Updated checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431526140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000'
15/05/13 10:09:00 INFO CheckpointWriter: Checkpoint for time 1431526140000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000', took 7659 bytes and 41 ms
15/05/13 10:09:00 INFO DStreamGraph: Clearing checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO DStreamGraph: Cleared checkpoint data for time 1431526140000 ms
15/05/13 10:09:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:10:00 INFO FileInputDStream: Finding new files took 79 ms
15/05/13 10:10:00 INFO FileInputDStream: New files at time 1431526200000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526096814.json
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=16031564, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_180 stored as values in memory (estimated size 232.9 KB, free 249.9 MB)
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=16270096, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_180_piece0 stored as bytes in memory (estimated size 34.9 KB, free 249.9 MB)
15/05/13 10:10:00 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:10:00 INFO BlockManagerMaster: Updated info of block broadcast_180_piece0
15/05/13 10:10:00 INFO SparkContext: Created broadcast 180 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:10:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:10:00 INFO JobScheduler: Added jobs for time 1431526200000 ms
15/05/13 10:10:00 INFO JobGenerator: Checkpointing graph for time 1431526200000 ms
15/05/13 10:10:00 INFO DStreamGraph: Updating checkpoint data for time 1431526200000 ms
15/05/13 10:10:00 INFO JobScheduler: Starting job streaming job 1431526200000 ms.0 from job set of time 1431526200000 ms
15/05/13 10:10:00 INFO DStreamGraph: Updated checkpoint data for time 1431526200000 ms
15/05/13 10:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431526200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000'
15/05/13 10:10:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:10:00 INFO DAGScheduler: Got job 123 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:10:00 INFO DAGScheduler: Final stage: Stage 120(reduce at JsonRDD.scala:51)
15/05/13 10:10:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:10:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:10:00 INFO DAGScheduler: Submitting Stage 120 (MapPartitionsRDD[864] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:10:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83276): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431526200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000'
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=16305804, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 5.9 KB, free 249.9 MB)
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=16311844, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 4.1 KB, free 249.8 MB)
15/05/13 10:10:00 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:10:00 INFO BlockManagerMaster: Updated info of block broadcast_181_piece0
15/05/13 10:10:00 INFO SparkContext: Created broadcast 181 from broadcast at DAGScheduler.scala:839
15/05/13 10:10:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 120 (MapPartitionsRDD[864] at map at JsonRDD.scala:51)
15/05/13 10:10:00 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks
15/05/13 10:10:00 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 120, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:10:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83278): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431526200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000'
15/05/13 10:10:00 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:10:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525900000.bk
15/05/13 10:10:00 INFO CheckpointWriter: Checkpoint for time 1431526200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000', took 7666 bytes and 170 ms
15/05/13 10:10:00 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:10:00 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 120) in 378 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:10:00 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool 
15/05/13 10:10:00 INFO DAGScheduler: Stage 120 (reduce at JsonRDD.scala:51) finished in 0.401 s
15/05/13 10:10:00 INFO DAGScheduler: Job 123 finished: reduce at JsonRDD.scala:51, took 0.454781 s
15/05/13 10:10:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:10:00 INFO DAGScheduler: Got job 124 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:10:00 INFO DAGScheduler: Final stage: Stage 121(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:10:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:10:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:10:00 INFO DAGScheduler: Submitting Stage 121 (MapPartitionsRDD[871] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=16316070, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 20.4 KB, free 249.8 MB)
15/05/13 10:10:00 INFO MemoryStore: ensureFreeSpace(10879) called with curMem=16336990, maxMem=278302556
15/05/13 10:10:00 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 10.6 KB, free 249.8 MB)
15/05/13 10:10:00 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.3 MB)
15/05/13 10:10:00 INFO BlockManagerMaster: Updated info of block broadcast_182_piece0
15/05/13 10:10:00 INFO SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:839
15/05/13 10:10:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 121 (MapPartitionsRDD[871] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:10:01 INFO TaskSchedulerImpl: Adding task set 121.0 with 1 tasks
15/05/13 10:10:01 INFO TaskSetManager: Starting task 0.0 in stage 121.0 (TID 121, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:10:01 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 263.9 MB)
15/05/13 10:10:01 INFO BlockManagerInfo: Added broadcast_180_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.9 MB)
15/05/13 10:10:01 INFO TaskSetManager: Finished task 0.0 in stage 121.0 (TID 121) in 359 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:10:01 INFO TaskSchedulerImpl: Removed TaskSet 121.0, whose tasks have all completed, from pool 
15/05/13 10:10:01 INFO DAGScheduler: Stage 121 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.361 s
15/05/13 10:10:01 INFO DAGScheduler: Job 124 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.402449 s
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 181
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_181
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_181 of size 6040 dropped from memory (free 261960727)
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_181_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_181_piece0 of size 4226 dropped from memory (free 261964953)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_181_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_181_piece0
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_181_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 181
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 176
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_176_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_176_piece0 of size 10845 dropped from memory (free 261975798)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_176_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_176_piece0
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_176
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_176 of size 20920 dropped from memory (free 261996718)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_176_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 176
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 175
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_175_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_175_piece0 of size 4226 dropped from memory (free 262000944)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_175_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_175_piece0
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_175
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_175 of size 6040 dropped from memory (free 262006984)
15/05/13 10:10:01 INFO JobScheduler: Finished job streaming job 1431526200000 ms.0 from job set of time 1431526200000 ms
15/05/13 10:10:01 INFO JobScheduler: Total delay: 1.463 s for time 1431526200000 ms (execution: 1.289 s)
15/05/13 10:10:01 INFO MapPartitionsRDD: Removing RDD 846 from persistence list
15/05/13 10:10:01 INFO BlockManager: Removing RDD 846
15/05/13 10:10:01 INFO UnionRDD: Removing RDD 845 from persistence list
15/05/13 10:10:01 INFO BlockManager: Removing RDD 845
15/05/13 10:10:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526140000 ms: 1431526080000 ms
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_175_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:10:01 INFO JobGenerator: Checkpointing graph for time 1431526200000 ms
15/05/13 10:10:01 INFO DStreamGraph: Updating checkpoint data for time 1431526200000 ms
15/05/13 10:10:01 INFO DStreamGraph: Updated checkpoint data for time 1431526200000 ms
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 175
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 174
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_174
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_174 of size 238532 dropped from memory (free 262245516)
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_174_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_174_piece0 of size 35708 dropped from memory (free 262281224)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_174_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_174_piece0
15/05/13 10:10:01 INFO CheckpointWriter: Saving checkpoint for time 1431526200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000'
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_174_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 174
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 182
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_182_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_182_piece0 of size 10879 dropped from memory (free 262292103)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_182_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_182_piece0
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_182
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_182 of size 20920 dropped from memory (free 262313023)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_182_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 263.9 MB)
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 182
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 179
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_179
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_179 of size 20920 dropped from memory (free 262333943)
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_179_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_179_piece0 of size 10910 dropped from memory (free 262344853)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_179_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_179_piece0
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_179_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 263.9 MB)
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 179
15/05/13 10:10:01 INFO BlockManager: Removing broadcast 178
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_178_piece0
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_178_piece0 of size 4226 dropped from memory (free 262349079)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_178_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:10:01 INFO BlockManagerMaster: Updated info of block broadcast_178_piece0
15/05/13 10:10:01 INFO BlockManager: Removing block broadcast_178
15/05/13 10:10:01 INFO MemoryStore: Block broadcast_178 of size 6040 dropped from memory (free 262355119)
15/05/13 10:10:01 INFO BlockManagerInfo: Removed broadcast_178_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 10:10:01 INFO CheckpointWriter: Checkpoint for time 1431526200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526200000', took 7656 bytes and 77 ms
15/05/13 10:10:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526200000 ms
15/05/13 10:10:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526200000 ms
15/05/13 10:10:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:10:01 INFO ContextCleaner: Cleaned broadcast 178
15/05/13 10:11:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 10:11:00 INFO FileInputDStream: New files at time 1431526260000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526157455.json
15/05/13 10:11:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=15947437, maxMem=278302556
15/05/13 10:11:00 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 232.9 KB, free 250.0 MB)
15/05/13 10:11:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=16185969, maxMem=278302556
15/05/13 10:11:00 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 34.9 KB, free 249.9 MB)
15/05/13 10:11:00 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:11:00 INFO BlockManagerMaster: Updated info of block broadcast_183_piece0
15/05/13 10:11:00 INFO SparkContext: Created broadcast 183 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:11:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:11:00 INFO JobScheduler: Added jobs for time 1431526260000 ms
15/05/13 10:11:00 INFO JobGenerator: Checkpointing graph for time 1431526260000 ms
15/05/13 10:11:00 INFO JobScheduler: Starting job streaming job 1431526260000 ms.0 from job set of time 1431526260000 ms
15/05/13 10:11:00 INFO DStreamGraph: Updating checkpoint data for time 1431526260000 ms
15/05/13 10:11:00 INFO DStreamGraph: Updated checkpoint data for time 1431526260000 ms
15/05/13 10:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431526260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000'
15/05/13 10:11:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83286): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431526260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000'
15/05/13 10:11:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:11:00 INFO DAGScheduler: Got job 125 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:11:00 INFO DAGScheduler: Final stage: Stage 122(reduce at JsonRDD.scala:51)
15/05/13 10:11:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:11:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:11:00 INFO DAGScheduler: Submitting Stage 122 (MapPartitionsRDD[878] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:11:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=16221677, maxMem=278302556
15/05/13 10:11:00 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 5.9 KB, free 249.9 MB)
15/05/13 10:11:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83288): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431526260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000'
15/05/13 10:11:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=16227717, maxMem=278302556
15/05/13 10:11:00 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 4.1 KB, free 249.9 MB)
15/05/13 10:11:00 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:11:00 INFO BlockManagerMaster: Updated info of block broadcast_184_piece0
15/05/13 10:11:00 INFO SparkContext: Created broadcast 184 from broadcast at DAGScheduler.scala:839
15/05/13 10:11:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 122 (MapPartitionsRDD[878] at map at JsonRDD.scala:51)
15/05/13 10:11:00 INFO TaskSchedulerImpl: Adding task set 122.0 with 1 tasks
15/05/13 10:11:00 INFO TaskSetManager: Starting task 0.0 in stage 122.0 (TID 122, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:11:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431525960000.bk
15/05/13 10:11:00 INFO CheckpointWriter: Checkpoint for time 1431526260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000', took 7665 bytes and 192 ms
15/05/13 10:11:00 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.9 MB)
15/05/13 10:11:00 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.8 MB)
15/05/13 10:11:00 INFO TaskSetManager: Finished task 0.0 in stage 122.0 (TID 122) in 404 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:11:00 INFO DAGScheduler: Stage 122 (reduce at JsonRDD.scala:51) finished in 0.419 s
15/05/13 10:11:00 INFO TaskSchedulerImpl: Removed TaskSet 122.0, whose tasks have all completed, from pool 
15/05/13 10:11:00 INFO DAGScheduler: Job 125 finished: reduce at JsonRDD.scala:51, took 0.461848 s
15/05/13 10:11:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:11:01 INFO DAGScheduler: Got job 126 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:11:01 INFO DAGScheduler: Final stage: Stage 123(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:11:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:11:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:11:01 INFO DAGScheduler: Submitting Stage 123 (MapPartitionsRDD[885] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:11:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=16231943, maxMem=278302556
15/05/13 10:11:01 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 20.4 KB, free 249.9 MB)
15/05/13 10:11:01 INFO MemoryStore: ensureFreeSpace(10902) called with curMem=16252863, maxMem=278302556
15/05/13 10:11:01 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 10.6 KB, free 249.9 MB)
15/05/13 10:11:01 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.4 MB)
15/05/13 10:11:01 INFO BlockManagerMaster: Updated info of block broadcast_185_piece0
15/05/13 10:11:01 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:839
15/05/13 10:11:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 123 (MapPartitionsRDD[885] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:11:01 INFO TaskSchedulerImpl: Adding task set 123.0 with 1 tasks
15/05/13 10:11:01 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 123, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:11:01 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 263.8 MB)
15/05/13 10:11:01 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 123) in 176 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:11:01 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool 
15/05/13 10:11:01 INFO DAGScheduler: Stage 123 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.179 s
15/05/13 10:11:01 INFO DAGScheduler: Job 126 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.219765 s
15/05/13 10:11:01 INFO JobScheduler: Finished job streaming job 1431526260000 ms.0 from job set of time 1431526260000 ms
15/05/13 10:11:01 INFO JobScheduler: Total delay: 1.329 s for time 1431526260000 ms (execution: 1.115 s)
15/05/13 10:11:01 INFO MapPartitionsRDD: Removing RDD 860 from persistence list
15/05/13 10:11:01 INFO BlockManager: Removing RDD 860
15/05/13 10:11:01 INFO UnionRDD: Removing RDD 859 from persistence list
15/05/13 10:11:01 INFO BlockManager: Removing RDD 859
15/05/13 10:11:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526200000 ms: 1431526140000 ms
15/05/13 10:11:01 INFO JobGenerator: Checkpointing graph for time 1431526260000 ms
15/05/13 10:11:01 INFO DStreamGraph: Updating checkpoint data for time 1431526260000 ms
15/05/13 10:11:01 INFO DStreamGraph: Updated checkpoint data for time 1431526260000 ms
15/05/13 10:11:01 INFO CheckpointWriter: Saving checkpoint for time 1431526260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000'
15/05/13 10:11:01 INFO CheckpointWriter: Checkpoint for time 1431526260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000', took 7657 bytes and 97 ms
15/05/13 10:11:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526260000 ms
15/05/13 10:11:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526260000 ms
15/05/13 10:11:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:12:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 10:12:00 INFO FileInputDStream: New files at time 1431526320000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526219272.json
15/05/13 10:12:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=16263765, maxMem=278302556
15/05/13 10:12:00 INFO MemoryStore: Block broadcast_186 stored as values in memory (estimated size 232.9 KB, free 249.7 MB)
15/05/13 10:12:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=16502297, maxMem=278302556
15/05/13 10:12:00 INFO MemoryStore: Block broadcast_186_piece0 stored as bytes in memory (estimated size 34.9 KB, free 249.6 MB)
15/05/13 10:12:00 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:12:00 INFO BlockManagerMaster: Updated info of block broadcast_186_piece0
15/05/13 10:12:00 INFO SparkContext: Created broadcast 186 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:12:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:12:00 INFO JobScheduler: Added jobs for time 1431526320000 ms
15/05/13 10:12:00 INFO JobGenerator: Checkpointing graph for time 1431526320000 ms
15/05/13 10:12:00 INFO DStreamGraph: Updating checkpoint data for time 1431526320000 ms
15/05/13 10:12:00 INFO DStreamGraph: Updated checkpoint data for time 1431526320000 ms
15/05/13 10:12:00 INFO JobScheduler: Starting job streaming job 1431526320000 ms.0 from job set of time 1431526320000 ms
15/05/13 10:12:00 INFO CheckpointWriter: Saving checkpoint for time 1431526320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000'
15/05/13 10:12:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83301): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:12:00 INFO CheckpointWriter: Saving checkpoint for time 1431526320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000'
15/05/13 10:12:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83303): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:12:00 INFO CheckpointWriter: Saving checkpoint for time 1431526320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000'
15/05/13 10:12:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:12:00 INFO DAGScheduler: Got job 127 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:12:00 INFO DAGScheduler: Final stage: Stage 124(reduce at JsonRDD.scala:51)
15/05/13 10:12:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:12:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:12:00 INFO DAGScheduler: Submitting Stage 124 (MapPartitionsRDD[892] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:12:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=16538007, maxMem=278302556
15/05/13 10:12:00 INFO MemoryStore: Block broadcast_187 stored as values in memory (estimated size 5.9 KB, free 249.6 MB)
15/05/13 10:12:00 INFO MemoryStore: ensureFreeSpace(4230) called with curMem=16544047, maxMem=278302556
15/05/13 10:12:00 INFO MemoryStore: Block broadcast_187_piece0 stored as bytes in memory (estimated size 4.1 KB, free 249.6 MB)
15/05/13 10:12:00 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:12:00 INFO BlockManagerMaster: Updated info of block broadcast_187_piece0
15/05/13 10:12:00 INFO SparkContext: Created broadcast 187 from broadcast at DAGScheduler.scala:839
15/05/13 10:12:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 124 (MapPartitionsRDD[892] at map at JsonRDD.scala:51)
15/05/13 10:12:00 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks
15/05/13 10:12:00 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 124, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:12:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83305): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:12:00 WARN CheckpointWriter: Could not write checkpoint for time 1431526320000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000'
15/05/13 10:12:00 INFO BlockManagerInfo: Added broadcast_187_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:12:00 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.8 MB)
15/05/13 10:12:00 INFO DAGScheduler: Stage 124 (reduce at JsonRDD.scala:51) finished in 0.415 s
15/05/13 10:12:00 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 124) in 399 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:12:00 INFO DAGScheduler: Job 127 finished: reduce at JsonRDD.scala:51, took 0.473120 s
15/05/13 10:12:00 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool 
15/05/13 10:12:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:12:00 INFO DAGScheduler: Got job 128 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:12:00 INFO DAGScheduler: Final stage: Stage 125(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:12:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:12:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:12:01 INFO DAGScheduler: Submitting Stage 125 (MapPartitionsRDD[899] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:12:01 INFO MemoryStore: ensureFreeSpace(21424) called with curMem=16548277, maxMem=278302556
15/05/13 10:12:01 INFO MemoryStore: Block broadcast_188 stored as values in memory (estimated size 20.9 KB, free 249.6 MB)
15/05/13 10:12:01 INFO MemoryStore: ensureFreeSpace(11220) called with curMem=16569701, maxMem=278302556
15/05/13 10:12:01 INFO MemoryStore: Block broadcast_188_piece0 stored as bytes in memory (estimated size 11.0 KB, free 249.6 MB)
15/05/13 10:12:01 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 263.3 MB)
15/05/13 10:12:01 INFO BlockManagerMaster: Updated info of block broadcast_188_piece0
15/05/13 10:12:01 INFO SparkContext: Created broadcast 188 from broadcast at DAGScheduler.scala:839
15/05/13 10:12:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 125 (MapPartitionsRDD[899] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:12:01 INFO TaskSchedulerImpl: Adding task set 125.0 with 1 tasks
15/05/13 10:12:01 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 125, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:12:01 INFO BlockManagerInfo: Added broadcast_188_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 11.0 KB, free: 264.7 MB)
15/05/13 10:12:01 INFO BlockManagerInfo: Added broadcast_186_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:12:01 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 125) in 232 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:12:01 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool 
15/05/13 10:12:01 INFO DAGScheduler: Stage 125 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.235 s
15/05/13 10:12:01 INFO DAGScheduler: Job 128 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.260612 s
15/05/13 10:12:01 INFO JobScheduler: Finished job streaming job 1431526320000 ms.0 from job set of time 1431526320000 ms
15/05/13 10:12:01 INFO JobScheduler: Total delay: 1.310 s for time 1431526320000 ms (execution: 1.112 s)
15/05/13 10:12:01 INFO MapPartitionsRDD: Removing RDD 874 from persistence list
15/05/13 10:12:01 INFO BlockManager: Removing RDD 874
15/05/13 10:12:01 INFO UnionRDD: Removing RDD 873 from persistence list
15/05/13 10:12:01 INFO BlockManager: Removing RDD 873
15/05/13 10:12:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526260000 ms: 1431526200000 ms
15/05/13 10:12:01 INFO JobGenerator: Checkpointing graph for time 1431526320000 ms
15/05/13 10:12:01 INFO DStreamGraph: Updating checkpoint data for time 1431526320000 ms
15/05/13 10:12:01 INFO DStreamGraph: Updated checkpoint data for time 1431526320000 ms
15/05/13 10:12:01 INFO CheckpointWriter: Saving checkpoint for time 1431526320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000'
15/05/13 10:12:01 INFO CheckpointWriter: Checkpoint for time 1431526320000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000', took 7657 bytes and 53 ms
15/05/13 10:12:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526320000 ms
15/05/13 10:12:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526320000 ms
15/05/13 10:12:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:13:00 INFO FileInputDStream: Finding new files took 41 ms
15/05/13 10:13:00 INFO FileInputDStream: New files at time 1431526380000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526281823.json
15/05/13 10:13:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=16580921, maxMem=278302556
15/05/13 10:13:00 INFO MemoryStore: Block broadcast_189 stored as values in memory (estimated size 232.9 KB, free 249.4 MB)
15/05/13 10:13:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=16819453, maxMem=278302556
15/05/13 10:13:00 INFO MemoryStore: Block broadcast_189_piece0 stored as bytes in memory (estimated size 34.9 KB, free 249.3 MB)
15/05/13 10:13:00 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:13:00 INFO BlockManagerMaster: Updated info of block broadcast_189_piece0
15/05/13 10:13:00 INFO SparkContext: Created broadcast 189 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:13:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:13:00 INFO JobScheduler: Added jobs for time 1431526380000 ms
15/05/13 10:13:00 INFO JobGenerator: Checkpointing graph for time 1431526380000 ms
15/05/13 10:13:00 INFO JobScheduler: Starting job streaming job 1431526380000 ms.0 from job set of time 1431526380000 ms
15/05/13 10:13:00 INFO DStreamGraph: Updating checkpoint data for time 1431526380000 ms
15/05/13 10:13:00 INFO DStreamGraph: Updated checkpoint data for time 1431526380000 ms
15/05/13 10:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431526380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526380000'
15/05/13 10:13:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526380000
15/05/13 10:13:00 INFO CheckpointWriter: Checkpoint for time 1431526380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526380000', took 7665 bytes and 80 ms
15/05/13 10:13:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:13:00 INFO DAGScheduler: Got job 129 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:13:00 INFO DAGScheduler: Final stage: Stage 126(reduce at JsonRDD.scala:51)
15/05/13 10:13:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:13:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:13:00 INFO DAGScheduler: Submitting Stage 126 (MapPartitionsRDD[906] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:13:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=16855161, maxMem=278302556
15/05/13 10:13:00 INFO MemoryStore: Block broadcast_190 stored as values in memory (estimated size 5.9 KB, free 249.3 MB)
15/05/13 10:13:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=16861201, maxMem=278302556
15/05/13 10:13:00 INFO MemoryStore: Block broadcast_190_piece0 stored as bytes in memory (estimated size 4.1 KB, free 249.3 MB)
15/05/13 10:13:00 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:13:00 INFO BlockManagerMaster: Updated info of block broadcast_190_piece0
15/05/13 10:13:00 INFO SparkContext: Created broadcast 190 from broadcast at DAGScheduler.scala:839
15/05/13 10:13:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 126 (MapPartitionsRDD[906] at map at JsonRDD.scala:51)
15/05/13 10:13:00 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks
15/05/13 10:13:00 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 126, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:13:00 INFO BlockManagerInfo: Added broadcast_190_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:13:00 INFO BlockManagerInfo: Added broadcast_189_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:13:00 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 126) in 382 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:13:00 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool 
15/05/13 10:13:00 INFO DAGScheduler: Stage 126 (reduce at JsonRDD.scala:51) finished in 0.397 s
15/05/13 10:13:00 INFO DAGScheduler: Job 129 finished: reduce at JsonRDD.scala:51, took 0.430367 s
15/05/13 10:13:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:13:00 INFO DAGScheduler: Got job 130 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:13:00 INFO DAGScheduler: Final stage: Stage 127(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:13:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:13:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:13:00 INFO DAGScheduler: Submitting Stage 127 (MapPartitionsRDD[913] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:13:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=16865427, maxMem=278302556
15/05/13 10:13:00 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 20.4 KB, free 249.3 MB)
15/05/13 10:13:01 INFO MemoryStore: ensureFreeSpace(10842) called with curMem=16886347, maxMem=278302556
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 10.6 KB, free 249.3 MB)
15/05/13 10:13:01 INFO BlockManager: Removing broadcast 187
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_187
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_187 of size 6040 dropped from memory (free 261411407)
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_187_piece0
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_187_piece0 of size 4230 dropped from memory (free 261415637)
15/05/13 10:13:01 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_187_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_187_piece0
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_191_piece0
15/05/13 10:13:01 INFO SparkContext: Created broadcast 191 from broadcast at DAGScheduler.scala:839
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_187_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:13:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 127 (MapPartitionsRDD[913] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:13:01 INFO TaskSchedulerImpl: Adding task set 127.0 with 1 tasks
15/05/13 10:13:01 INFO TaskSetManager: Starting task 0.0 in stage 127.0 (TID 127, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:13:01 INFO ContextCleaner: Cleaned broadcast 187
15/05/13 10:13:01 INFO BlockManager: Removing broadcast 185
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_185_piece0
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_185_piece0 of size 10902 dropped from memory (free 261426539)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_185_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_185_piece0
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_185
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_185 of size 20920 dropped from memory (free 261447459)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_185_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 263.8 MB)
15/05/13 10:13:01 INFO ContextCleaner: Cleaned broadcast 185
15/05/13 10:13:01 INFO BlockManager: Removing broadcast 184
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_184_piece0
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_184_piece0 of size 4226 dropped from memory (free 261451685)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_184_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_184_piece0
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_184
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_184 of size 6040 dropped from memory (free 261457725)
15/05/13 10:13:01 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_184_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:13:01 INFO ContextCleaner: Cleaned broadcast 184
15/05/13 10:13:01 INFO BlockManager: Removing broadcast 190
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_190
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_190 of size 6040 dropped from memory (free 261463765)
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_190_piece0
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_190_piece0 of size 4226 dropped from memory (free 261467991)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_190_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_190_piece0
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_190_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:13:01 INFO ContextCleaner: Cleaned broadcast 190
15/05/13 10:13:01 INFO BlockManager: Removing broadcast 188
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_188_piece0
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_188_piece0 of size 11220 dropped from memory (free 261479211)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_188_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 263.3 MB)
15/05/13 10:13:01 INFO BlockManagerMaster: Updated info of block broadcast_188_piece0
15/05/13 10:13:01 INFO BlockManager: Removing block broadcast_188
15/05/13 10:13:01 INFO MemoryStore: Block broadcast_188 of size 21424 dropped from memory (free 261500635)
15/05/13 10:13:01 INFO BlockManagerInfo: Removed broadcast_188_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 11.0 KB, free: 264.7 MB)
15/05/13 10:13:01 INFO ContextCleaner: Cleaned broadcast 188
15/05/13 10:13:01 INFO TaskSetManager: Finished task 0.0 in stage 127.0 (TID 127) in 133 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:13:01 INFO TaskSchedulerImpl: Removed TaskSet 127.0, whose tasks have all completed, from pool 
15/05/13 10:13:01 INFO DAGScheduler: Stage 127 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.134 s
15/05/13 10:13:01 INFO DAGScheduler: Job 130 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.253036 s
15/05/13 10:13:01 INFO JobScheduler: Finished job streaming job 1431526380000 ms.0 from job set of time 1431526380000 ms
15/05/13 10:13:01 INFO JobScheduler: Total delay: 1.256 s for time 1431526380000 ms (execution: 1.128 s)
15/05/13 10:13:01 INFO MapPartitionsRDD: Removing RDD 888 from persistence list
15/05/13 10:13:01 INFO BlockManager: Removing RDD 888
15/05/13 10:13:01 INFO UnionRDD: Removing RDD 887 from persistence list
15/05/13 10:13:01 INFO BlockManager: Removing RDD 887
15/05/13 10:13:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526320000 ms: 1431526260000 ms
15/05/13 10:13:01 INFO JobGenerator: Checkpointing graph for time 1431526380000 ms
15/05/13 10:13:01 INFO DStreamGraph: Updating checkpoint data for time 1431526380000 ms
15/05/13 10:13:01 INFO DStreamGraph: Updated checkpoint data for time 1431526380000 ms
15/05/13 10:13:01 INFO CheckpointWriter: Saving checkpoint for time 1431526380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526380000'
15/05/13 10:13:01 INFO CheckpointWriter: Checkpoint for time 1431526380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526380000', took 7656 bytes and 58 ms
15/05/13 10:13:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526380000 ms
15/05/13 10:13:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526380000 ms
15/05/13 10:13:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:14:00 INFO FileInputDStream: Finding new files took 27 ms
15/05/13 10:14:00 INFO FileInputDStream: New files at time 1431526440000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526342320.json
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=16801921, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 232.9 KB, free 249.2 MB)
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17040453, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 34.9 KB, free 249.1 MB)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:14:00 INFO BlockManagerMaster: Updated info of block broadcast_192_piece0
15/05/13 10:14:00 INFO SparkContext: Created broadcast 192 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:14:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:14:00 INFO JobScheduler: Added jobs for time 1431526440000 ms
15/05/13 10:14:00 INFO JobGenerator: Checkpointing graph for time 1431526440000 ms
15/05/13 10:14:00 INFO DStreamGraph: Updating checkpoint data for time 1431526440000 ms
15/05/13 10:14:00 INFO DStreamGraph: Updated checkpoint data for time 1431526440000 ms
15/05/13 10:14:00 INFO JobScheduler: Starting job streaming job 1431526440000 ms.0 from job set of time 1431526440000 ms
15/05/13 10:14:00 INFO CheckpointWriter: Saving checkpoint for time 1431526440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000'
15/05/13 10:14:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526140000.bk
15/05/13 10:14:00 INFO CheckpointWriter: Checkpoint for time 1431526440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000', took 7666 bytes and 85 ms
15/05/13 10:14:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:14:00 INFO DAGScheduler: Got job 131 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:14:00 INFO DAGScheduler: Final stage: Stage 128(reduce at JsonRDD.scala:51)
15/05/13 10:14:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:14:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:14:00 INFO DAGScheduler: Submitting Stage 128 (MapPartitionsRDD[920] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17076161, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 5.9 KB, free 249.1 MB)
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=17082201, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 4.1 KB, free 249.1 MB)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:14:00 INFO BlockManagerMaster: Updated info of block broadcast_193_piece0
15/05/13 10:14:00 INFO SparkContext: Created broadcast 193 from broadcast at DAGScheduler.scala:839
15/05/13 10:14:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 128 (MapPartitionsRDD[920] at map at JsonRDD.scala:51)
15/05/13 10:14:00 INFO TaskSchedulerImpl: Adding task set 128.0 with 1 tasks
15/05/13 10:14:00 INFO TaskSetManager: Starting task 0.0 in stage 128.0 (TID 128, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:14:00 INFO TaskSetManager: Finished task 0.0 in stage 128.0 (TID 128) in 324 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:14:00 INFO DAGScheduler: Stage 128 (reduce at JsonRDD.scala:51) finished in 0.345 s
15/05/13 10:14:00 INFO TaskSchedulerImpl: Removed TaskSet 128.0, whose tasks have all completed, from pool 
15/05/13 10:14:00 INFO DAGScheduler: Job 131 finished: reduce at JsonRDD.scala:51, took 0.383362 s
15/05/13 10:14:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:14:00 INFO DAGScheduler: Got job 132 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:14:00 INFO DAGScheduler: Final stage: Stage 129(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:14:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:14:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:14:00 INFO DAGScheduler: Submitting Stage 129 (MapPartitionsRDD[927] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=17086427, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 20.0 KB, free 249.1 MB)
15/05/13 10:14:00 INFO MemoryStore: ensureFreeSpace(10758) called with curMem=17106915, maxMem=278302556
15/05/13 10:14:00 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 10.5 KB, free 249.1 MB)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.3 MB)
15/05/13 10:14:00 INFO BlockManagerMaster: Updated info of block broadcast_194_piece0
15/05/13 10:14:00 INFO SparkContext: Created broadcast 194 from broadcast at DAGScheduler.scala:839
15/05/13 10:14:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 129 (MapPartitionsRDD[927] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:14:00 INFO TaskSchedulerImpl: Adding task set 129.0 with 1 tasks
15/05/13 10:14:00 INFO TaskSetManager: Starting task 0.0 in stage 129.0 (TID 129, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:14:00 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.5 KB, free: 264.6 MB)
15/05/13 10:14:01 INFO TaskSetManager: Finished task 0.0 in stage 129.0 (TID 129) in 97 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:14:01 INFO TaskSchedulerImpl: Removed TaskSet 129.0, whose tasks have all completed, from pool 
15/05/13 10:14:01 INFO DAGScheduler: Stage 129 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.101 s
15/05/13 10:14:01 INFO DAGScheduler: Job 132 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.144163 s
15/05/13 10:14:01 INFO JobScheduler: Finished job streaming job 1431526440000 ms.0 from job set of time 1431526440000 ms
15/05/13 10:14:01 INFO JobScheduler: Total delay: 1.106 s for time 1431526440000 ms (execution: 0.986 s)
15/05/13 10:14:01 INFO MapPartitionsRDD: Removing RDD 902 from persistence list
15/05/13 10:14:01 INFO BlockManager: Removing RDD 902
15/05/13 10:14:01 INFO UnionRDD: Removing RDD 901 from persistence list
15/05/13 10:14:01 INFO BlockManager: Removing RDD 901
15/05/13 10:14:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526380000 ms: 1431526320000 ms
15/05/13 10:14:01 INFO JobGenerator: Checkpointing graph for time 1431526440000 ms
15/05/13 10:14:01 INFO DStreamGraph: Updating checkpoint data for time 1431526440000 ms
15/05/13 10:14:01 INFO DStreamGraph: Updated checkpoint data for time 1431526440000 ms
15/05/13 10:14:01 INFO CheckpointWriter: Saving checkpoint for time 1431526440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000'
15/05/13 10:14:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83319): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:14:01 INFO CheckpointWriter: Saving checkpoint for time 1431526440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000'
15/05/13 10:14:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83321): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:14:01 INFO CheckpointWriter: Saving checkpoint for time 1431526440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000'
15/05/13 10:14:01 INFO CheckpointWriter: Checkpoint for time 1431526440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000', took 7655 bytes and 186 ms
15/05/13 10:14:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526440000 ms
15/05/13 10:14:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526440000 ms
15/05/13 10:14:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:15:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 10:15:00 INFO FileInputDStream: New files at time 1431526500000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526402525.json
15/05/13 10:15:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17117673, maxMem=278302556
15/05/13 10:15:00 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 232.9 KB, free 248.9 MB)
15/05/13 10:15:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17356205, maxMem=278302556
15/05/13 10:15:00 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.8 MB)
15/05/13 10:15:00 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:15:00 INFO BlockManagerMaster: Updated info of block broadcast_195_piece0
15/05/13 10:15:00 INFO SparkContext: Created broadcast 195 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:15:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:15:00 INFO JobScheduler: Added jobs for time 1431526500000 ms
15/05/13 10:15:00 INFO JobGenerator: Checkpointing graph for time 1431526500000 ms
15/05/13 10:15:00 INFO DStreamGraph: Updating checkpoint data for time 1431526500000 ms
15/05/13 10:15:00 INFO JobScheduler: Starting job streaming job 1431526500000 ms.0 from job set of time 1431526500000 ms
15/05/13 10:15:00 INFO DStreamGraph: Updated checkpoint data for time 1431526500000 ms
15/05/13 10:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431526500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000'
15/05/13 10:15:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83326): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:15:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83326): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431526500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000'
15/05/13 10:15:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83328): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431526500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000'
15/05/13 10:15:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:15:00 INFO DAGScheduler: Got job 133 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:15:00 INFO DAGScheduler: Final stage: Stage 130(reduce at JsonRDD.scala:51)
15/05/13 10:15:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:15:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:15:00 INFO DAGScheduler: Submitting Stage 130 (MapPartitionsRDD[934] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:15:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17391913, maxMem=278302556
15/05/13 10:15:00 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 5.9 KB, free 248.8 MB)
15/05/13 10:15:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83330): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:15:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83330): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:15:00 WARN CheckpointWriter: Could not write checkpoint for time 1431526500000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000'
15/05/13 10:15:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=17397953, maxMem=278302556
15/05/13 10:15:00 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.8 MB)
15/05/13 10:15:00 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:15:00 INFO BlockManagerMaster: Updated info of block broadcast_196_piece0
15/05/13 10:15:00 INFO SparkContext: Created broadcast 196 from broadcast at DAGScheduler.scala:839
15/05/13 10:15:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 130 (MapPartitionsRDD[934] at map at JsonRDD.scala:51)
15/05/13 10:15:00 INFO TaskSchedulerImpl: Adding task set 130.0 with 1 tasks
15/05/13 10:15:00 INFO TaskSetManager: Starting task 0.0 in stage 130.0 (TID 130, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:15:00 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:15:00 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:15:00 INFO TaskSetManager: Finished task 0.0 in stage 130.0 (TID 130) in 474 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:15:00 INFO TaskSchedulerImpl: Removed TaskSet 130.0, whose tasks have all completed, from pool 
15/05/13 10:15:00 INFO DAGScheduler: Stage 130 (reduce at JsonRDD.scala:51) finished in 0.503 s
15/05/13 10:15:00 INFO DAGScheduler: Job 133 finished: reduce at JsonRDD.scala:51, took 0.547392 s
15/05/13 10:15:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:15:01 INFO DAGScheduler: Got job 134 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:15:01 INFO DAGScheduler: Final stage: Stage 131(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:15:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:15:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:15:01 INFO DAGScheduler: Submitting Stage 131 (MapPartitionsRDD[941] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:15:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=17402180, maxMem=278302556
15/05/13 10:15:01 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 20.4 KB, free 248.8 MB)
15/05/13 10:15:01 INFO MemoryStore: ensureFreeSpace(10901) called with curMem=17423100, maxMem=278302556
15/05/13 10:15:01 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 10.6 KB, free 248.8 MB)
15/05/13 10:15:01 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.2 MB)
15/05/13 10:15:01 INFO BlockManagerMaster: Updated info of block broadcast_197_piece0
15/05/13 10:15:01 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:839
15/05/13 10:15:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 131 (MapPartitionsRDD[941] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:15:01 INFO TaskSchedulerImpl: Adding task set 131.0 with 1 tasks
15/05/13 10:15:01 INFO TaskSetManager: Starting task 0.0 in stage 131.0 (TID 131, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:15:01 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.6 MB)
15/05/13 10:15:01 INFO TaskSetManager: Finished task 0.0 in stage 131.0 (TID 131) in 184 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:15:01 INFO TaskSchedulerImpl: Removed TaskSet 131.0, whose tasks have all completed, from pool 
15/05/13 10:15:01 INFO DAGScheduler: Stage 131 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.186 s
15/05/13 10:15:01 INFO DAGScheduler: Job 134 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.226391 s
15/05/13 10:15:01 INFO JobScheduler: Finished job streaming job 1431526500000 ms.0 from job set of time 1431526500000 ms
15/05/13 10:15:01 INFO JobScheduler: Total delay: 1.447 s for time 1431526500000 ms (execution: 1.220 s)
15/05/13 10:15:01 INFO MapPartitionsRDD: Removing RDD 916 from persistence list
15/05/13 10:15:01 INFO BlockManager: Removing RDD 916
15/05/13 10:15:01 INFO UnionRDD: Removing RDD 915 from persistence list
15/05/13 10:15:01 INFO BlockManager: Removing RDD 915
15/05/13 10:15:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526440000 ms: 1431526380000 ms
15/05/13 10:15:01 INFO JobGenerator: Checkpointing graph for time 1431526500000 ms
15/05/13 10:15:01 INFO DStreamGraph: Updating checkpoint data for time 1431526500000 ms
15/05/13 10:15:01 INFO DStreamGraph: Updated checkpoint data for time 1431526500000 ms
15/05/13 10:15:01 INFO CheckpointWriter: Saving checkpoint for time 1431526500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000'
15/05/13 10:15:01 INFO CheckpointWriter: Checkpoint for time 1431526500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000', took 7656 bytes and 70 ms
15/05/13 10:15:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526500000 ms
15/05/13 10:15:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526500000 ms
15/05/13 10:15:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:16:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 10:16:00 INFO FileInputDStream: New files at time 1431526560000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526463168.json
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17434001, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 232.9 KB, free 248.6 MB)
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17672533, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.5 MB)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:16:00 INFO BlockManagerMaster: Updated info of block broadcast_198_piece0
15/05/13 10:16:00 INFO SparkContext: Created broadcast 198 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:16:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:16:00 INFO JobScheduler: Added jobs for time 1431526560000 ms
15/05/13 10:16:00 INFO JobGenerator: Checkpointing graph for time 1431526560000 ms
15/05/13 10:16:00 INFO JobScheduler: Starting job streaming job 1431526560000 ms.0 from job set of time 1431526560000 ms
15/05/13 10:16:00 INFO DStreamGraph: Updating checkpoint data for time 1431526560000 ms
15/05/13 10:16:00 INFO DStreamGraph: Updated checkpoint data for time 1431526560000 ms
15/05/13 10:16:00 INFO CheckpointWriter: Saving checkpoint for time 1431526560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526560000'
15/05/13 10:16:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:16:00 INFO DAGScheduler: Got job 135 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:16:00 INFO DAGScheduler: Final stage: Stage 132(reduce at JsonRDD.scala:51)
15/05/13 10:16:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:16:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526260000.bk
15/05/13 10:16:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:16:00 INFO DAGScheduler: Submitting Stage 132 (MapPartitionsRDD[948] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:16:00 INFO CheckpointWriter: Checkpoint for time 1431526560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526560000', took 7666 bytes and 66 ms
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17708241, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 5.9 KB, free 248.5 MB)
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=17714281, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.5 MB)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:16:00 INFO BlockManagerMaster: Updated info of block broadcast_199_piece0
15/05/13 10:16:00 INFO SparkContext: Created broadcast 199 from broadcast at DAGScheduler.scala:839
15/05/13 10:16:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 132 (MapPartitionsRDD[948] at map at JsonRDD.scala:51)
15/05/13 10:16:00 INFO TaskSchedulerImpl: Adding task set 132.0 with 1 tasks
15/05/13 10:16:00 INFO TaskSetManager: Starting task 0.0 in stage 132.0 (TID 132, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.8 MB)
15/05/13 10:16:00 INFO TaskSetManager: Finished task 0.0 in stage 132.0 (TID 132) in 386 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:16:00 INFO TaskSchedulerImpl: Removed TaskSet 132.0, whose tasks have all completed, from pool 
15/05/13 10:16:00 INFO DAGScheduler: Stage 132 (reduce at JsonRDD.scala:51) finished in 0.400 s
15/05/13 10:16:00 INFO DAGScheduler: Job 135 finished: reduce at JsonRDD.scala:51, took 0.442889 s
15/05/13 10:16:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:16:00 INFO DAGScheduler: Got job 136 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:16:00 INFO DAGScheduler: Final stage: Stage 133(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:16:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:16:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:16:00 INFO DAGScheduler: Submitting Stage 133 (MapPartitionsRDD[955] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=17718507, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 20.0 KB, free 248.5 MB)
15/05/13 10:16:00 INFO MemoryStore: ensureFreeSpace(10762) called with curMem=17738995, maxMem=278302556
15/05/13 10:16:00 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 10.5 KB, free 248.5 MB)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.2 MB)
15/05/13 10:16:00 INFO BlockManagerMaster: Updated info of block broadcast_200_piece0
15/05/13 10:16:00 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:839
15/05/13 10:16:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 133 (MapPartitionsRDD[955] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:16:00 INFO TaskSchedulerImpl: Adding task set 133.0 with 1 tasks
15/05/13 10:16:00 INFO TaskSetManager: Starting task 0.0 in stage 133.0 (TID 133, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:16:00 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 263.8 MB)
15/05/13 10:16:00 INFO TaskSetManager: Finished task 0.0 in stage 133.0 (TID 133) in 116 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:16:00 INFO TaskSchedulerImpl: Removed TaskSet 133.0, whose tasks have all completed, from pool 
15/05/13 10:16:00 INFO DAGScheduler: Stage 133 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.118 s
15/05/13 10:16:00 INFO DAGScheduler: Job 136 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.140239 s
15/05/13 10:16:01 INFO JobScheduler: Finished job streaming job 1431526560000 ms.0 from job set of time 1431526560000 ms
15/05/13 10:16:01 INFO JobScheduler: Total delay: 1.069 s for time 1431526560000 ms (execution: 0.924 s)
15/05/13 10:16:01 INFO MapPartitionsRDD: Removing RDD 930 from persistence list
15/05/13 10:16:01 INFO BlockManager: Removing RDD 930
15/05/13 10:16:01 INFO UnionRDD: Removing RDD 929 from persistence list
15/05/13 10:16:01 INFO BlockManager: Removing RDD 929
15/05/13 10:16:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526500000 ms: 1431526440000 ms
15/05/13 10:16:01 INFO JobGenerator: Checkpointing graph for time 1431526560000 ms
15/05/13 10:16:01 INFO DStreamGraph: Updating checkpoint data for time 1431526560000 ms
15/05/13 10:16:01 INFO DStreamGraph: Updated checkpoint data for time 1431526560000 ms
15/05/13 10:16:01 INFO CheckpointWriter: Saving checkpoint for time 1431526560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526560000'
15/05/13 10:16:01 INFO CheckpointWriter: Checkpoint for time 1431526560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526560000', took 7656 bytes and 74 ms
15/05/13 10:16:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526560000 ms
15/05/13 10:16:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526560000 ms
15/05/13 10:16:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 191
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_191
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_191 of size 20920 dropped from memory (free 260573719)
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_191_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_191_piece0 of size 10842 dropped from memory (free 260584561)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_191_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.2 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_191_piece0
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_191_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.6 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 191
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 200
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_200
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_200 of size 20488 dropped from memory (free 260605049)
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_200_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_200_piece0 of size 10762 dropped from memory (free 260615811)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_200_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.2 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_200_piece0
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_200_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 263.8 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 200
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 195
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_195
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_195 of size 238532 dropped from memory (free 260854343)
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_195_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_195_piece0 of size 35708 dropped from memory (free 260890051)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_195_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_195_piece0
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_195_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 195
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 194
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_194_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_194_piece0 of size 10758 dropped from memory (free 260900809)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_194_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.2 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_194_piece0
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_194
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_194 of size 20488 dropped from memory (free 260921297)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_194_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.5 KB, free: 264.6 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 194
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 193
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_193
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_193 of size 6040 dropped from memory (free 260927337)
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_193_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_193_piece0 of size 4226 dropped from memory (free 260931563)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_193_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_193_piece0
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_193_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 193
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 192
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_192_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_192_piece0 of size 35708 dropped from memory (free 260967271)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_192_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_192_piece0
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_192
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_192 of size 238532 dropped from memory (free 261205803)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_192_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 264.7 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 192
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 199
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_199_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_199_piece0 of size 4226 dropped from memory (free 261210029)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_199_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_199_piece0
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_199
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_199 of size 6040 dropped from memory (free 261216069)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_199_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 199
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 197
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_197_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_197_piece0 of size 10901 dropped from memory (free 261226970)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_197_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.3 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_197_piece0
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_197
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_197 of size 20920 dropped from memory (free 261247890)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_197_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.7 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 197
15/05/13 10:16:14 INFO BlockManager: Removing broadcast 196
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_196_piece0
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_196_piece0 of size 4227 dropped from memory (free 261252117)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_196_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:16:14 INFO BlockManagerMaster: Updated info of block broadcast_196_piece0
15/05/13 10:16:14 INFO BlockManager: Removing block broadcast_196
15/05/13 10:16:14 INFO MemoryStore: Block broadcast_196 of size 6040 dropped from memory (free 261258157)
15/05/13 10:16:14 INFO BlockManagerInfo: Removed broadcast_196_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:16:14 INFO ContextCleaner: Cleaned broadcast 196
15/05/13 10:17:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 10:17:00 INFO FileInputDStream: New files at time 1431526620000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526525874.json
15/05/13 10:17:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17044399, maxMem=278302556
15/05/13 10:17:00 INFO MemoryStore: Block broadcast_201 stored as values in memory (estimated size 232.9 KB, free 248.9 MB)
15/05/13 10:17:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17282931, maxMem=278302556
15/05/13 10:17:00 INFO MemoryStore: Block broadcast_201_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.9 MB)
15/05/13 10:17:00 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:17:00 INFO BlockManagerMaster: Updated info of block broadcast_201_piece0
15/05/13 10:17:00 INFO SparkContext: Created broadcast 201 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:17:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:17:00 INFO JobScheduler: Added jobs for time 1431526620000 ms
15/05/13 10:17:00 INFO JobGenerator: Checkpointing graph for time 1431526620000 ms
15/05/13 10:17:00 INFO JobScheduler: Starting job streaming job 1431526620000 ms.0 from job set of time 1431526620000 ms
15/05/13 10:17:00 INFO DStreamGraph: Updating checkpoint data for time 1431526620000 ms
15/05/13 10:17:00 INFO DStreamGraph: Updated checkpoint data for time 1431526620000 ms
15/05/13 10:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431526620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000'
15/05/13 10:17:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83348): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:17:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83348): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431526620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000'
15/05/13 10:17:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83350): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:17:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431526620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000'
15/05/13 10:17:00 INFO DAGScheduler: Got job 137 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:17:00 INFO DAGScheduler: Final stage: Stage 134(reduce at JsonRDD.scala:51)
15/05/13 10:17:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:17:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:17:00 INFO DAGScheduler: Submitting Stage 134 (MapPartitionsRDD[962] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:17:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17318639, maxMem=278302556
15/05/13 10:17:00 INFO MemoryStore: Block broadcast_202 stored as values in memory (estimated size 5.9 KB, free 248.9 MB)
15/05/13 10:17:00 INFO MemoryStore: ensureFreeSpace(4229) called with curMem=17324679, maxMem=278302556
15/05/13 10:17:00 INFO MemoryStore: Block broadcast_202_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.9 MB)
15/05/13 10:17:00 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:17:00 INFO BlockManagerMaster: Updated info of block broadcast_202_piece0
15/05/13 10:17:00 INFO SparkContext: Created broadcast 202 from broadcast at DAGScheduler.scala:839
15/05/13 10:17:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 134 (MapPartitionsRDD[962] at map at JsonRDD.scala:51)
15/05/13 10:17:00 INFO TaskSchedulerImpl: Adding task set 134.0 with 1 tasks
15/05/13 10:17:00 INFO TaskSetManager: Starting task 0.0 in stage 134.0 (TID 134, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:17:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526320000.bk
15/05/13 10:17:00 INFO CheckpointWriter: Checkpoint for time 1431526620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000', took 7666 bytes and 195 ms
15/05/13 10:17:00 INFO BlockManagerInfo: Added broadcast_202_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.8 MB)
15/05/13 10:17:00 INFO BlockManagerInfo: Added broadcast_201_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.7 MB)
15/05/13 10:17:00 INFO TaskSetManager: Finished task 0.0 in stage 134.0 (TID 134) in 460 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:17:00 INFO DAGScheduler: Stage 134 (reduce at JsonRDD.scala:51) finished in 0.479 s
15/05/13 10:17:00 INFO TaskSchedulerImpl: Removed TaskSet 134.0, whose tasks have all completed, from pool 
15/05/13 10:17:00 INFO DAGScheduler: Job 137 finished: reduce at JsonRDD.scala:51, took 0.519480 s
15/05/13 10:17:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:17:01 INFO DAGScheduler: Got job 138 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:17:01 INFO DAGScheduler: Final stage: Stage 135(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:17:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:17:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:17:01 INFO DAGScheduler: Submitting Stage 135 (MapPartitionsRDD[969] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:17:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=17328908, maxMem=278302556
15/05/13 10:17:01 INFO MemoryStore: Block broadcast_203 stored as values in memory (estimated size 20.6 KB, free 248.9 MB)
15/05/13 10:17:01 INFO MemoryStore: ensureFreeSpace(10946) called with curMem=17350004, maxMem=278302556
15/05/13 10:17:01 INFO MemoryStore: Block broadcast_203_piece0 stored as bytes in memory (estimated size 10.7 KB, free 248.9 MB)
15/05/13 10:17:01 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.2 MB)
15/05/13 10:17:01 INFO BlockManagerMaster: Updated info of block broadcast_203_piece0
15/05/13 10:17:01 INFO SparkContext: Created broadcast 203 from broadcast at DAGScheduler.scala:839
15/05/13 10:17:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 135 (MapPartitionsRDD[969] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:17:01 INFO TaskSchedulerImpl: Adding task set 135.0 with 1 tasks
15/05/13 10:17:01 INFO TaskSetManager: Starting task 0.0 in stage 135.0 (TID 135, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:17:01 INFO BlockManagerInfo: Added broadcast_203_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.7 MB)
15/05/13 10:17:01 INFO TaskSetManager: Finished task 0.0 in stage 135.0 (TID 135) in 133 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:17:01 INFO TaskSchedulerImpl: Removed TaskSet 135.0, whose tasks have all completed, from pool 
15/05/13 10:17:01 INFO DAGScheduler: Stage 135 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.133 s
15/05/13 10:17:01 INFO DAGScheduler: Job 138 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.168049 s
15/05/13 10:17:01 INFO JobScheduler: Finished job streaming job 1431526620000 ms.0 from job set of time 1431526620000 ms
15/05/13 10:17:01 INFO JobScheduler: Total delay: 1.376 s for time 1431526620000 ms (execution: 1.171 s)
15/05/13 10:17:01 INFO MapPartitionsRDD: Removing RDD 944 from persistence list
15/05/13 10:17:01 INFO BlockManager: Removing RDD 944
15/05/13 10:17:01 INFO UnionRDD: Removing RDD 943 from persistence list
15/05/13 10:17:01 INFO BlockManager: Removing RDD 943
15/05/13 10:17:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526560000 ms: 1431526500000 ms
15/05/13 10:17:01 INFO JobGenerator: Checkpointing graph for time 1431526620000 ms
15/05/13 10:17:01 INFO DStreamGraph: Updating checkpoint data for time 1431526620000 ms
15/05/13 10:17:01 INFO DStreamGraph: Updated checkpoint data for time 1431526620000 ms
15/05/13 10:17:01 INFO CheckpointWriter: Saving checkpoint for time 1431526620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000'
15/05/13 10:17:01 INFO CheckpointWriter: Checkpoint for time 1431526620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526620000', took 7655 bytes and 73 ms
15/05/13 10:17:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526620000 ms
15/05/13 10:17:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526620000 ms
15/05/13 10:17:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:18:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 10:18:00 INFO FileInputDStream: New files at time 1431526680000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526588523.json
15/05/13 10:18:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17360950, maxMem=278302556
15/05/13 10:18:00 INFO MemoryStore: Block broadcast_204 stored as values in memory (estimated size 232.9 KB, free 248.6 MB)
15/05/13 10:18:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17599482, maxMem=278302556
15/05/13 10:18:00 INFO MemoryStore: Block broadcast_204_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.6 MB)
15/05/13 10:18:00 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:18:00 INFO BlockManagerMaster: Updated info of block broadcast_204_piece0
15/05/13 10:18:00 INFO SparkContext: Created broadcast 204 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:18:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:18:00 INFO JobScheduler: Added jobs for time 1431526680000 ms
15/05/13 10:18:00 INFO JobGenerator: Checkpointing graph for time 1431526680000 ms
15/05/13 10:18:00 INFO DStreamGraph: Updating checkpoint data for time 1431526680000 ms
15/05/13 10:18:00 INFO JobScheduler: Starting job streaming job 1431526680000 ms.0 from job set of time 1431526680000 ms
15/05/13 10:18:00 INFO DStreamGraph: Updated checkpoint data for time 1431526680000 ms
15/05/13 10:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431526680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000'
15/05/13 10:18:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83357): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:18:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83357): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431526680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000'
15/05/13 10:18:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:18:00 INFO DAGScheduler: Got job 139 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:18:00 INFO DAGScheduler: Final stage: Stage 136(reduce at JsonRDD.scala:51)
15/05/13 10:18:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:18:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:18:00 INFO DAGScheduler: Submitting Stage 136 (MapPartitionsRDD[976] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:18:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17635190, maxMem=278302556
15/05/13 10:18:00 INFO MemoryStore: Block broadcast_205 stored as values in memory (estimated size 5.9 KB, free 248.6 MB)
15/05/13 10:18:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=17641230, maxMem=278302556
15/05/13 10:18:00 INFO MemoryStore: Block broadcast_205_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.6 MB)
15/05/13 10:18:00 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:18:00 INFO BlockManagerMaster: Updated info of block broadcast_205_piece0
15/05/13 10:18:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83359): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431526680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000'
15/05/13 10:18:00 INFO SparkContext: Created broadcast 205 from broadcast at DAGScheduler.scala:839
15/05/13 10:18:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 136 (MapPartitionsRDD[976] at map at JsonRDD.scala:51)
15/05/13 10:18:00 INFO TaskSchedulerImpl: Adding task set 136.0 with 1 tasks
15/05/13 10:18:00 INFO TaskSetManager: Starting task 0.0 in stage 136.0 (TID 136, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:18:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83361): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:18:00 WARN CheckpointWriter: Could not write checkpoint for time 1431526680000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000'
15/05/13 10:18:00 INFO BlockManagerInfo: Added broadcast_205_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:18:00 INFO BlockManagerInfo: Added broadcast_204_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.7 MB)
15/05/13 10:18:00 INFO TaskSetManager: Finished task 0.0 in stage 136.0 (TID 136) in 403 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:18:00 INFO TaskSchedulerImpl: Removed TaskSet 136.0, whose tasks have all completed, from pool 
15/05/13 10:18:00 INFO DAGScheduler: Stage 136 (reduce at JsonRDD.scala:51) finished in 0.415 s
15/05/13 10:18:00 INFO DAGScheduler: Job 139 finished: reduce at JsonRDD.scala:51, took 0.453300 s
15/05/13 10:18:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:18:01 INFO DAGScheduler: Got job 140 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:18:01 INFO DAGScheduler: Final stage: Stage 137(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:18:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:18:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:18:01 INFO DAGScheduler: Submitting Stage 137 (MapPartitionsRDD[983] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:18:01 INFO MemoryStore: ensureFreeSpace(19208) called with curMem=17645456, maxMem=278302556
15/05/13 10:18:01 INFO MemoryStore: Block broadcast_206 stored as values in memory (estimated size 18.8 KB, free 248.6 MB)
15/05/13 10:18:01 INFO MemoryStore: ensureFreeSpace(10430) called with curMem=17664664, maxMem=278302556
15/05/13 10:18:01 INFO MemoryStore: Block broadcast_206_piece0 stored as bytes in memory (estimated size 10.2 KB, free 248.6 MB)
15/05/13 10:18:01 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.2 KB, free: 263.2 MB)
15/05/13 10:18:01 INFO BlockManagerMaster: Updated info of block broadcast_206_piece0
15/05/13 10:18:01 INFO SparkContext: Created broadcast 206 from broadcast at DAGScheduler.scala:839
15/05/13 10:18:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 137 (MapPartitionsRDD[983] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:18:01 INFO TaskSchedulerImpl: Adding task set 137.0 with 1 tasks
15/05/13 10:18:01 INFO TaskSetManager: Starting task 0.0 in stage 137.0 (TID 137, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:18:01 INFO BlockManagerInfo: Added broadcast_206_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.2 KB, free: 263.7 MB)
15/05/13 10:18:01 INFO TaskSetManager: Finished task 0.0 in stage 137.0 (TID 137) in 127 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:18:01 INFO TaskSchedulerImpl: Removed TaskSet 137.0, whose tasks have all completed, from pool 
15/05/13 10:18:01 INFO DAGScheduler: Stage 137 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.128 s
15/05/13 10:18:01 INFO DAGScheduler: Job 140 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.165796 s
15/05/13 10:18:01 INFO JobScheduler: Finished job streaming job 1431526680000 ms.0 from job set of time 1431526680000 ms
15/05/13 10:18:01 INFO JobScheduler: Total delay: 1.251 s for time 1431526680000 ms (execution: 1.048 s)
15/05/13 10:18:01 INFO MapPartitionsRDD: Removing RDD 958 from persistence list
15/05/13 10:18:01 INFO BlockManager: Removing RDD 958
15/05/13 10:18:01 INFO UnionRDD: Removing RDD 957 from persistence list
15/05/13 10:18:01 INFO BlockManager: Removing RDD 957
15/05/13 10:18:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526620000 ms: 1431526560000 ms
15/05/13 10:18:01 INFO JobGenerator: Checkpointing graph for time 1431526680000 ms
15/05/13 10:18:01 INFO DStreamGraph: Updating checkpoint data for time 1431526680000 ms
15/05/13 10:18:01 INFO DStreamGraph: Updated checkpoint data for time 1431526680000 ms
15/05/13 10:18:01 INFO CheckpointWriter: Saving checkpoint for time 1431526680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000'
15/05/13 10:18:01 INFO CheckpointWriter: Checkpoint for time 1431526680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000', took 7656 bytes and 67 ms
15/05/13 10:18:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526680000 ms
15/05/13 10:18:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526680000 ms
15/05/13 10:18:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:19:00 INFO FileInputDStream: Finding new files took 69 ms
15/05/13 10:19:00 INFO FileInputDStream: New files at time 1431526740000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526649738.json
15/05/13 10:19:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17675094, maxMem=278302556
15/05/13 10:19:00 INFO MemoryStore: Block broadcast_207 stored as values in memory (estimated size 232.9 KB, free 248.3 MB)
15/05/13 10:19:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=17913626, maxMem=278302556
15/05/13 10:19:00 INFO MemoryStore: Block broadcast_207_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.3 MB)
15/05/13 10:19:00 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:19:00 INFO BlockManagerMaster: Updated info of block broadcast_207_piece0
15/05/13 10:19:00 INFO SparkContext: Created broadcast 207 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:19:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:19:00 INFO JobScheduler: Added jobs for time 1431526740000 ms
15/05/13 10:19:00 INFO JobGenerator: Checkpointing graph for time 1431526740000 ms
15/05/13 10:19:00 INFO JobScheduler: Starting job streaming job 1431526740000 ms.0 from job set of time 1431526740000 ms
15/05/13 10:19:00 INFO DStreamGraph: Updating checkpoint data for time 1431526740000 ms
15/05/13 10:19:00 INFO DStreamGraph: Updated checkpoint data for time 1431526740000 ms
15/05/13 10:19:00 INFO CheckpointWriter: Saving checkpoint for time 1431526740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000'
15/05/13 10:19:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83367): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:19:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:19:00 INFO CheckpointWriter: Saving checkpoint for time 1431526740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000'
15/05/13 10:19:00 INFO DAGScheduler: Got job 141 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:19:00 INFO DAGScheduler: Final stage: Stage 138(reduce at JsonRDD.scala:51)
15/05/13 10:19:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:19:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:19:00 INFO DAGScheduler: Submitting Stage 138 (MapPartitionsRDD[990] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:19:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17949334, maxMem=278302556
15/05/13 10:19:00 INFO MemoryStore: Block broadcast_208 stored as values in memory (estimated size 5.9 KB, free 248.3 MB)
15/05/13 10:19:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=17955374, maxMem=278302556
15/05/13 10:19:00 INFO MemoryStore: Block broadcast_208_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.3 MB)
15/05/13 10:19:00 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:19:00 INFO BlockManagerMaster: Updated info of block broadcast_208_piece0
15/05/13 10:19:00 INFO SparkContext: Created broadcast 208 from broadcast at DAGScheduler.scala:839
15/05/13 10:19:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 138 (MapPartitionsRDD[990] at map at JsonRDD.scala:51)
15/05/13 10:19:00 INFO TaskSchedulerImpl: Adding task set 138.0 with 1 tasks
15/05/13 10:19:00 INFO TaskSetManager: Starting task 0.0 in stage 138.0 (TID 138, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:19:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526440000.bk
15/05/13 10:19:00 INFO CheckpointWriter: Checkpoint for time 1431526740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000', took 7665 bytes and 143 ms
15/05/13 10:19:00 INFO BlockManagerInfo: Added broadcast_208_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:19:00 INFO BlockManagerInfo: Added broadcast_207_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:19:00 INFO TaskSetManager: Finished task 0.0 in stage 138.0 (TID 138) in 389 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:19:00 INFO TaskSchedulerImpl: Removed TaskSet 138.0, whose tasks have all completed, from pool 
15/05/13 10:19:00 INFO DAGScheduler: Stage 138 (reduce at JsonRDD.scala:51) finished in 0.407 s
15/05/13 10:19:00 INFO DAGScheduler: Job 141 finished: reduce at JsonRDD.scala:51, took 0.446933 s
15/05/13 10:19:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:19:01 INFO DAGScheduler: Got job 142 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:19:01 INFO DAGScheduler: Final stage: Stage 139(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:19:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:19:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:19:01 INFO DAGScheduler: Submitting Stage 139 (MapPartitionsRDD[997] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:19:01 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=17959600, maxMem=278302556
15/05/13 10:19:01 INFO MemoryStore: Block broadcast_209 stored as values in memory (estimated size 21.0 KB, free 248.3 MB)
15/05/13 10:19:01 INFO MemoryStore: ensureFreeSpace(11228) called with curMem=17981104, maxMem=278302556
15/05/13 10:19:01 INFO MemoryStore: Block broadcast_209_piece0 stored as bytes in memory (estimated size 11.0 KB, free 248.3 MB)
15/05/13 10:19:01 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 263.1 MB)
15/05/13 10:19:01 INFO BlockManagerMaster: Updated info of block broadcast_209_piece0
15/05/13 10:19:01 INFO SparkContext: Created broadcast 209 from broadcast at DAGScheduler.scala:839
15/05/13 10:19:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 139 (MapPartitionsRDD[997] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:19:01 INFO TaskSchedulerImpl: Adding task set 139.0 with 1 tasks
15/05/13 10:19:01 INFO TaskSetManager: Starting task 0.0 in stage 139.0 (TID 139, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:19:01 INFO BlockManagerInfo: Added broadcast_209_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:19:01 INFO TaskSetManager: Finished task 0.0 in stage 139.0 (TID 139) in 143 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:19:01 INFO TaskSchedulerImpl: Removed TaskSet 139.0, whose tasks have all completed, from pool 
15/05/13 10:19:01 INFO DAGScheduler: Stage 139 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.145 s
15/05/13 10:19:01 INFO DAGScheduler: Job 142 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.182114 s
15/05/13 10:19:01 INFO JobScheduler: Finished job streaming job 1431526740000 ms.0 from job set of time 1431526740000 ms
15/05/13 10:19:01 INFO JobScheduler: Total delay: 1.219 s for time 1431526740000 ms (execution: 1.000 s)
15/05/13 10:19:01 INFO MapPartitionsRDD: Removing RDD 972 from persistence list
15/05/13 10:19:01 INFO BlockManager: Removing RDD 972
15/05/13 10:19:01 INFO UnionRDD: Removing RDD 971 from persistence list
15/05/13 10:19:01 INFO BlockManager: Removing RDD 971
15/05/13 10:19:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526680000 ms: 1431526620000 ms
15/05/13 10:19:01 INFO JobGenerator: Checkpointing graph for time 1431526740000 ms
15/05/13 10:19:01 INFO DStreamGraph: Updating checkpoint data for time 1431526740000 ms
15/05/13 10:19:01 INFO DStreamGraph: Updated checkpoint data for time 1431526740000 ms
15/05/13 10:19:01 INFO CheckpointWriter: Saving checkpoint for time 1431526740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000'
15/05/13 10:19:01 INFO CheckpointWriter: Checkpoint for time 1431526740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526740000', took 7657 bytes and 59 ms
15/05/13 10:19:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526740000 ms
15/05/13 10:19:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526740000 ms
15/05/13 10:19:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:20:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 10:20:00 INFO FileInputDStream: New files at time 1431526800000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526710369.json
15/05/13 10:20:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17992332, maxMem=278302556
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_210 stored as values in memory (estimated size 232.9 KB, free 248.0 MB)
15/05/13 10:20:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=18230864, maxMem=278302556
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_210_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.0 MB)
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 202
15/05/13 10:20:00 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_202_piece0
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_210_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_202_piece0 of size 4229 dropped from memory (free 260040213)
15/05/13 10:20:00 INFO SparkContext: Created broadcast 210 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_202_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_202_piece0
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_202
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_202 of size 6040 dropped from memory (free 260046253)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_202_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 202
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 201
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_201_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_201_piece0 of size 35708 dropped from memory (free 260081961)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_201_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_201_piece0
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_201
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_201 of size 238532 dropped from memory (free 260320493)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_201_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 201
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 205
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_205
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_205 of size 6040 dropped from memory (free 260326533)
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_205_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_205_piece0 of size 4226 dropped from memory (free 260330759)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_205_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_205_piece0
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_205_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 205
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 203
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_203_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_203_piece0 of size 10946 dropped from memory (free 260341705)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_203_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 263.2 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_203_piece0
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_203
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_203 of size 21096 dropped from memory (free 260362801)
15/05/13 10:20:00 INFO JobScheduler: Starting job streaming job 1431526800000 ms.0 from job set of time 1431526800000 ms
15/05/13 10:20:00 INFO JobScheduler: Added jobs for time 1431526800000 ms
15/05/13 10:20:00 INFO JobGenerator: Checkpointing graph for time 1431526800000 ms
15/05/13 10:20:00 INFO DStreamGraph: Updating checkpoint data for time 1431526800000 ms
15/05/13 10:20:00 INFO DStreamGraph: Updated checkpoint data for time 1431526800000 ms
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_203_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 203
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 209
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_209
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_209 of size 21504 dropped from memory (free 260384305)
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_209_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_209_piece0 of size 11228 dropped from memory (free 260395533)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_209_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 263.2 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_209_piece0
15/05/13 10:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431526800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000'
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_209_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 209
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 208
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_208_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_208_piece0 of size 4226 dropped from memory (free 260399759)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_208_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_208_piece0
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_208
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_208 of size 6040 dropped from memory (free 260405799)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_208_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 208
15/05/13 10:20:00 INFO BlockManager: Removing broadcast 206
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_206
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_206 of size 19208 dropped from memory (free 260425007)
15/05/13 10:20:00 INFO BlockManager: Removing block broadcast_206_piece0
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_206_piece0 of size 10430 dropped from memory (free 260435437)
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_206_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.2 KB, free: 263.2 MB)
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_206_piece0
15/05/13 10:20:00 INFO BlockManagerInfo: Removed broadcast_206_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.2 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO ContextCleaner: Cleaned broadcast 206
15/05/13 10:20:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83375): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431526800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000'
15/05/13 10:20:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:20:00 INFO DAGScheduler: Got job 143 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:20:00 INFO DAGScheduler: Final stage: Stage 140(reduce at JsonRDD.scala:51)
15/05/13 10:20:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:20:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:20:00 INFO DAGScheduler: Submitting Stage 140 (MapPartitionsRDD[1004] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:20:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=17867119, maxMem=278302556
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_211 stored as values in memory (estimated size 5.9 KB, free 248.4 MB)
15/05/13 10:20:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=17873159, maxMem=278302556
15/05/13 10:20:00 INFO MemoryStore: Block broadcast_211_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.4 MB)
15/05/13 10:20:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83377): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:20:00 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431526800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000'
15/05/13 10:20:00 INFO BlockManagerMaster: Updated info of block broadcast_211_piece0
15/05/13 10:20:00 INFO SparkContext: Created broadcast 211 from broadcast at DAGScheduler.scala:839
15/05/13 10:20:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 140 (MapPartitionsRDD[1004] at map at JsonRDD.scala:51)
15/05/13 10:20:00 INFO TaskSchedulerImpl: Adding task set 140.0 with 1 tasks
15/05/13 10:20:00 INFO TaskSetManager: Starting task 0.0 in stage 140.0 (TID 140, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:20:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526500000.bk
15/05/13 10:20:00 INFO CheckpointWriter: Checkpoint for time 1431526800000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000', took 7663 bytes and 230 ms
15/05/13 10:20:00 INFO BlockManagerInfo: Added broadcast_211_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO BlockManagerInfo: Added broadcast_210_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.7 MB)
15/05/13 10:20:00 INFO TaskSetManager: Finished task 0.0 in stage 140.0 (TID 140) in 488 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:20:00 INFO TaskSchedulerImpl: Removed TaskSet 140.0, whose tasks have all completed, from pool 
15/05/13 10:20:00 INFO DAGScheduler: Stage 140 (reduce at JsonRDD.scala:51) finished in 0.498 s
15/05/13 10:20:00 INFO DAGScheduler: Job 143 finished: reduce at JsonRDD.scala:51, took 0.531581 s
15/05/13 10:20:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:20:01 INFO DAGScheduler: Got job 144 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:20:01 INFO DAGScheduler: Final stage: Stage 141(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:20:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:20:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:20:01 INFO DAGScheduler: Submitting Stage 141 (MapPartitionsRDD[1011] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:20:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=17877385, maxMem=278302556
15/05/13 10:20:01 INFO MemoryStore: Block broadcast_212 stored as values in memory (estimated size 20.0 KB, free 248.3 MB)
15/05/13 10:20:01 INFO MemoryStore: ensureFreeSpace(10763) called with curMem=17897873, maxMem=278302556
15/05/13 10:20:01 INFO MemoryStore: Block broadcast_212_piece0 stored as bytes in memory (estimated size 10.5 KB, free 248.3 MB)
15/05/13 10:20:01 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.2 MB)
15/05/13 10:20:01 INFO BlockManagerMaster: Updated info of block broadcast_212_piece0
15/05/13 10:20:01 INFO SparkContext: Created broadcast 212 from broadcast at DAGScheduler.scala:839
15/05/13 10:20:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 141 (MapPartitionsRDD[1011] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:20:01 INFO TaskSchedulerImpl: Adding task set 141.0 with 1 tasks
15/05/13 10:20:01 INFO TaskSetManager: Starting task 0.0 in stage 141.0 (TID 141, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:20:01 INFO BlockManagerInfo: Added broadcast_212_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 263.7 MB)
15/05/13 10:20:01 INFO TaskSetManager: Finished task 0.0 in stage 141.0 (TID 141) in 126 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:20:01 INFO TaskSchedulerImpl: Removed TaskSet 141.0, whose tasks have all completed, from pool 
15/05/13 10:20:01 INFO DAGScheduler: Stage 141 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.128 s
15/05/13 10:20:01 INFO DAGScheduler: Job 144 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.167087 s
15/05/13 10:20:01 INFO JobScheduler: Finished job streaming job 1431526800000 ms.0 from job set of time 1431526800000 ms
15/05/13 10:20:01 INFO JobScheduler: Total delay: 1.403 s for time 1431526800000 ms (execution: 1.097 s)
15/05/13 10:20:01 INFO MapPartitionsRDD: Removing RDD 986 from persistence list
15/05/13 10:20:01 INFO BlockManager: Removing RDD 986
15/05/13 10:20:01 INFO UnionRDD: Removing RDD 985 from persistence list
15/05/13 10:20:01 INFO BlockManager: Removing RDD 985
15/05/13 10:20:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526740000 ms: 1431526680000 ms
15/05/13 10:20:01 INFO JobGenerator: Checkpointing graph for time 1431526800000 ms
15/05/13 10:20:01 INFO DStreamGraph: Updating checkpoint data for time 1431526800000 ms
15/05/13 10:20:01 INFO DStreamGraph: Updated checkpoint data for time 1431526800000 ms
15/05/13 10:20:01 INFO CheckpointWriter: Saving checkpoint for time 1431526800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000'
15/05/13 10:20:01 INFO CheckpointWriter: Checkpoint for time 1431526800000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000', took 7655 bytes and 57 ms
15/05/13 10:20:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526800000 ms
15/05/13 10:20:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526800000 ms
15/05/13 10:20:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:21:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 10:21:00 INFO FileInputDStream: New files at time 1431526860000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526772159.json
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=17908636, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_213 stored as values in memory (estimated size 232.9 KB, free 248.1 MB)
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=18147168, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_213_piece0 stored as bytes in memory (estimated size 34.9 KB, free 248.1 MB)
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:21:00 INFO BlockManagerMaster: Updated info of block broadcast_213_piece0
15/05/13 10:21:00 INFO SparkContext: Created broadcast 213 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:21:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:21:00 INFO JobScheduler: Added jobs for time 1431526860000 ms
15/05/13 10:21:00 INFO JobGenerator: Checkpointing graph for time 1431526860000 ms
15/05/13 10:21:00 INFO DStreamGraph: Updating checkpoint data for time 1431526860000 ms
15/05/13 10:21:00 INFO JobScheduler: Starting job streaming job 1431526860000 ms.0 from job set of time 1431526860000 ms
15/05/13 10:21:00 INFO DStreamGraph: Updated checkpoint data for time 1431526860000 ms
15/05/13 10:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431526860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000'
15/05/13 10:21:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:21:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83384): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431526860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000'
15/05/13 10:21:00 INFO DAGScheduler: Got job 145 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:21:00 INFO DAGScheduler: Final stage: Stage 142(reduce at JsonRDD.scala:51)
15/05/13 10:21:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:21:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:21:00 INFO DAGScheduler: Submitting Stage 142 (MapPartitionsRDD[1018] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=18182876, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_214 stored as values in memory (estimated size 5.9 KB, free 248.1 MB)
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=18188916, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_214_piece0 stored as bytes in memory (estimated size 4.1 KB, free 248.1 MB)
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:21:00 INFO BlockManagerMaster: Updated info of block broadcast_214_piece0
15/05/13 10:21:00 INFO SparkContext: Created broadcast 214 from broadcast at DAGScheduler.scala:839
15/05/13 10:21:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 142 (MapPartitionsRDD[1018] at map at JsonRDD.scala:51)
15/05/13 10:21:00 INFO TaskSchedulerImpl: Adding task set 142.0 with 1 tasks
15/05/13 10:21:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83386): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:21:00 INFO TaskSetManager: Starting task 0.0 in stage 142.0 (TID 142, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431526860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000'
15/05/13 10:21:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_214_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.7 MB)
15/05/13 10:21:00 INFO CheckpointWriter: Checkpoint for time 1431526860000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000', took 7665 bytes and 191 ms
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_213_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:21:00 INFO TaskSetManager: Finished task 0.0 in stage 142.0 (TID 142) in 332 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:21:00 INFO TaskSchedulerImpl: Removed TaskSet 142.0, whose tasks have all completed, from pool 
15/05/13 10:21:00 INFO DAGScheduler: Stage 142 (reduce at JsonRDD.scala:51) finished in 0.349 s
15/05/13 10:21:00 INFO DAGScheduler: Job 145 finished: reduce at JsonRDD.scala:51, took 0.400946 s
15/05/13 10:21:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:21:00 INFO DAGScheduler: Got job 146 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:21:00 INFO DAGScheduler: Final stage: Stage 143(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:21:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:21:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:21:00 INFO DAGScheduler: Submitting Stage 143 (MapPartitionsRDD[1025] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=18193142, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_215 stored as values in memory (estimated size 21.0 KB, free 248.0 MB)
15/05/13 10:21:00 INFO MemoryStore: ensureFreeSpace(11283) called with curMem=18214646, maxMem=278302556
15/05/13 10:21:00 INFO MemoryStore: Block broadcast_215_piece0 stored as bytes in memory (estimated size 11.0 KB, free 248.0 MB)
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 263.1 MB)
15/05/13 10:21:00 INFO BlockManagerMaster: Updated info of block broadcast_215_piece0
15/05/13 10:21:00 INFO SparkContext: Created broadcast 215 from broadcast at DAGScheduler.scala:839
15/05/13 10:21:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 143 (MapPartitionsRDD[1025] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:21:00 INFO TaskSchedulerImpl: Adding task set 143.0 with 1 tasks
15/05/13 10:21:00 INFO TaskSetManager: Starting task 0.0 in stage 143.0 (TID 143, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:21:00 INFO BlockManagerInfo: Added broadcast_215_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:21:01 INFO TaskSetManager: Finished task 0.0 in stage 143.0 (TID 143) in 128 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:21:01 INFO TaskSchedulerImpl: Removed TaskSet 143.0, whose tasks have all completed, from pool 
15/05/13 10:21:01 INFO DAGScheduler: Stage 143 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.131 s
15/05/13 10:21:01 INFO DAGScheduler: Job 146 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.165698 s
15/05/13 10:21:01 INFO JobScheduler: Finished job streaming job 1431526860000 ms.0 from job set of time 1431526860000 ms
15/05/13 10:21:01 INFO JobScheduler: Total delay: 1.127 s for time 1431526860000 ms (execution: 0.939 s)
15/05/13 10:21:01 INFO MapPartitionsRDD: Removing RDD 1000 from persistence list
15/05/13 10:21:01 INFO BlockManager: Removing RDD 1000
15/05/13 10:21:01 INFO UnionRDD: Removing RDD 999 from persistence list
15/05/13 10:21:01 INFO BlockManager: Removing RDD 999
15/05/13 10:21:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526800000 ms: 1431526740000 ms
15/05/13 10:21:01 INFO JobGenerator: Checkpointing graph for time 1431526860000 ms
15/05/13 10:21:01 INFO DStreamGraph: Updating checkpoint data for time 1431526860000 ms
15/05/13 10:21:01 INFO DStreamGraph: Updated checkpoint data for time 1431526860000 ms
15/05/13 10:21:01 INFO CheckpointWriter: Saving checkpoint for time 1431526860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000'
15/05/13 10:21:01 INFO CheckpointWriter: Checkpoint for time 1431526860000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000', took 7655 bytes and 88 ms
15/05/13 10:21:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526860000 ms
15/05/13 10:21:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526860000 ms
15/05/13 10:21:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:22:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 10:22:00 INFO FileInputDStream: New files at time 1431526920000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526834901.json
15/05/13 10:22:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=18225929, maxMem=278302556
15/05/13 10:22:00 INFO MemoryStore: Block broadcast_216 stored as values in memory (estimated size 232.9 KB, free 247.8 MB)
15/05/13 10:22:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=18464461, maxMem=278302556
15/05/13 10:22:00 INFO MemoryStore: Block broadcast_216_piece0 stored as bytes in memory (estimated size 34.9 KB, free 247.8 MB)
15/05/13 10:22:00 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:22:00 INFO BlockManagerMaster: Updated info of block broadcast_216_piece0
15/05/13 10:22:00 INFO SparkContext: Created broadcast 216 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:22:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:22:00 INFO JobScheduler: Added jobs for time 1431526920000 ms
15/05/13 10:22:00 INFO JobGenerator: Checkpointing graph for time 1431526920000 ms
15/05/13 10:22:00 INFO JobScheduler: Starting job streaming job 1431526920000 ms.0 from job set of time 1431526920000 ms
15/05/13 10:22:00 INFO DStreamGraph: Updating checkpoint data for time 1431526920000 ms
15/05/13 10:22:00 INFO DStreamGraph: Updated checkpoint data for time 1431526920000 ms
15/05/13 10:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431526920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000'
15/05/13 10:22:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83398): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431526920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000'
15/05/13 10:22:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:22:00 INFO DAGScheduler: Got job 147 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:22:00 INFO DAGScheduler: Final stage: Stage 144(reduce at JsonRDD.scala:51)
15/05/13 10:22:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:22:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:22:00 INFO DAGScheduler: Submitting Stage 144 (MapPartitionsRDD[1032] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:22:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83400): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431526920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000'
15/05/13 10:22:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=18500169, maxMem=278302556
15/05/13 10:22:00 INFO MemoryStore: Block broadcast_217 stored as values in memory (estimated size 5.9 KB, free 247.8 MB)
15/05/13 10:22:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=18506209, maxMem=278302556
15/05/13 10:22:00 INFO MemoryStore: Block broadcast_217_piece0 stored as bytes in memory (estimated size 4.1 KB, free 247.8 MB)
15/05/13 10:22:00 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:22:00 INFO BlockManagerMaster: Updated info of block broadcast_217_piece0
15/05/13 10:22:00 INFO SparkContext: Created broadcast 217 from broadcast at DAGScheduler.scala:839
15/05/13 10:22:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 144 (MapPartitionsRDD[1032] at map at JsonRDD.scala:51)
15/05/13 10:22:00 INFO TaskSchedulerImpl: Adding task set 144.0 with 1 tasks
15/05/13 10:22:00 INFO TaskSetManager: Starting task 0.0 in stage 144.0 (TID 144, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:22:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83402): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:22:00 WARN CheckpointWriter: Could not write checkpoint for time 1431526920000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000'
15/05/13 10:22:00 INFO BlockManagerInfo: Added broadcast_217_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:22:00 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:22:01 INFO TaskSetManager: Finished task 0.0 in stage 144.0 (TID 144) in 628 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:22:01 INFO TaskSchedulerImpl: Removed TaskSet 144.0, whose tasks have all completed, from pool 
15/05/13 10:22:01 INFO DAGScheduler: Stage 144 (reduce at JsonRDD.scala:51) finished in 0.641 s
15/05/13 10:22:01 INFO DAGScheduler: Job 147 finished: reduce at JsonRDD.scala:51, took 0.698600 s
15/05/13 10:22:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:22:01 INFO DAGScheduler: Got job 148 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:22:01 INFO DAGScheduler: Final stage: Stage 145(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:22:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:22:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:22:01 INFO DAGScheduler: Submitting Stage 145 (MapPartitionsRDD[1039] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:22:01 INFO MemoryStore: ensureFreeSpace(20664) called with curMem=18510435, maxMem=278302556
15/05/13 10:22:01 INFO MemoryStore: Block broadcast_218 stored as values in memory (estimated size 20.2 KB, free 247.7 MB)
15/05/13 10:22:01 INFO MemoryStore: ensureFreeSpace(10815) called with curMem=18531099, maxMem=278302556
15/05/13 10:22:01 INFO MemoryStore: Block broadcast_218_piece0 stored as bytes in memory (estimated size 10.6 KB, free 247.7 MB)
15/05/13 10:22:01 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.1 MB)
15/05/13 10:22:01 INFO BlockManagerMaster: Updated info of block broadcast_218_piece0
15/05/13 10:22:01 INFO SparkContext: Created broadcast 218 from broadcast at DAGScheduler.scala:839
15/05/13 10:22:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 145 (MapPartitionsRDD[1039] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:22:01 INFO TaskSchedulerImpl: Adding task set 145.0 with 1 tasks
15/05/13 10:22:01 INFO TaskSetManager: Starting task 0.0 in stage 145.0 (TID 145, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:22:01 INFO BlockManagerInfo: Added broadcast_218_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 263.6 MB)
15/05/13 10:22:01 INFO BlockManagerInfo: Added broadcast_216_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:22:01 INFO TaskSetManager: Finished task 0.0 in stage 145.0 (TID 145) in 235 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:22:01 INFO TaskSchedulerImpl: Removed TaskSet 145.0, whose tasks have all completed, from pool 
15/05/13 10:22:01 INFO DAGScheduler: Stage 145 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.236 s
15/05/13 10:22:01 INFO DAGScheduler: Job 148 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.265542 s
15/05/13 10:22:01 INFO JobScheduler: Finished job streaming job 1431526920000 ms.0 from job set of time 1431526920000 ms
15/05/13 10:22:01 INFO JobScheduler: Total delay: 1.554 s for time 1431526920000 ms (execution: 1.325 s)
15/05/13 10:22:01 INFO MapPartitionsRDD: Removing RDD 1014 from persistence list
15/05/13 10:22:01 INFO BlockManager: Removing RDD 1014
15/05/13 10:22:01 INFO UnionRDD: Removing RDD 1013 from persistence list
15/05/13 10:22:01 INFO BlockManager: Removing RDD 1013
15/05/13 10:22:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526860000 ms: 1431526800000 ms
15/05/13 10:22:01 INFO JobGenerator: Checkpointing graph for time 1431526920000 ms
15/05/13 10:22:01 INFO DStreamGraph: Updating checkpoint data for time 1431526920000 ms
15/05/13 10:22:01 INFO DStreamGraph: Updated checkpoint data for time 1431526920000 ms
15/05/13 10:22:01 INFO CheckpointWriter: Saving checkpoint for time 1431526920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000'
15/05/13 10:22:01 INFO CheckpointWriter: Checkpoint for time 1431526920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526920000', took 7655 bytes and 43 ms
15/05/13 10:22:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526920000 ms
15/05/13 10:22:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526920000 ms
15/05/13 10:22:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:23:00 INFO FileInputDStream: Finding new files took 63 ms
15/05/13 10:23:00 INFO FileInputDStream: New files at time 1431526980000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526895259.json
15/05/13 10:23:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=18541914, maxMem=278302556
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_219 stored as values in memory (estimated size 232.9 KB, free 247.5 MB)
15/05/13 10:23:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=18780446, maxMem=278302556
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_219_piece0 stored as bytes in memory (estimated size 34.9 KB, free 247.5 MB)
15/05/13 10:23:00 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.0 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_219_piece0
15/05/13 10:23:00 INFO SparkContext: Created broadcast 219 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:23:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:23:00 INFO JobScheduler: Added jobs for time 1431526980000 ms
15/05/13 10:23:00 INFO JobGenerator: Checkpointing graph for time 1431526980000 ms
15/05/13 10:23:00 INFO DStreamGraph: Updating checkpoint data for time 1431526980000 ms
15/05/13 10:23:00 INFO DStreamGraph: Updated checkpoint data for time 1431526980000 ms
15/05/13 10:23:00 INFO JobScheduler: Starting job streaming job 1431526980000 ms.0 from job set of time 1431526980000 ms
15/05/13 10:23:00 INFO CheckpointWriter: Saving checkpoint for time 1431526980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000'
15/05/13 10:23:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:23:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83408): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:23:00 INFO DAGScheduler: Got job 149 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:23:00 INFO DAGScheduler: Final stage: Stage 146(reduce at JsonRDD.scala:51)
15/05/13 10:23:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:23:00 INFO CheckpointWriter: Saving checkpoint for time 1431526980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000'
15/05/13 10:23:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:23:00 INFO DAGScheduler: Submitting Stage 146 (MapPartitionsRDD[1046] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:23:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=18816154, maxMem=278302556
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_220 stored as values in memory (estimated size 5.9 KB, free 247.5 MB)
15/05/13 10:23:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=18822194, maxMem=278302556
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_220_piece0 stored as bytes in memory (estimated size 4.1 KB, free 247.5 MB)
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 218
15/05/13 10:23:00 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_218
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_220_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_218 of size 20664 dropped from memory (free 259496798)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_218_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_218_piece0 of size 10815 dropped from memory (free 259507613)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_218_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 263.0 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_218_piece0
15/05/13 10:23:00 INFO SparkContext: Created broadcast 220 from broadcast at DAGScheduler.scala:839
15/05/13 10:23:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 146 (MapPartitionsRDD[1046] at map at JsonRDD.scala:51)
15/05/13 10:23:00 INFO TaskSchedulerImpl: Adding task set 146.0 with 1 tasks
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_218_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO TaskSetManager: Starting task 0.0 in stage 146.0 (TID 146, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:23:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83410): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:23:00 INFO CheckpointWriter: Saving checkpoint for time 1431526980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000'
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 218
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 213
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_213
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_213 of size 238532 dropped from memory (free 259746145)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_213_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_213_piece0 of size 35708 dropped from memory (free 259781853)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_213_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_213_piece0
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_213_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 213
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 214
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_214_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_214_piece0 of size 4226 dropped from memory (free 259786079)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_214_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_214_piece0
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_214
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_214 of size 6040 dropped from memory (free 259792119)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_214_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO BlockManagerInfo: Added broadcast_220_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.7 MB)
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 214
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 211
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_211_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_211_piece0 of size 4226 dropped from memory (free 259796345)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_211_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_211_piece0
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_211
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_211 of size 6040 dropped from memory (free 259802385)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_211_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 211
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 215
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_215
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_215 of size 21504 dropped from memory (free 259823889)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_215_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_215_piece0 of size 11283 dropped from memory (free 259835172)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_215_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_215_piece0
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_215_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO BlockManagerInfo: Added broadcast_219_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:23:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526680000
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 215
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 217
15/05/13 10:23:00 INFO CheckpointWriter: Checkpoint for time 1431526980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000', took 7663 bytes and 273 ms
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_217
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_217 of size 6040 dropped from memory (free 259841212)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_217_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_217_piece0 of size 4226 dropped from memory (free 259845438)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_217_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_217_piece0
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_217_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 217
15/05/13 10:23:00 INFO BlockManager: Removing broadcast 212
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_212
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_212 of size 20488 dropped from memory (free 259865926)
15/05/13 10:23:00 INFO BlockManager: Removing block broadcast_212_piece0
15/05/13 10:23:00 INFO MemoryStore: Block broadcast_212_piece0 of size 10763 dropped from memory (free 259876689)
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_212_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.1 MB)
15/05/13 10:23:00 INFO BlockManagerMaster: Updated info of block broadcast_212_piece0
15/05/13 10:23:00 INFO BlockManagerInfo: Removed broadcast_212_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 263.6 MB)
15/05/13 10:23:00 INFO ContextCleaner: Cleaned broadcast 212
15/05/13 10:23:00 INFO TaskSetManager: Finished task 0.0 in stage 146.0 (TID 146) in 297 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:23:00 INFO TaskSchedulerImpl: Removed TaskSet 146.0, whose tasks have all completed, from pool 
15/05/13 10:23:00 INFO DAGScheduler: Stage 146 (reduce at JsonRDD.scala:51) finished in 0.306 s
15/05/13 10:23:00 INFO DAGScheduler: Job 149 finished: reduce at JsonRDD.scala:51, took 0.407806 s
15/05/13 10:23:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:23:01 INFO DAGScheduler: Got job 150 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:23:01 INFO DAGScheduler: Final stage: Stage 147(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:23:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:23:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:23:01 INFO DAGScheduler: Submitting Stage 147 (MapPartitionsRDD[1053] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:23:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=18425867, maxMem=278302556
15/05/13 10:23:01 INFO MemoryStore: Block broadcast_221 stored as values in memory (estimated size 20.6 KB, free 247.8 MB)
15/05/13 10:23:01 INFO MemoryStore: ensureFreeSpace(10956) called with curMem=18446963, maxMem=278302556
15/05/13 10:23:01 INFO MemoryStore: Block broadcast_221_piece0 stored as bytes in memory (estimated size 10.7 KB, free 247.8 MB)
15/05/13 10:23:01 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.1 MB)
15/05/13 10:23:01 INFO BlockManagerMaster: Updated info of block broadcast_221_piece0
15/05/13 10:23:01 INFO SparkContext: Created broadcast 221 from broadcast at DAGScheduler.scala:839
15/05/13 10:23:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 147 (MapPartitionsRDD[1053] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:23:01 INFO TaskSchedulerImpl: Adding task set 147.0 with 1 tasks
15/05/13 10:23:01 INFO TaskSetManager: Starting task 0.0 in stage 147.0 (TID 147, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:23:01 INFO BlockManagerInfo: Added broadcast_221_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 264.6 MB)
15/05/13 10:23:01 INFO TaskSetManager: Finished task 0.0 in stage 147.0 (TID 147) in 99 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:23:01 INFO TaskSchedulerImpl: Removed TaskSet 147.0, whose tasks have all completed, from pool 
15/05/13 10:23:01 INFO DAGScheduler: Stage 147 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.102 s
15/05/13 10:23:01 INFO DAGScheduler: Job 150 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.144005 s
15/05/13 10:23:01 INFO JobScheduler: Finished job streaming job 1431526980000 ms.0 from job set of time 1431526980000 ms
15/05/13 10:23:01 INFO JobScheduler: Total delay: 1.197 s for time 1431526980000 ms (execution: 0.996 s)
15/05/13 10:23:01 INFO MapPartitionsRDD: Removing RDD 1028 from persistence list
15/05/13 10:23:01 INFO BlockManager: Removing RDD 1028
15/05/13 10:23:01 INFO UnionRDD: Removing RDD 1027 from persistence list
15/05/13 10:23:01 INFO BlockManager: Removing RDD 1027
15/05/13 10:23:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526920000 ms: 1431526860000 ms
15/05/13 10:23:01 INFO JobGenerator: Checkpointing graph for time 1431526980000 ms
15/05/13 10:23:01 INFO DStreamGraph: Updating checkpoint data for time 1431526980000 ms
15/05/13 10:23:01 INFO DStreamGraph: Updated checkpoint data for time 1431526980000 ms
15/05/13 10:23:01 INFO CheckpointWriter: Saving checkpoint for time 1431526980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000'
15/05/13 10:23:01 INFO CheckpointWriter: Checkpoint for time 1431526980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526980000', took 7656 bytes and 58 ms
15/05/13 10:23:01 INFO DStreamGraph: Clearing checkpoint data for time 1431526980000 ms
15/05/13 10:23:01 INFO DStreamGraph: Cleared checkpoint data for time 1431526980000 ms
15/05/13 10:23:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:24:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 10:24:00 INFO FileInputDStream: New files at time 1431527040000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431526957858.json
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=18457919, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_222 stored as values in memory (estimated size 232.9 KB, free 247.6 MB)
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=18696451, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_222_piece0 stored as bytes in memory (estimated size 34.9 KB, free 247.5 MB)
15/05/13 10:24:00 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:24:00 INFO BlockManagerMaster: Updated info of block broadcast_222_piece0
15/05/13 10:24:00 INFO SparkContext: Created broadcast 222 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:24:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:24:00 INFO JobScheduler: Added jobs for time 1431527040000 ms
15/05/13 10:24:00 INFO JobGenerator: Checkpointing graph for time 1431527040000 ms
15/05/13 10:24:00 INFO JobScheduler: Starting job streaming job 1431527040000 ms.0 from job set of time 1431527040000 ms
15/05/13 10:24:00 INFO DStreamGraph: Updating checkpoint data for time 1431527040000 ms
15/05/13 10:24:00 INFO DStreamGraph: Updated checkpoint data for time 1431527040000 ms
15/05/13 10:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431527040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000'
15/05/13 10:24:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83417): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431527040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000'
15/05/13 10:24:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:24:00 INFO DAGScheduler: Got job 151 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:24:00 INFO DAGScheduler: Final stage: Stage 148(reduce at JsonRDD.scala:51)
15/05/13 10:24:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:24:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:24:00 INFO DAGScheduler: Submitting Stage 148 (MapPartitionsRDD[1060] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=18732159, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_223 stored as values in memory (estimated size 5.9 KB, free 247.5 MB)
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=18738199, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_223_piece0 stored as bytes in memory (estimated size 4.1 KB, free 247.5 MB)
15/05/13 10:24:00 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.1 MB)
15/05/13 10:24:00 INFO BlockManagerMaster: Updated info of block broadcast_223_piece0
15/05/13 10:24:00 INFO SparkContext: Created broadcast 223 from broadcast at DAGScheduler.scala:839
15/05/13 10:24:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 148 (MapPartitionsRDD[1060] at map at JsonRDD.scala:51)
15/05/13 10:24:00 INFO TaskSchedulerImpl: Adding task set 148.0 with 1 tasks
15/05/13 10:24:00 INFO TaskSetManager: Starting task 0.0 in stage 148.0 (TID 148, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:24:00 INFO BlockManagerInfo: Added broadcast_223_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:24:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83419): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431527040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000'
15/05/13 10:24:00 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:24:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83421): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:24:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527040000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000'
15/05/13 10:24:00 INFO TaskSetManager: Finished task 0.0 in stage 148.0 (TID 148) in 286 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:24:00 INFO TaskSchedulerImpl: Removed TaskSet 148.0, whose tasks have all completed, from pool 
15/05/13 10:24:00 INFO DAGScheduler: Stage 148 (reduce at JsonRDD.scala:51) finished in 0.298 s
15/05/13 10:24:00 INFO DAGScheduler: Job 151 finished: reduce at JsonRDD.scala:51, took 0.336435 s
15/05/13 10:24:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:24:00 INFO DAGScheduler: Got job 152 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:24:00 INFO DAGScheduler: Final stage: Stage 149(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:24:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:24:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:24:00 INFO DAGScheduler: Submitting Stage 149 (MapPartitionsRDD[1067] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=18742425, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_224 stored as values in memory (estimated size 20.0 KB, free 247.5 MB)
15/05/13 10:24:00 INFO MemoryStore: ensureFreeSpace(10759) called with curMem=18762913, maxMem=278302556
15/05/13 10:24:00 INFO MemoryStore: Block broadcast_224_piece0 stored as bytes in memory (estimated size 10.5 KB, free 247.5 MB)
15/05/13 10:24:00 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 263.1 MB)
15/05/13 10:24:00 INFO BlockManagerMaster: Updated info of block broadcast_224_piece0
15/05/13 10:24:00 INFO SparkContext: Created broadcast 224 from broadcast at DAGScheduler.scala:839
15/05/13 10:24:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 149 (MapPartitionsRDD[1067] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:24:00 INFO TaskSchedulerImpl: Adding task set 149.0 with 1 tasks
15/05/13 10:24:00 INFO TaskSetManager: Starting task 0.0 in stage 149.0 (TID 149, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:24:01 INFO BlockManagerInfo: Added broadcast_224_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 263.6 MB)
15/05/13 10:24:01 INFO BlockManagerInfo: Added broadcast_222_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:24:01 INFO TaskSetManager: Finished task 0.0 in stage 149.0 (TID 149) in 337 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:24:01 INFO TaskSchedulerImpl: Removed TaskSet 149.0, whose tasks have all completed, from pool 
15/05/13 10:24:01 INFO DAGScheduler: Stage 149 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.340 s
15/05/13 10:24:01 INFO DAGScheduler: Job 152 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.374637 s
15/05/13 10:24:01 INFO JobScheduler: Finished job streaming job 1431527040000 ms.0 from job set of time 1431527040000 ms
15/05/13 10:24:01 INFO JobScheduler: Total delay: 1.350 s for time 1431527040000 ms (execution: 1.131 s)
15/05/13 10:24:01 INFO MapPartitionsRDD: Removing RDD 1042 from persistence list
15/05/13 10:24:01 INFO BlockManager: Removing RDD 1042
15/05/13 10:24:01 INFO UnionRDD: Removing RDD 1041 from persistence list
15/05/13 10:24:01 INFO BlockManager: Removing RDD 1041
15/05/13 10:24:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431526980000 ms: 1431526920000 ms
15/05/13 10:24:01 INFO JobGenerator: Checkpointing graph for time 1431527040000 ms
15/05/13 10:24:01 INFO DStreamGraph: Updating checkpoint data for time 1431527040000 ms
15/05/13 10:24:01 INFO DStreamGraph: Updated checkpoint data for time 1431527040000 ms
15/05/13 10:24:01 INFO CheckpointWriter: Saving checkpoint for time 1431527040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000'
15/05/13 10:24:01 INFO CheckpointWriter: Checkpoint for time 1431527040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000', took 7657 bytes and 79 ms
15/05/13 10:24:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527040000 ms
15/05/13 10:24:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527040000 ms
15/05/13 10:24:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:25:00 INFO FileInputDStream: Finding new files took 61 ms
15/05/13 10:25:00 INFO FileInputDStream: New files at time 1431527100000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527021913.json
15/05/13 10:25:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=18773672, maxMem=278302556
15/05/13 10:25:00 INFO MemoryStore: Block broadcast_225 stored as values in memory (estimated size 232.9 KB, free 247.3 MB)
15/05/13 10:25:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=19012204, maxMem=278302556
15/05/13 10:25:00 INFO MemoryStore: Block broadcast_225_piece0 stored as bytes in memory (estimated size 34.9 KB, free 247.2 MB)
15/05/13 10:25:00 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.0 MB)
15/05/13 10:25:00 INFO BlockManagerMaster: Updated info of block broadcast_225_piece0
15/05/13 10:25:00 INFO SparkContext: Created broadcast 225 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:25:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:25:00 INFO JobScheduler: Starting job streaming job 1431527100000 ms.0 from job set of time 1431527100000 ms
15/05/13 10:25:00 INFO JobScheduler: Added jobs for time 1431527100000 ms
15/05/13 10:25:00 INFO JobGenerator: Checkpointing graph for time 1431527100000 ms
15/05/13 10:25:00 INFO DStreamGraph: Updating checkpoint data for time 1431527100000 ms
15/05/13 10:25:00 INFO DStreamGraph: Updated checkpoint data for time 1431527100000 ms
15/05/13 10:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431527100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000'
15/05/13 10:25:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:25:00 INFO DAGScheduler: Got job 153 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:25:00 INFO DAGScheduler: Final stage: Stage 150(reduce at JsonRDD.scala:51)
15/05/13 10:25:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:25:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:25:00 INFO DAGScheduler: Submitting Stage 150 (MapPartitionsRDD[1074] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:25:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=19047912, maxMem=278302556
15/05/13 10:25:00 INFO MemoryStore: Block broadcast_226 stored as values in memory (estimated size 5.9 KB, free 247.2 MB)
15/05/13 10:25:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83426): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431527100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000'
15/05/13 10:25:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=19053952, maxMem=278302556
15/05/13 10:25:00 INFO MemoryStore: Block broadcast_226_piece0 stored as bytes in memory (estimated size 4.1 KB, free 247.2 MB)
15/05/13 10:25:00 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:25:00 INFO BlockManagerMaster: Updated info of block broadcast_226_piece0
15/05/13 10:25:00 INFO SparkContext: Created broadcast 226 from broadcast at DAGScheduler.scala:839
15/05/13 10:25:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 150 (MapPartitionsRDD[1074] at map at JsonRDD.scala:51)
15/05/13 10:25:00 INFO TaskSchedulerImpl: Adding task set 150.0 with 1 tasks
15/05/13 10:25:00 INFO TaskSetManager: Starting task 0.0 in stage 150.0 (TID 150, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:25:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83428): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:25:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83428): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431527100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000'
15/05/13 10:25:00 INFO BlockManagerInfo: Added broadcast_226_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:25:00 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.6 MB)
15/05/13 10:25:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526800000.bk
15/05/13 10:25:00 INFO CheckpointWriter: Checkpoint for time 1431527100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000', took 7669 bytes and 234 ms
15/05/13 10:25:01 INFO TaskSetManager: Finished task 0.0 in stage 150.0 (TID 150) in 737 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:25:01 INFO TaskSchedulerImpl: Removed TaskSet 150.0, whose tasks have all completed, from pool 
15/05/13 10:25:01 INFO DAGScheduler: Stage 150 (reduce at JsonRDD.scala:51) finished in 0.747 s
15/05/13 10:25:01 INFO DAGScheduler: Job 153 finished: reduce at JsonRDD.scala:51, took 0.790993 s
15/05/13 10:25:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:25:01 INFO DAGScheduler: Got job 154 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:25:01 INFO DAGScheduler: Final stage: Stage 151(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:25:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:25:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:25:01 INFO DAGScheduler: Submitting Stage 151 (MapPartitionsRDD[1081] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:25:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=19058178, maxMem=278302556
15/05/13 10:25:01 INFO MemoryStore: Block broadcast_227 stored as values in memory (estimated size 20.8 KB, free 247.2 MB)
15/05/13 10:25:01 INFO MemoryStore: ensureFreeSpace(11121) called with curMem=19079482, maxMem=278302556
15/05/13 10:25:01 INFO MemoryStore: Block broadcast_227_piece0 stored as bytes in memory (estimated size 10.9 KB, free 247.2 MB)
15/05/13 10:25:01 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 263.0 MB)
15/05/13 10:25:01 INFO BlockManagerMaster: Updated info of block broadcast_227_piece0
15/05/13 10:25:01 INFO SparkContext: Created broadcast 227 from broadcast at DAGScheduler.scala:839
15/05/13 10:25:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 151 (MapPartitionsRDD[1081] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:25:01 INFO TaskSchedulerImpl: Adding task set 151.0 with 1 tasks
15/05/13 10:25:01 INFO TaskSetManager: Starting task 0.0 in stage 151.0 (TID 151, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:25:01 INFO BlockManagerInfo: Added broadcast_227_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 264.6 MB)
15/05/13 10:25:01 INFO BlockManagerInfo: Added broadcast_225_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:25:01 INFO TaskSetManager: Finished task 0.0 in stage 151.0 (TID 151) in 367 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:25:01 INFO TaskSchedulerImpl: Removed TaskSet 151.0, whose tasks have all completed, from pool 
15/05/13 10:25:01 INFO DAGScheduler: Stage 151 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.369 s
15/05/13 10:25:01 INFO DAGScheduler: Job 154 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.399696 s
15/05/13 10:25:01 INFO JobScheduler: Finished job streaming job 1431527100000 ms.0 from job set of time 1431527100000 ms
15/05/13 10:25:01 INFO JobScheduler: Total delay: 1.739 s for time 1431527100000 ms (execution: 1.558 s)
15/05/13 10:25:01 INFO MapPartitionsRDD: Removing RDD 1056 from persistence list
15/05/13 10:25:01 INFO BlockManager: Removing RDD 1056
15/05/13 10:25:01 INFO UnionRDD: Removing RDD 1055 from persistence list
15/05/13 10:25:01 INFO BlockManager: Removing RDD 1055
15/05/13 10:25:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527040000 ms: 1431526980000 ms
15/05/13 10:25:01 INFO JobGenerator: Checkpointing graph for time 1431527100000 ms
15/05/13 10:25:01 INFO DStreamGraph: Updating checkpoint data for time 1431527100000 ms
15/05/13 10:25:01 INFO DStreamGraph: Updated checkpoint data for time 1431527100000 ms
15/05/13 10:25:01 INFO CheckpointWriter: Saving checkpoint for time 1431527100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000'
15/05/13 10:25:01 INFO CheckpointWriter: Checkpoint for time 1431527100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000', took 7660 bytes and 61 ms
15/05/13 10:25:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527100000 ms
15/05/13 10:25:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527100000 ms
15/05/13 10:25:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:26:00 INFO FileInputDStream: Finding new files took 66 ms
15/05/13 10:26:00 INFO FileInputDStream: New files at time 1431527160000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527083226.json
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=19090603, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_228 stored as values in memory (estimated size 232.9 KB, free 247.0 MB)
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=19329135, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_228_piece0 stored as bytes in memory (estimated size 34.9 KB, free 246.9 MB)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_228_piece0
15/05/13 10:26:00 INFO SparkContext: Created broadcast 228 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:26:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:26:00 INFO JobScheduler: Added jobs for time 1431527160000 ms
15/05/13 10:26:00 INFO JobGenerator: Checkpointing graph for time 1431527160000 ms
15/05/13 10:26:00 INFO JobScheduler: Starting job streaming job 1431527160000 ms.0 from job set of time 1431527160000 ms
15/05/13 10:26:00 INFO DStreamGraph: Updating checkpoint data for time 1431527160000 ms
15/05/13 10:26:00 INFO DStreamGraph: Updated checkpoint data for time 1431527160000 ms
15/05/13 10:26:00 INFO CheckpointWriter: Saving checkpoint for time 1431527160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527160000'
15/05/13 10:26:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:26:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431526860000.bk
15/05/13 10:26:00 INFO DAGScheduler: Got job 155 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:26:00 INFO DAGScheduler: Final stage: Stage 152(reduce at JsonRDD.scala:51)
15/05/13 10:26:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:26:00 INFO CheckpointWriter: Checkpoint for time 1431527160000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527160000', took 7668 bytes and 99 ms
15/05/13 10:26:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:26:00 INFO DAGScheduler: Submitting Stage 152 (MapPartitionsRDD[1088] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=19364843, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_229 stored as values in memory (estimated size 5.9 KB, free 246.9 MB)
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=19370883, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_229_piece0 stored as bytes in memory (estimated size 4.1 KB, free 246.9 MB)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_229_piece0
15/05/13 10:26:00 INFO SparkContext: Created broadcast 229 from broadcast at DAGScheduler.scala:839
15/05/13 10:26:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 152 (MapPartitionsRDD[1088] at map at JsonRDD.scala:51)
15/05/13 10:26:00 INFO TaskSchedulerImpl: Adding task set 152.0 with 1 tasks
15/05/13 10:26:00 INFO TaskSetManager: Starting task 0.0 in stage 152.0 (TID 152, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_229_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:26:00 INFO TaskSetManager: Finished task 0.0 in stage 152.0 (TID 152) in 357 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:26:00 INFO TaskSchedulerImpl: Removed TaskSet 152.0, whose tasks have all completed, from pool 
15/05/13 10:26:00 INFO DAGScheduler: Stage 152 (reduce at JsonRDD.scala:51) finished in 0.364 s
15/05/13 10:26:00 INFO DAGScheduler: Job 155 finished: reduce at JsonRDD.scala:51, took 0.400277 s
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 220
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_220
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_220 of size 6040 dropped from memory (free 258933489)
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_220_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_220_piece0 of size 4228 dropped from memory (free 258937717)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_220_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_220_piece0
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_220_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 220
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 224
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_224_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_224_piece0 of size 10759 dropped from memory (free 258948476)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_224_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_224_piece0
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_224
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_224 of size 20488 dropped from memory (free 258968964)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_224_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 263.6 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 224
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 223
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_223
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_223 of size 6040 dropped from memory (free 258975004)
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_223_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_223_piece0 of size 4226 dropped from memory (free 258979230)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_223_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_223_piece0
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_223_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 223
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 221
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_221_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_221_piece0 of size 10956 dropped from memory (free 258990186)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_221_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_221_piece0
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_221
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_221 of size 21096 dropped from memory (free 259011282)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_221_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 264.6 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 221
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 229
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_229
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_229 of size 6040 dropped from memory (free 259017322)
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_229_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_229_piece0 of size 4224 dropped from memory (free 259021546)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_229_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_229_piece0
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_229_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 229
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 227
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_227_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_227_piece0 of size 11121 dropped from memory (free 259032667)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_227_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_227_piece0
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_227
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_227 of size 21304 dropped from memory (free 259053971)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_227_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 264.6 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 227
15/05/13 10:26:00 INFO BlockManager: Removing broadcast 226
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_226
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_226 of size 6040 dropped from memory (free 259060011)
15/05/13 10:26:00 INFO BlockManager: Removing block broadcast_226_piece0
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_226_piece0 of size 4226 dropped from memory (free 259064237)
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_226_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_226_piece0
15/05/13 10:26:00 INFO BlockManagerInfo: Removed broadcast_226_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 10:26:00 INFO ContextCleaner: Cleaned broadcast 226
15/05/13 10:26:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:26:00 INFO DAGScheduler: Got job 156 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:26:00 INFO DAGScheduler: Final stage: Stage 153(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:26:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:26:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:26:00 INFO DAGScheduler: Submitting Stage 153 (MapPartitionsRDD[1095] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(20984) called with curMem=19238319, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_230 stored as values in memory (estimated size 20.5 KB, free 247.0 MB)
15/05/13 10:26:00 INFO MemoryStore: ensureFreeSpace(10949) called with curMem=19259303, maxMem=278302556
15/05/13 10:26:00 INFO MemoryStore: Block broadcast_230_piece0 stored as bytes in memory (estimated size 10.7 KB, free 247.0 MB)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 263.0 MB)
15/05/13 10:26:00 INFO BlockManagerMaster: Updated info of block broadcast_230_piece0
15/05/13 10:26:00 INFO SparkContext: Created broadcast 230 from broadcast at DAGScheduler.scala:839
15/05/13 10:26:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 153 (MapPartitionsRDD[1095] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:26:00 INFO TaskSchedulerImpl: Adding task set 153.0 with 1 tasks
15/05/13 10:26:00 INFO TaskSetManager: Starting task 0.0 in stage 153.0 (TID 153, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_230_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.6 MB)
15/05/13 10:26:00 INFO BlockManagerInfo: Added broadcast_228_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:26:01 INFO TaskSetManager: Finished task 0.0 in stage 153.0 (TID 153) in 188 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:26:01 INFO TaskSchedulerImpl: Removed TaskSet 153.0, whose tasks have all completed, from pool 
15/05/13 10:26:01 INFO DAGScheduler: Stage 153 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.191 s
15/05/13 10:26:01 INFO DAGScheduler: Job 156 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.207077 s
15/05/13 10:26:01 INFO JobScheduler: Finished job streaming job 1431527160000 ms.0 from job set of time 1431527160000 ms
15/05/13 10:26:01 INFO JobScheduler: Total delay: 1.057 s for time 1431527160000 ms (execution: 0.881 s)
15/05/13 10:26:01 INFO MapPartitionsRDD: Removing RDD 1070 from persistence list
15/05/13 10:26:01 INFO BlockManager: Removing RDD 1070
15/05/13 10:26:01 INFO UnionRDD: Removing RDD 1069 from persistence list
15/05/13 10:26:01 INFO BlockManager: Removing RDD 1069
15/05/13 10:26:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527100000 ms: 1431527040000 ms
15/05/13 10:26:01 INFO JobGenerator: Checkpointing graph for time 1431527160000 ms
15/05/13 10:26:01 INFO DStreamGraph: Updating checkpoint data for time 1431527160000 ms
15/05/13 10:26:01 INFO DStreamGraph: Updated checkpoint data for time 1431527160000 ms
15/05/13 10:26:01 INFO CheckpointWriter: Saving checkpoint for time 1431527160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527160000'
15/05/13 10:26:01 INFO CheckpointWriter: Checkpoint for time 1431527160000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527160000', took 7656 bytes and 37 ms
15/05/13 10:26:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527160000 ms
15/05/13 10:26:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527160000 ms
15/05/13 10:26:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:27:00 INFO FileInputDStream: Finding new files took 64 ms
15/05/13 10:27:00 INFO FileInputDStream: New files at time 1431527220000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527148111.json
15/05/13 10:27:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=19270252, maxMem=278302556
15/05/13 10:27:00 INFO MemoryStore: Block broadcast_231 stored as values in memory (estimated size 232.9 KB, free 246.8 MB)
15/05/13 10:27:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=19508784, maxMem=278302556
15/05/13 10:27:00 INFO MemoryStore: Block broadcast_231_piece0 stored as bytes in memory (estimated size 34.9 KB, free 246.8 MB)
15/05/13 10:27:00 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 263.0 MB)
15/05/13 10:27:00 INFO BlockManagerMaster: Updated info of block broadcast_231_piece0
15/05/13 10:27:00 INFO SparkContext: Created broadcast 231 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:27:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:27:00 INFO JobScheduler: Added jobs for time 1431527220000 ms
15/05/13 10:27:00 INFO JobScheduler: Starting job streaming job 1431527220000 ms.0 from job set of time 1431527220000 ms
15/05/13 10:27:00 INFO JobGenerator: Checkpointing graph for time 1431527220000 ms
15/05/13 10:27:00 INFO DStreamGraph: Updating checkpoint data for time 1431527220000 ms
15/05/13 10:27:00 INFO DStreamGraph: Updated checkpoint data for time 1431527220000 ms
15/05/13 10:27:00 INFO CheckpointWriter: Saving checkpoint for time 1431527220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000'
15/05/13 10:27:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83448): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:27:00 INFO CheckpointWriter: Saving checkpoint for time 1431527220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000'
15/05/13 10:27:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:27:00 INFO DAGScheduler: Got job 157 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:27:00 INFO DAGScheduler: Final stage: Stage 154(reduce at JsonRDD.scala:51)
15/05/13 10:27:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:27:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:27:00 INFO DAGScheduler: Submitting Stage 154 (MapPartitionsRDD[1102] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:27:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=19544492, maxMem=278302556
15/05/13 10:27:00 INFO MemoryStore: Block broadcast_232 stored as values in memory (estimated size 5.9 KB, free 246.8 MB)
15/05/13 10:27:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=19550532, maxMem=278302556
15/05/13 10:27:00 INFO MemoryStore: Block broadcast_232_piece0 stored as bytes in memory (estimated size 4.1 KB, free 246.8 MB)
15/05/13 10:27:00 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 263.0 MB)
15/05/13 10:27:00 INFO BlockManagerMaster: Updated info of block broadcast_232_piece0
15/05/13 10:27:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83450): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:27:00 INFO CheckpointWriter: Saving checkpoint for time 1431527220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000'
15/05/13 10:27:00 INFO SparkContext: Created broadcast 232 from broadcast at DAGScheduler.scala:839
15/05/13 10:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 154 (MapPartitionsRDD[1102] at map at JsonRDD.scala:51)
15/05/13 10:27:00 INFO TaskSchedulerImpl: Adding task set 154.0 with 1 tasks
15/05/13 10:27:00 INFO TaskSetManager: Starting task 0.0 in stage 154.0 (TID 154, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:27:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83452): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:27:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83452): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:27:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527220000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000'
15/05/13 10:27:00 INFO BlockManagerInfo: Added broadcast_232_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.5 MB)
15/05/13 10:27:00 INFO BlockManagerInfo: Added broadcast_231_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.5 MB)
15/05/13 10:27:00 INFO TaskSetManager: Finished task 0.0 in stage 154.0 (TID 154) in 426 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:27:00 INFO TaskSchedulerImpl: Removed TaskSet 154.0, whose tasks have all completed, from pool 
15/05/13 10:27:00 INFO DAGScheduler: Stage 154 (reduce at JsonRDD.scala:51) finished in 0.436 s
15/05/13 10:27:00 INFO DAGScheduler: Job 157 finished: reduce at JsonRDD.scala:51, took 0.499595 s
15/05/13 10:27:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:27:01 INFO DAGScheduler: Got job 158 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:27:01 INFO DAGScheduler: Final stage: Stage 155(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:27:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:27:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:27:01 INFO DAGScheduler: Submitting Stage 155 (MapPartitionsRDD[1109] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:27:01 INFO MemoryStore: ensureFreeSpace(20776) called with curMem=19554758, maxMem=278302556
15/05/13 10:27:01 INFO MemoryStore: Block broadcast_233 stored as values in memory (estimated size 20.3 KB, free 246.7 MB)
15/05/13 10:27:01 INFO MemoryStore: ensureFreeSpace(10889) called with curMem=19575534, maxMem=278302556
15/05/13 10:27:01 INFO MemoryStore: Block broadcast_233_piece0 stored as bytes in memory (estimated size 10.6 KB, free 246.7 MB)
15/05/13 10:27:01 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 263.0 MB)
15/05/13 10:27:01 INFO BlockManagerMaster: Updated info of block broadcast_233_piece0
15/05/13 10:27:01 INFO SparkContext: Created broadcast 233 from broadcast at DAGScheduler.scala:839
15/05/13 10:27:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 155 (MapPartitionsRDD[1109] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:27:01 INFO TaskSchedulerImpl: Adding task set 155.0 with 1 tasks
15/05/13 10:27:01 INFO TaskSetManager: Starting task 0.0 in stage 155.0 (TID 155, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:27:01 INFO BlockManagerInfo: Added broadcast_233_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 263.5 MB)
15/05/13 10:27:01 INFO TaskSetManager: Finished task 0.0 in stage 155.0 (TID 155) in 165 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:27:01 INFO TaskSchedulerImpl: Removed TaskSet 155.0, whose tasks have all completed, from pool 
15/05/13 10:27:01 INFO DAGScheduler: Stage 155 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.167 s
15/05/13 10:27:01 INFO DAGScheduler: Job 158 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.209507 s
15/05/13 10:27:01 INFO JobScheduler: Finished job streaming job 1431527220000 ms.0 from job set of time 1431527220000 ms
15/05/13 10:27:01 INFO JobScheduler: Total delay: 1.360 s for time 1431527220000 ms (execution: 1.111 s)
15/05/13 10:27:01 INFO MapPartitionsRDD: Removing RDD 1084 from persistence list
15/05/13 10:27:01 INFO BlockManager: Removing RDD 1084
15/05/13 10:27:01 INFO UnionRDD: Removing RDD 1083 from persistence list
15/05/13 10:27:01 INFO BlockManager: Removing RDD 1083
15/05/13 10:27:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527160000 ms: 1431527100000 ms
15/05/13 10:27:01 INFO JobGenerator: Checkpointing graph for time 1431527220000 ms
15/05/13 10:27:01 INFO DStreamGraph: Updating checkpoint data for time 1431527220000 ms
15/05/13 10:27:01 INFO DStreamGraph: Updated checkpoint data for time 1431527220000 ms
15/05/13 10:27:01 INFO CheckpointWriter: Saving checkpoint for time 1431527220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000'
15/05/13 10:27:01 INFO CheckpointWriter: Checkpoint for time 1431527220000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000', took 7655 bytes and 80 ms
15/05/13 10:27:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527220000 ms
15/05/13 10:27:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527220000 ms
15/05/13 10:27:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:28:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 10:28:00 INFO FileInputDStream: New files at time 1431527280000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527213233.json
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=19586423, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_234 stored as values in memory (estimated size 232.9 KB, free 246.5 MB)
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=19824955, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_234_piece0 stored as bytes in memory (estimated size 34.9 KB, free 246.5 MB)
15/05/13 10:28:00 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.9 MB)
15/05/13 10:28:00 INFO BlockManagerMaster: Updated info of block broadcast_234_piece0
15/05/13 10:28:00 INFO SparkContext: Created broadcast 234 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:28:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:28:00 INFO JobScheduler: Added jobs for time 1431527280000 ms
15/05/13 10:28:00 INFO JobGenerator: Checkpointing graph for time 1431527280000 ms
15/05/13 10:28:00 INFO JobScheduler: Starting job streaming job 1431527280000 ms.0 from job set of time 1431527280000 ms
15/05/13 10:28:00 INFO DStreamGraph: Updating checkpoint data for time 1431527280000 ms
15/05/13 10:28:00 INFO DStreamGraph: Updated checkpoint data for time 1431527280000 ms
15/05/13 10:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431527280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000'
15/05/13 10:28:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83458): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431527280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000'
15/05/13 10:28:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:28:00 INFO DAGScheduler: Got job 159 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:28:00 INFO DAGScheduler: Final stage: Stage 156(reduce at JsonRDD.scala:51)
15/05/13 10:28:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:28:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:28:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83460): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:28:00 INFO DAGScheduler: Submitting Stage 156 (MapPartitionsRDD[1116] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:28:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83460): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431527280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000'
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=19860664, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_235 stored as values in memory (estimated size 5.9 KB, free 246.5 MB)
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=19866704, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_235_piece0 stored as bytes in memory (estimated size 4.1 KB, free 246.5 MB)
15/05/13 10:28:00 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:28:00 INFO BlockManagerMaster: Updated info of block broadcast_235_piece0
15/05/13 10:28:00 INFO SparkContext: Created broadcast 235 from broadcast at DAGScheduler.scala:839
15/05/13 10:28:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 156 (MapPartitionsRDD[1116] at map at JsonRDD.scala:51)
15/05/13 10:28:00 INFO TaskSchedulerImpl: Adding task set 156.0 with 1 tasks
15/05/13 10:28:00 INFO TaskSetManager: Starting task 0.0 in stage 156.0 (TID 156, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:28:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83462): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:28:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527280000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000'
15/05/13 10:28:00 INFO BlockManagerInfo: Added broadcast_235_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:28:00 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.6 MB)
15/05/13 10:28:00 INFO TaskSetManager: Finished task 0.0 in stage 156.0 (TID 156) in 477 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:28:00 INFO TaskSchedulerImpl: Removed TaskSet 156.0, whose tasks have all completed, from pool 
15/05/13 10:28:00 INFO DAGScheduler: Stage 156 (reduce at JsonRDD.scala:51) finished in 0.488 s
15/05/13 10:28:00 INFO DAGScheduler: Job 159 finished: reduce at JsonRDD.scala:51, took 0.529440 s
15/05/13 10:28:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:28:00 INFO DAGScheduler: Got job 160 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:28:00 INFO DAGScheduler: Final stage: Stage 157(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:28:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:28:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:28:00 INFO DAGScheduler: Submitting Stage 157 (MapPartitionsRDD[1123] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(21072) called with curMem=19870930, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_236 stored as values in memory (estimated size 20.6 KB, free 246.4 MB)
15/05/13 10:28:00 INFO MemoryStore: ensureFreeSpace(11013) called with curMem=19892002, maxMem=278302556
15/05/13 10:28:00 INFO MemoryStore: Block broadcast_236_piece0 stored as bytes in memory (estimated size 10.8 KB, free 246.4 MB)
15/05/13 10:28:00 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 262.9 MB)
15/05/13 10:28:00 INFO BlockManagerMaster: Updated info of block broadcast_236_piece0
15/05/13 10:28:00 INFO SparkContext: Created broadcast 236 from broadcast at DAGScheduler.scala:839
15/05/13 10:28:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 157 (MapPartitionsRDD[1123] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:28:00 INFO TaskSchedulerImpl: Adding task set 157.0 with 1 tasks
15/05/13 10:28:00 INFO TaskSetManager: Starting task 0.0 in stage 157.0 (TID 157, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:28:01 INFO BlockManagerInfo: Added broadcast_236_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 263.5 MB)
15/05/13 10:28:01 INFO BlockManagerInfo: Added broadcast_234_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:28:01 INFO TaskSetManager: Finished task 0.0 in stage 157.0 (TID 157) in 267 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:28:01 INFO TaskSchedulerImpl: Removed TaskSet 157.0, whose tasks have all completed, from pool 
15/05/13 10:28:01 INFO DAGScheduler: Stage 157 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.269 s
15/05/13 10:28:01 INFO DAGScheduler: Job 160 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.297838 s
15/05/13 10:28:01 INFO JobScheduler: Finished job streaming job 1431527280000 ms.0 from job set of time 1431527280000 ms
15/05/13 10:28:01 INFO JobScheduler: Total delay: 1.292 s for time 1431527280000 ms (execution: 1.133 s)
15/05/13 10:28:01 INFO MapPartitionsRDD: Removing RDD 1098 from persistence list
15/05/13 10:28:01 INFO BlockManager: Removing RDD 1098
15/05/13 10:28:01 INFO UnionRDD: Removing RDD 1097 from persistence list
15/05/13 10:28:01 INFO BlockManager: Removing RDD 1097
15/05/13 10:28:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527220000 ms: 1431527160000 ms
15/05/13 10:28:01 INFO JobGenerator: Checkpointing graph for time 1431527280000 ms
15/05/13 10:28:01 INFO DStreamGraph: Updating checkpoint data for time 1431527280000 ms
15/05/13 10:28:01 INFO DStreamGraph: Updated checkpoint data for time 1431527280000 ms
15/05/13 10:28:01 INFO CheckpointWriter: Saving checkpoint for time 1431527280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000'
15/05/13 10:28:01 INFO CheckpointWriter: Checkpoint for time 1431527280000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527280000', took 7657 bytes and 63 ms
15/05/13 10:28:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527280000 ms
15/05/13 10:28:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527280000 ms
15/05/13 10:28:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:29:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 10:29:00 INFO FileInputDStream: New files at time 1431527340000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527274188.json
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=19903015, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_237 stored as values in memory (estimated size 232.9 KB, free 246.2 MB)
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=20141547, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_237_piece0 stored as bytes in memory (estimated size 34.9 KB, free 246.2 MB)
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_237_piece0
15/05/13 10:29:00 INFO SparkContext: Created broadcast 237 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:29:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:29:00 INFO JobScheduler: Added jobs for time 1431527340000 ms
15/05/13 10:29:00 INFO JobGenerator: Checkpointing graph for time 1431527340000 ms
15/05/13 10:29:00 INFO DStreamGraph: Updating checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO DStreamGraph: Updated checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO JobScheduler: Starting job streaming job 1431527340000 ms.0 from job set of time 1431527340000 ms
15/05/13 10:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431527340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000'
15/05/13 10:29:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:29:00 INFO DAGScheduler: Got job 161 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:29:00 INFO DAGScheduler: Final stage: Stage 158(reduce at JsonRDD.scala:51)
15/05/13 10:29:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:29:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:29:00 INFO DAGScheduler: Submitting Stage 158 (MapPartitionsRDD[1130] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=20177255, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_238 stored as values in memory (estimated size 5.9 KB, free 246.2 MB)
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(4231) called with curMem=20183295, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_238_piece0 stored as bytes in memory (estimated size 4.1 KB, free 246.2 MB)
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:29:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83469): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_238_piece0
15/05/13 10:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431527340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000'
15/05/13 10:29:00 INFO SparkContext: Created broadcast 238 from broadcast at DAGScheduler.scala:839
15/05/13 10:29:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 158 (MapPartitionsRDD[1130] at map at JsonRDD.scala:51)
15/05/13 10:29:00 INFO TaskSchedulerImpl: Adding task set 158.0 with 1 tasks
15/05/13 10:29:00 INFO TaskSetManager: Starting task 0.0 in stage 158.0 (TID 158, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:29:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83471): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431527340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000'
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_238_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527040000.bk
15/05/13 10:29:00 INFO CheckpointWriter: Checkpoint for time 1431527340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000', took 7665 bytes and 138 ms
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO DAGScheduler: Stage 158 (reduce at JsonRDD.scala:51) finished in 0.247 s
15/05/13 10:29:00 INFO DAGScheduler: Job 161 finished: reduce at JsonRDD.scala:51, took 0.268814 s
15/05/13 10:29:00 INFO TaskSetManager: Finished task 0.0 in stage 158.0 (TID 158) in 237 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:29:00 INFO TaskSchedulerImpl: Removed TaskSet 158.0, whose tasks have all completed, from pool 
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 233
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_233
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_233 of size 20776 dropped from memory (free 258135806)
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_233_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_233_piece0 of size 10889 dropped from memory (free 258146695)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_233_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_233_piece0
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_233_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 233
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 232
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_232_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_232_piece0 of size 4226 dropped from memory (free 258150921)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_232_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_232_piece0
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_232
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_232 of size 6040 dropped from memory (free 258156961)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_232_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 232
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 230
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_230_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_230_piece0 of size 10949 dropped from memory (free 258167910)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_230_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_230_piece0
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_230
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_230 of size 20984 dropped from memory (free 258188894)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_230_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 230
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 236
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_236
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_236 of size 21072 dropped from memory (free 258209966)
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_236_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_236_piece0 of size 11013 dropped from memory (free 258220979)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_236_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_236_piece0
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_236_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 236
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 235
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_235
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_235 of size 6040 dropped from memory (free 258227019)
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_235_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_235_piece0 of size 4226 dropped from memory (free 258231245)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_235_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_235_piece0
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_235_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 235
15/05/13 10:29:00 INFO BlockManager: Removing broadcast 238
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_238_piece0
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_238_piece0 of size 4231 dropped from memory (free 258235476)
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_238_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_238_piece0
15/05/13 10:29:00 INFO BlockManager: Removing block broadcast_238
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_238 of size 6040 dropped from memory (free 258241516)
15/05/13 10:29:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:29:00 INFO DAGScheduler: Got job 162 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:29:00 INFO DAGScheduler: Final stage: Stage 159(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:29:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:29:00 INFO BlockManagerInfo: Removed broadcast_238_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:29:00 INFO ContextCleaner: Cleaned broadcast 238
15/05/13 10:29:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:29:00 INFO DAGScheduler: Submitting Stage 159 (MapPartitionsRDD[1137] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=20061040, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_239 stored as values in memory (estimated size 20.4 KB, free 246.3 MB)
15/05/13 10:29:00 INFO MemoryStore: ensureFreeSpace(10912) called with curMem=20081960, maxMem=278302556
15/05/13 10:29:00 INFO MemoryStore: Block broadcast_239_piece0 stored as bytes in memory (estimated size 10.7 KB, free 246.2 MB)
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 262.9 MB)
15/05/13 10:29:00 INFO BlockManagerMaster: Updated info of block broadcast_239_piece0
15/05/13 10:29:00 INFO SparkContext: Created broadcast 239 from broadcast at DAGScheduler.scala:839
15/05/13 10:29:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 159 (MapPartitionsRDD[1137] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:29:00 INFO TaskSchedulerImpl: Adding task set 159.0 with 1 tasks
15/05/13 10:29:00 INFO TaskSetManager: Starting task 0.0 in stage 159.0 (TID 159, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_239_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 264.5 MB)
15/05/13 10:29:00 INFO BlockManagerInfo: Added broadcast_237_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:29:00 INFO TaskSetManager: Finished task 0.0 in stage 159.0 (TID 159) in 176 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:29:00 INFO TaskSchedulerImpl: Removed TaskSet 159.0, whose tasks have all completed, from pool 
15/05/13 10:29:00 INFO DAGScheduler: Stage 159 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.178 s
15/05/13 10:29:00 INFO DAGScheduler: Job 162 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.196733 s
15/05/13 10:29:00 INFO JobScheduler: Finished job streaming job 1431527340000 ms.0 from job set of time 1431527340000 ms
15/05/13 10:29:00 INFO JobScheduler: Total delay: 0.877 s for time 1431527340000 ms (execution: 0.708 s)
15/05/13 10:29:00 INFO MapPartitionsRDD: Removing RDD 1112 from persistence list
15/05/13 10:29:00 INFO BlockManager: Removing RDD 1112
15/05/13 10:29:00 INFO UnionRDD: Removing RDD 1111 from persistence list
15/05/13 10:29:00 INFO BlockManager: Removing RDD 1111
15/05/13 10:29:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431527280000 ms: 1431527220000 ms
15/05/13 10:29:00 INFO JobGenerator: Checkpointing graph for time 1431527340000 ms
15/05/13 10:29:00 INFO DStreamGraph: Updating checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO DStreamGraph: Updated checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431527340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000'
15/05/13 10:29:00 INFO CheckpointWriter: Checkpoint for time 1431527340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000', took 7656 bytes and 36 ms
15/05/13 10:29:00 INFO DStreamGraph: Clearing checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO DStreamGraph: Cleared checkpoint data for time 1431527340000 ms
15/05/13 10:29:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:30:00 INFO FileInputDStream: Finding new files took 47 ms
15/05/13 10:30:00 INFO FileInputDStream: New files at time 1431527400000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527334650.json
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=20092872, maxMem=278302556
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_240 stored as values in memory (estimated size 232.9 KB, free 246.0 MB)
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=20331404, maxMem=278302556
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_240_piece0 stored as bytes in memory (estimated size 34.9 KB, free 246.0 MB)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.9 MB)
15/05/13 10:30:00 INFO BlockManagerMaster: Updated info of block broadcast_240_piece0
15/05/13 10:30:00 INFO SparkContext: Created broadcast 240 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:30:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:30:00 INFO JobScheduler: Added jobs for time 1431527400000 ms
15/05/13 10:30:00 INFO JobGenerator: Checkpointing graph for time 1431527400000 ms
15/05/13 10:30:00 INFO DStreamGraph: Updating checkpoint data for time 1431527400000 ms
15/05/13 10:30:00 INFO JobScheduler: Starting job streaming job 1431527400000 ms.0 from job set of time 1431527400000 ms
15/05/13 10:30:00 INFO DStreamGraph: Updated checkpoint data for time 1431527400000 ms
15/05/13 10:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431527400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527400000'
15/05/13 10:30:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:30:00 INFO DAGScheduler: Got job 163 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:30:00 INFO DAGScheduler: Final stage: Stage 160(reduce at JsonRDD.scala:51)
15/05/13 10:30:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:30:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:30:00 INFO DAGScheduler: Submitting Stage 160 (MapPartitionsRDD[1144] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=20367112, maxMem=278302556
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_241 stored as values in memory (estimated size 5.9 KB, free 246.0 MB)
15/05/13 10:30:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527100000.bk
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=20373152, maxMem=278302556
15/05/13 10:30:00 INFO CheckpointWriter: Checkpoint for time 1431527400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527400000', took 7666 bytes and 74 ms
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_241_piece0 stored as bytes in memory (estimated size 4.1 KB, free 246.0 MB)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.9 MB)
15/05/13 10:30:00 INFO BlockManagerMaster: Updated info of block broadcast_241_piece0
15/05/13 10:30:00 INFO SparkContext: Created broadcast 241 from broadcast at DAGScheduler.scala:839
15/05/13 10:30:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 160 (MapPartitionsRDD[1144] at map at JsonRDD.scala:51)
15/05/13 10:30:00 INFO TaskSchedulerImpl: Adding task set 160.0 with 1 tasks
15/05/13 10:30:00 INFO TaskSetManager: Starting task 0.0 in stage 160.0 (TID 160, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_241_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_240_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:30:00 INFO TaskSetManager: Finished task 0.0 in stage 160.0 (TID 160) in 345 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:30:00 INFO TaskSchedulerImpl: Removed TaskSet 160.0, whose tasks have all completed, from pool 
15/05/13 10:30:00 INFO DAGScheduler: Stage 160 (reduce at JsonRDD.scala:51) finished in 0.362 s
15/05/13 10:30:00 INFO DAGScheduler: Job 163 finished: reduce at JsonRDD.scala:51, took 0.396733 s
15/05/13 10:30:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:30:00 INFO DAGScheduler: Got job 164 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:30:00 INFO DAGScheduler: Final stage: Stage 161(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:30:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:30:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:30:00 INFO DAGScheduler: Submitting Stage 161 (MapPartitionsRDD[1151] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=20377379, maxMem=278302556
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_242 stored as values in memory (estimated size 20.4 KB, free 246.0 MB)
15/05/13 10:30:00 INFO MemoryStore: ensureFreeSpace(10909) called with curMem=20398299, maxMem=278302556
15/05/13 10:30:00 INFO MemoryStore: Block broadcast_242_piece0 stored as bytes in memory (estimated size 10.7 KB, free 245.9 MB)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 262.9 MB)
15/05/13 10:30:00 INFO BlockManagerMaster: Updated info of block broadcast_242_piece0
15/05/13 10:30:00 INFO SparkContext: Created broadcast 242 from broadcast at DAGScheduler.scala:839
15/05/13 10:30:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 161 (MapPartitionsRDD[1151] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:30:00 INFO TaskSchedulerImpl: Adding task set 161.0 with 1 tasks
15/05/13 10:30:00 INFO TaskSetManager: Starting task 0.0 in stage 161.0 (TID 161, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:30:00 INFO BlockManagerInfo: Added broadcast_242_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.4 MB)
15/05/13 10:30:01 INFO TaskSetManager: Finished task 0.0 in stage 161.0 (TID 161) in 148 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:30:01 INFO TaskSchedulerImpl: Removed TaskSet 161.0, whose tasks have all completed, from pool 
15/05/13 10:30:01 INFO DAGScheduler: Stage 161 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.151 s
15/05/13 10:30:01 INFO DAGScheduler: Job 164 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.185671 s
15/05/13 10:30:01 INFO JobScheduler: Finished job streaming job 1431527400000 ms.0 from job set of time 1431527400000 ms
15/05/13 10:30:01 INFO JobScheduler: Total delay: 1.069 s for time 1431527400000 ms (execution: 0.929 s)
15/05/13 10:30:01 INFO MapPartitionsRDD: Removing RDD 1126 from persistence list
15/05/13 10:30:01 INFO BlockManager: Removing RDD 1126
15/05/13 10:30:01 INFO UnionRDD: Removing RDD 1125 from persistence list
15/05/13 10:30:01 INFO BlockManager: Removing RDD 1125
15/05/13 10:30:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527340000 ms: 1431527280000 ms
15/05/13 10:30:01 INFO JobGenerator: Checkpointing graph for time 1431527400000 ms
15/05/13 10:30:01 INFO DStreamGraph: Updating checkpoint data for time 1431527400000 ms
15/05/13 10:30:01 INFO DStreamGraph: Updated checkpoint data for time 1431527400000 ms
15/05/13 10:30:01 INFO CheckpointWriter: Saving checkpoint for time 1431527400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527400000'
15/05/13 10:30:01 INFO CheckpointWriter: Checkpoint for time 1431527400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527400000', took 7655 bytes and 70 ms
15/05/13 10:30:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527400000 ms
15/05/13 10:30:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527400000 ms
15/05/13 10:30:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:31:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 10:31:00 INFO FileInputDStream: New files at time 1431527460000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527395945.json
15/05/13 10:31:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=20409208, maxMem=278302556
15/05/13 10:31:00 INFO MemoryStore: Block broadcast_243 stored as values in memory (estimated size 232.9 KB, free 245.7 MB)
15/05/13 10:31:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=20647740, maxMem=278302556
15/05/13 10:31:00 INFO MemoryStore: Block broadcast_243_piece0 stored as bytes in memory (estimated size 34.9 KB, free 245.7 MB)
15/05/13 10:31:00 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.8 MB)
15/05/13 10:31:00 INFO BlockManagerMaster: Updated info of block broadcast_243_piece0
15/05/13 10:31:00 INFO SparkContext: Created broadcast 243 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:31:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:31:00 INFO JobScheduler: Added jobs for time 1431527460000 ms
15/05/13 10:31:00 INFO JobScheduler: Starting job streaming job 1431527460000 ms.0 from job set of time 1431527460000 ms
15/05/13 10:31:00 INFO JobGenerator: Checkpointing graph for time 1431527460000 ms
15/05/13 10:31:00 INFO DStreamGraph: Updating checkpoint data for time 1431527460000 ms
15/05/13 10:31:00 INFO DStreamGraph: Updated checkpoint data for time 1431527460000 ms
15/05/13 10:31:00 INFO CheckpointWriter: Saving checkpoint for time 1431527460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527460000'
15/05/13 10:31:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527160000
15/05/13 10:31:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:31:00 INFO CheckpointWriter: Checkpoint for time 1431527460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527460000', took 7662 bytes and 90 ms
15/05/13 10:31:00 INFO DAGScheduler: Got job 165 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:31:00 INFO DAGScheduler: Final stage: Stage 162(reduce at JsonRDD.scala:51)
15/05/13 10:31:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:31:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:31:00 INFO DAGScheduler: Submitting Stage 162 (MapPartitionsRDD[1158] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:31:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=20683448, maxMem=278302556
15/05/13 10:31:00 INFO MemoryStore: Block broadcast_244 stored as values in memory (estimated size 5.9 KB, free 245.7 MB)
15/05/13 10:31:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=20689488, maxMem=278302556
15/05/13 10:31:00 INFO MemoryStore: Block broadcast_244_piece0 stored as bytes in memory (estimated size 4.1 KB, free 245.7 MB)
15/05/13 10:31:00 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:31:00 INFO BlockManagerMaster: Updated info of block broadcast_244_piece0
15/05/13 10:31:00 INFO SparkContext: Created broadcast 244 from broadcast at DAGScheduler.scala:839
15/05/13 10:31:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 162 (MapPartitionsRDD[1158] at map at JsonRDD.scala:51)
15/05/13 10:31:00 INFO TaskSchedulerImpl: Adding task set 162.0 with 1 tasks
15/05/13 10:31:00 INFO TaskSetManager: Starting task 0.0 in stage 162.0 (TID 162, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:31:00 INFO BlockManagerInfo: Added broadcast_244_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.6 MB)
15/05/13 10:31:00 INFO BlockManagerInfo: Added broadcast_243_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:31:00 INFO TaskSetManager: Finished task 0.0 in stage 162.0 (TID 162) in 384 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:31:00 INFO TaskSchedulerImpl: Removed TaskSet 162.0, whose tasks have all completed, from pool 
15/05/13 10:31:00 INFO DAGScheduler: Stage 162 (reduce at JsonRDD.scala:51) finished in 0.406 s
15/05/13 10:31:00 INFO DAGScheduler: Job 165 finished: reduce at JsonRDD.scala:51, took 0.454865 s
15/05/13 10:31:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:31:01 INFO DAGScheduler: Got job 166 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:31:01 INFO DAGScheduler: Final stage: Stage 163(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:31:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:31:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:31:01 INFO DAGScheduler: Submitting Stage 163 (MapPartitionsRDD[1165] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:31:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=20693716, maxMem=278302556
15/05/13 10:31:01 INFO MemoryStore: Block broadcast_245 stored as values in memory (estimated size 20.4 KB, free 245.7 MB)
15/05/13 10:31:01 INFO MemoryStore: ensureFreeSpace(10904) called with curMem=20714636, maxMem=278302556
15/05/13 10:31:01 INFO MemoryStore: Block broadcast_245_piece0 stored as bytes in memory (estimated size 10.6 KB, free 245.6 MB)
15/05/13 10:31:01 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.8 MB)
15/05/13 10:31:01 INFO BlockManagerMaster: Updated info of block broadcast_245_piece0
15/05/13 10:31:01 INFO SparkContext: Created broadcast 245 from broadcast at DAGScheduler.scala:839
15/05/13 10:31:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 163 (MapPartitionsRDD[1165] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:31:01 INFO TaskSchedulerImpl: Adding task set 163.0 with 1 tasks
15/05/13 10:31:01 INFO TaskSetManager: Starting task 0.0 in stage 163.0 (TID 163, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:31:01 INFO BlockManagerInfo: Added broadcast_245_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.5 MB)
15/05/13 10:31:01 INFO TaskSetManager: Finished task 0.0 in stage 163.0 (TID 163) in 130 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:31:01 INFO TaskSchedulerImpl: Removed TaskSet 163.0, whose tasks have all completed, from pool 
15/05/13 10:31:01 INFO DAGScheduler: Stage 163 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.132 s
15/05/13 10:31:01 INFO DAGScheduler: Job 166 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.161838 s
15/05/13 10:31:01 INFO JobScheduler: Finished job streaming job 1431527460000 ms.0 from job set of time 1431527460000 ms
15/05/13 10:31:01 INFO JobScheduler: Total delay: 1.275 s for time 1431527460000 ms (execution: 1.045 s)
15/05/13 10:31:01 INFO MapPartitionsRDD: Removing RDD 1140 from persistence list
15/05/13 10:31:01 INFO BlockManager: Removing RDD 1140
15/05/13 10:31:01 INFO UnionRDD: Removing RDD 1139 from persistence list
15/05/13 10:31:01 INFO BlockManager: Removing RDD 1139
15/05/13 10:31:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527400000 ms: 1431527340000 ms
15/05/13 10:31:01 INFO JobGenerator: Checkpointing graph for time 1431527460000 ms
15/05/13 10:31:01 INFO DStreamGraph: Updating checkpoint data for time 1431527460000 ms
15/05/13 10:31:01 INFO DStreamGraph: Updated checkpoint data for time 1431527460000 ms
15/05/13 10:31:01 INFO CheckpointWriter: Saving checkpoint for time 1431527460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527460000'
15/05/13 10:31:01 INFO CheckpointWriter: Checkpoint for time 1431527460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527460000', took 7654 bytes and 57 ms
15/05/13 10:31:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527460000 ms
15/05/13 10:31:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527460000 ms
15/05/13 10:31:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:32:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 10:32:00 INFO FileInputDStream: New files at time 1431527520000 ms:

15/05/13 10:32:00 INFO JobScheduler: Added jobs for time 1431527520000 ms
15/05/13 10:32:00 INFO JobGenerator: Checkpointing graph for time 1431527520000 ms
15/05/13 10:32:00 INFO DStreamGraph: Updating checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 INFO JobScheduler: Starting job streaming job 1431527520000 ms.0 from job set of time 1431527520000 ms
15/05/13 10:32:00 INFO DStreamGraph: Updated checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431527520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000'
15/05/13 10:32:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:32:00 INFO DAGScheduler: Job 167 finished: reduce at JsonRDD.scala:51, took 0.000158 s
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 10:32:00 INFO JobScheduler: Finished job streaming job 1431527520000 ms.0 from job set of time 1431527520000 ms
15/05/13 10:32:00 INFO JobScheduler: Total delay: 0.139 s for time 1431527520000 ms (execution: 0.071 s)
15/05/13 10:32:00 INFO MapPartitionsRDD: Removing RDD 1154 from persistence list
15/05/13 10:32:00 INFO BlockManager: Removing RDD 1154
15/05/13 10:32:00 INFO UnionRDD: Removing RDD 1153 from persistence list
15/05/13 10:32:00 INFO BlockManager: Removing RDD 1153
15/05/13 10:32:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431527460000 ms: 1431527400000 ms
15/05/13 10:32:00 INFO JobGenerator: Checkpointing graph for time 1431527520000 ms
15/05/13 10:32:00 INFO DStreamGraph: Updating checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 INFO DStreamGraph: Updated checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83495): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431527520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000'
15/05/13 10:32:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83497): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431527520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000'
15/05/13 10:32:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527220000
15/05/13 10:32:00 INFO CheckpointWriter: Checkpoint for time 1431527520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000', took 7644 bytes and 205 ms
15/05/13 10:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431527520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000'
15/05/13 10:32:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000
15/05/13 10:32:00 INFO CheckpointWriter: Checkpoint for time 1431527520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527520000', took 7644 bytes and 58 ms
15/05/13 10:32:00 INFO DStreamGraph: Clearing checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 INFO DStreamGraph: Cleared checkpoint data for time 1431527520000 ms
15/05/13 10:32:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:33:00 INFO FileInputDStream: Finding new files took 25 ms
15/05/13 10:33:00 INFO FileInputDStream: New files at time 1431527580000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527462288.json
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=20725540, maxMem=278302556
15/05/13 10:33:00 INFO MemoryStore: Block broadcast_246 stored as values in memory (estimated size 232.9 KB, free 245.4 MB)
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=20964072, maxMem=278302556
15/05/13 10:33:00 INFO MemoryStore: Block broadcast_246_piece0 stored as bytes in memory (estimated size 34.9 KB, free 245.4 MB)
15/05/13 10:33:00 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.8 MB)
15/05/13 10:33:00 INFO BlockManagerMaster: Updated info of block broadcast_246_piece0
15/05/13 10:33:00 INFO SparkContext: Created broadcast 246 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:33:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:33:00 INFO JobScheduler: Added jobs for time 1431527580000 ms
15/05/13 10:33:00 INFO JobScheduler: Starting job streaming job 1431527580000 ms.0 from job set of time 1431527580000 ms
15/05/13 10:33:00 INFO JobGenerator: Checkpointing graph for time 1431527580000 ms
15/05/13 10:33:00 INFO DStreamGraph: Updating checkpoint data for time 1431527580000 ms
15/05/13 10:33:00 INFO DStreamGraph: Updated checkpoint data for time 1431527580000 ms
15/05/13 10:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431527580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000'
15/05/13 10:33:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83504): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431527580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000'
15/05/13 10:33:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:33:00 INFO DAGScheduler: Got job 168 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:33:00 INFO DAGScheduler: Final stage: Stage 164(reduce at JsonRDD.scala:51)
15/05/13 10:33:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:33:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:33:00 INFO DAGScheduler: Submitting Stage 164 (MapPartitionsRDD[1178] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=20999780, maxMem=278302556
15/05/13 10:33:00 INFO MemoryStore: Block broadcast_247 stored as values in memory (estimated size 5.9 KB, free 245.4 MB)
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(4229) called with curMem=21005820, maxMem=278302556
15/05/13 10:33:00 INFO MemoryStore: Block broadcast_247_piece0 stored as bytes in memory (estimated size 4.1 KB, free 245.4 MB)
15/05/13 10:33:00 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:33:00 INFO BlockManagerMaster: Updated info of block broadcast_247_piece0
15/05/13 10:33:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83506): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431527580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000'
15/05/13 10:33:00 INFO SparkContext: Created broadcast 247 from broadcast at DAGScheduler.scala:839
15/05/13 10:33:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 164 (MapPartitionsRDD[1178] at map at JsonRDD.scala:51)
15/05/13 10:33:00 INFO TaskSchedulerImpl: Adding task set 164.0 with 1 tasks
15/05/13 10:33:00 INFO TaskSetManager: Starting task 0.0 in stage 164.0 (TID 164, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:33:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83508): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:33:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527580000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000'
15/05/13 10:33:00 INFO BlockManagerInfo: Added broadcast_247_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:33:00 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:33:00 INFO TaskSetManager: Finished task 0.0 in stage 164.0 (TID 164) in 464 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:33:00 INFO TaskSchedulerImpl: Removed TaskSet 164.0, whose tasks have all completed, from pool 
15/05/13 10:33:00 INFO DAGScheduler: Stage 164 (reduce at JsonRDD.scala:51) finished in 0.473 s
15/05/13 10:33:00 INFO DAGScheduler: Job 168 finished: reduce at JsonRDD.scala:51, took 0.524866 s
15/05/13 10:33:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:33:00 INFO DAGScheduler: Got job 169 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:33:00 INFO DAGScheduler: Final stage: Stage 165(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:33:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:33:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:33:00 INFO DAGScheduler: Submitting Stage 165 (MapPartitionsRDD[1185] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=21010049, maxMem=278302556
15/05/13 10:33:00 INFO MemoryStore: Block broadcast_248 stored as values in memory (estimated size 20.4 KB, free 245.4 MB)
15/05/13 10:33:00 INFO MemoryStore: ensureFreeSpace(10838) called with curMem=21030969, maxMem=278302556
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_248_piece0 stored as bytes in memory (estimated size 10.6 KB, free 245.3 MB)
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 247
15/05/13 10:33:01 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_247
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_247 of size 6040 dropped from memory (free 257266789)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_247_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_247_piece0 of size 4229 dropped from memory (free 257271018)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_248_piece0
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_247_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_247_piece0
15/05/13 10:33:01 INFO SparkContext: Created broadcast 248 from broadcast at DAGScheduler.scala:839
15/05/13 10:33:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 165 (MapPartitionsRDD[1185] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:33:01 INFO TaskSchedulerImpl: Adding task set 165.0 with 1 tasks
15/05/13 10:33:01 INFO TaskSetManager: Starting task 0.0 in stage 165.0 (TID 165, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_247_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 247
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 242
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_242_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_242_piece0 of size 10909 dropped from memory (free 257281927)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_242_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_242_piece0
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_242
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_242 of size 20920 dropped from memory (free 257302847)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_242_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 263.4 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 242
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 244
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_244
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_244 of size 6040 dropped from memory (free 257308887)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_244_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_244_piece0 of size 4228 dropped from memory (free 257313115)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_244_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_244_piece0
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_244_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:33:01 INFO BlockManagerInfo: Added broadcast_248_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.5 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 244
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 245
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_245
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_245 of size 20920 dropped from memory (free 257334035)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_245_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_245_piece0 of size 10904 dropped from memory (free 257344939)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_245_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_245_piece0
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_245_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.5 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 245
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 239
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_239_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_239_piece0 of size 10912 dropped from memory (free 257355851)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_239_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_239_piece0
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_239
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_239 of size 20920 dropped from memory (free 257376771)
15/05/13 10:33:01 INFO BlockManagerInfo: Added broadcast_246_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_239_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 264.5 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 239
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 240
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_240
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_240 of size 238532 dropped from memory (free 257615303)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_240_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_240_piece0 of size 35708 dropped from memory (free 257651011)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_240_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_240_piece0
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_240_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 240
15/05/13 10:33:01 INFO BlockManager: Removing broadcast 241
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_241
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_241 of size 6040 dropped from memory (free 257657051)
15/05/13 10:33:01 INFO BlockManager: Removing block broadcast_241_piece0
15/05/13 10:33:01 INFO MemoryStore: Block broadcast_241_piece0 of size 4227 dropped from memory (free 257661278)
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_241_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:33:01 INFO BlockManagerMaster: Updated info of block broadcast_241_piece0
15/05/13 10:33:01 INFO BlockManagerInfo: Removed broadcast_241_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:33:01 INFO ContextCleaner: Cleaned broadcast 241
15/05/13 10:33:01 INFO TaskSetManager: Finished task 0.0 in stage 165.0 (TID 165) in 195 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:33:01 INFO TaskSchedulerImpl: Removed TaskSet 165.0, whose tasks have all completed, from pool 
15/05/13 10:33:01 INFO DAGScheduler: Stage 165 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.197 s
15/05/13 10:33:01 INFO DAGScheduler: Job 169 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.327696 s
15/05/13 10:33:01 INFO JobScheduler: Finished job streaming job 1431527580000 ms.0 from job set of time 1431527580000 ms
15/05/13 10:33:01 INFO MapPartitionsRDD: Removing RDD 1167 from persistence list
15/05/13 10:33:01 INFO JobScheduler: Total delay: 1.260 s for time 1431527580000 ms (execution: 1.133 s)
15/05/13 10:33:01 INFO BlockManager: Removing RDD 1167
15/05/13 10:33:01 INFO UnionRDD: Removing RDD 1166 from persistence list
15/05/13 10:33:01 INFO BlockManager: Removing RDD 1166
15/05/13 10:33:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527520000 ms: 1431527460000 ms
15/05/13 10:33:01 INFO JobGenerator: Checkpointing graph for time 1431527580000 ms
15/05/13 10:33:01 INFO DStreamGraph: Updating checkpoint data for time 1431527580000 ms
15/05/13 10:33:01 INFO DStreamGraph: Updated checkpoint data for time 1431527580000 ms
15/05/13 10:33:01 INFO CheckpointWriter: Saving checkpoint for time 1431527580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000'
15/05/13 10:33:01 INFO CheckpointWriter: Checkpoint for time 1431527580000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000', took 7647 bytes and 52 ms
15/05/13 10:33:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527580000 ms
15/05/13 10:33:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527580000 ms
15/05/13 10:33:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:34:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 10:34:00 INFO FileInputDStream: New files at time 1431527640000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527524351.json
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=20641278, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_249 stored as values in memory (estimated size 232.9 KB, free 245.5 MB)
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=20879810, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_249_piece0 stored as bytes in memory (estimated size 34.9 KB, free 245.5 MB)
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.8 MB)
15/05/13 10:34:00 INFO BlockManagerMaster: Updated info of block broadcast_249_piece0
15/05/13 10:34:00 INFO SparkContext: Created broadcast 249 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:34:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:34:00 INFO JobScheduler: Added jobs for time 1431527640000 ms
15/05/13 10:34:00 INFO JobGenerator: Checkpointing graph for time 1431527640000 ms
15/05/13 10:34:00 INFO JobScheduler: Starting job streaming job 1431527640000 ms.0 from job set of time 1431527640000 ms
15/05/13 10:34:00 INFO DStreamGraph: Updating checkpoint data for time 1431527640000 ms
15/05/13 10:34:00 INFO DStreamGraph: Updated checkpoint data for time 1431527640000 ms
15/05/13 10:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431527640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000'
15/05/13 10:34:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83514): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431527640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000'
15/05/13 10:34:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:34:00 INFO DAGScheduler: Got job 170 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:34:00 INFO DAGScheduler: Final stage: Stage 166(reduce at JsonRDD.scala:51)
15/05/13 10:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:34:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83516): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431527640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000'
15/05/13 10:34:00 INFO DAGScheduler: Submitting Stage 166 (MapPartitionsRDD[1192] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=20915518, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_250 stored as values in memory (estimated size 5.9 KB, free 245.5 MB)
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=20921558, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_250_piece0 stored as bytes in memory (estimated size 4.1 KB, free 245.5 MB)
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.8 MB)
15/05/13 10:34:00 INFO BlockManagerMaster: Updated info of block broadcast_250_piece0
15/05/13 10:34:00 INFO SparkContext: Created broadcast 250 from broadcast at DAGScheduler.scala:839
15/05/13 10:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 166 (MapPartitionsRDD[1192] at map at JsonRDD.scala:51)
15/05/13 10:34:00 INFO TaskSchedulerImpl: Adding task set 166.0 with 1 tasks
15/05/13 10:34:00 INFO TaskSetManager: Starting task 0.0 in stage 166.0 (TID 166, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_250_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:34:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83518): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:34:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527640000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000'
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_249_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:34:00 INFO TaskSetManager: Finished task 0.0 in stage 166.0 (TID 166) in 194 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:34:00 INFO TaskSchedulerImpl: Removed TaskSet 166.0, whose tasks have all completed, from pool 
15/05/13 10:34:00 INFO DAGScheduler: Stage 166 (reduce at JsonRDD.scala:51) finished in 0.204 s
15/05/13 10:34:00 INFO DAGScheduler: Job 170 finished: reduce at JsonRDD.scala:51, took 0.249729 s
15/05/13 10:34:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:34:00 INFO DAGScheduler: Got job 171 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:34:00 INFO DAGScheduler: Final stage: Stage 167(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:34:00 INFO DAGScheduler: Submitting Stage 167 (MapPartitionsRDD[1199] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=20925785, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_251 stored as values in memory (estimated size 20.4 KB, free 245.4 MB)
15/05/13 10:34:00 INFO MemoryStore: ensureFreeSpace(10895) called with curMem=20946705, maxMem=278302556
15/05/13 10:34:00 INFO MemoryStore: Block broadcast_251_piece0 stored as bytes in memory (estimated size 10.6 KB, free 245.4 MB)
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.8 MB)
15/05/13 10:34:00 INFO BlockManagerMaster: Updated info of block broadcast_251_piece0
15/05/13 10:34:00 INFO SparkContext: Created broadcast 251 from broadcast at DAGScheduler.scala:839
15/05/13 10:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 167 (MapPartitionsRDD[1199] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:34:00 INFO TaskSchedulerImpl: Adding task set 167.0 with 1 tasks
15/05/13 10:34:00 INFO TaskSetManager: Starting task 0.0 in stage 167.0 (TID 167, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:34:00 INFO BlockManagerInfo: Added broadcast_251_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.4 MB)
15/05/13 10:34:00 INFO TaskSetManager: Finished task 0.0 in stage 167.0 (TID 167) in 110 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:34:00 INFO TaskSchedulerImpl: Removed TaskSet 167.0, whose tasks have all completed, from pool 
15/05/13 10:34:00 INFO DAGScheduler: Stage 167 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.111 s
15/05/13 10:34:00 INFO DAGScheduler: Job 171 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.147328 s
15/05/13 10:34:00 INFO JobScheduler: Finished job streaming job 1431527640000 ms.0 from job set of time 1431527640000 ms
15/05/13 10:34:00 INFO MapPartitionsRDD: Removing RDD 1174 from persistence list
15/05/13 10:34:00 INFO JobScheduler: Total delay: 0.958 s for time 1431527640000 ms (execution: 0.771 s)
15/05/13 10:34:00 INFO BlockManager: Removing RDD 1174
15/05/13 10:34:00 INFO UnionRDD: Removing RDD 1173 from persistence list
15/05/13 10:34:00 INFO BlockManager: Removing RDD 1173
15/05/13 10:34:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431527580000 ms: 1431527520000 ms
15/05/13 10:34:00 INFO JobGenerator: Checkpointing graph for time 1431527640000 ms
15/05/13 10:34:00 INFO DStreamGraph: Updating checkpoint data for time 1431527640000 ms
15/05/13 10:34:00 INFO DStreamGraph: Updated checkpoint data for time 1431527640000 ms
15/05/13 10:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431527640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000'
15/05/13 10:34:01 INFO CheckpointWriter: Checkpoint for time 1431527640000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000', took 7657 bytes and 62 ms
15/05/13 10:34:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527640000 ms
15/05/13 10:34:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527640000 ms
15/05/13 10:34:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:35:00 INFO FileInputDStream: Finding new files took 20 ms
15/05/13 10:35:00 INFO FileInputDStream: New files at time 1431527700000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527586152.json
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=20957600, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_252 stored as values in memory (estimated size 232.9 KB, free 245.2 MB)
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=21196132, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_252_piece0 stored as bytes in memory (estimated size 34.9 KB, free 245.2 MB)
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.7 MB)
15/05/13 10:35:00 INFO BlockManagerMaster: Updated info of block broadcast_252_piece0
15/05/13 10:35:00 INFO SparkContext: Created broadcast 252 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:35:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:35:00 INFO JobScheduler: Added jobs for time 1431527700000 ms
15/05/13 10:35:00 INFO JobGenerator: Checkpointing graph for time 1431527700000 ms
15/05/13 10:35:00 INFO JobScheduler: Starting job streaming job 1431527700000 ms.0 from job set of time 1431527700000 ms
15/05/13 10:35:00 INFO DStreamGraph: Updating checkpoint data for time 1431527700000 ms
15/05/13 10:35:00 INFO DStreamGraph: Updated checkpoint data for time 1431527700000 ms
15/05/13 10:35:00 INFO CheckpointWriter: Saving checkpoint for time 1431527700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000'
15/05/13 10:35:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:35:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83523): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:35:00 INFO DAGScheduler: Got job 172 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:35:00 INFO CheckpointWriter: Saving checkpoint for time 1431527700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000'
15/05/13 10:35:00 INFO DAGScheduler: Final stage: Stage 168(reduce at JsonRDD.scala:51)
15/05/13 10:35:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:35:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:35:00 INFO DAGScheduler: Submitting Stage 168 (MapPartitionsRDD[1206] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=21231840, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_253 stored as values in memory (estimated size 5.9 KB, free 245.2 MB)
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=21237880, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_253_piece0 stored as bytes in memory (estimated size 4.1 KB, free 245.2 MB)
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:35:00 INFO BlockManagerMaster: Updated info of block broadcast_253_piece0
15/05/13 10:35:00 INFO SparkContext: Created broadcast 253 from broadcast at DAGScheduler.scala:839
15/05/13 10:35:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 168 (MapPartitionsRDD[1206] at map at JsonRDD.scala:51)
15/05/13 10:35:00 INFO TaskSchedulerImpl: Adding task set 168.0 with 1 tasks
15/05/13 10:35:00 INFO TaskSetManager: Starting task 0.0 in stage 168.0 (TID 168, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:35:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527340000
15/05/13 10:35:00 INFO CheckpointWriter: Checkpoint for time 1431527700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000', took 7663 bytes and 83 ms
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_253_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_252_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:35:00 INFO TaskSetManager: Finished task 0.0 in stage 168.0 (TID 168) in 503 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:35:00 INFO TaskSchedulerImpl: Removed TaskSet 168.0, whose tasks have all completed, from pool 
15/05/13 10:35:00 INFO DAGScheduler: Stage 168 (reduce at JsonRDD.scala:51) finished in 0.514 s
15/05/13 10:35:00 INFO DAGScheduler: Job 172 finished: reduce at JsonRDD.scala:51, took 0.537434 s
15/05/13 10:35:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:35:00 INFO DAGScheduler: Got job 173 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:35:00 INFO DAGScheduler: Final stage: Stage 169(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:35:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:35:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:35:00 INFO DAGScheduler: Submitting Stage 169 (MapPartitionsRDD[1213] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=21242107, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_254 stored as values in memory (estimated size 20.0 KB, free 245.1 MB)
15/05/13 10:35:00 INFO MemoryStore: ensureFreeSpace(10758) called with curMem=21262595, maxMem=278302556
15/05/13 10:35:00 INFO MemoryStore: Block broadcast_254_piece0 stored as bytes in memory (estimated size 10.5 KB, free 245.1 MB)
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 262.7 MB)
15/05/13 10:35:00 INFO BlockManagerMaster: Updated info of block broadcast_254_piece0
15/05/13 10:35:00 INFO SparkContext: Created broadcast 254 from broadcast at DAGScheduler.scala:839
15/05/13 10:35:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 169 (MapPartitionsRDD[1213] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:35:00 INFO TaskSchedulerImpl: Adding task set 169.0 with 1 tasks
15/05/13 10:35:00 INFO TaskSetManager: Starting task 0.0 in stage 169.0 (TID 169, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:35:00 INFO BlockManagerInfo: Added broadcast_254_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 264.5 MB)
15/05/13 10:35:01 INFO TaskSetManager: Finished task 0.0 in stage 169.0 (TID 169) in 163 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:35:01 INFO TaskSchedulerImpl: Removed TaskSet 169.0, whose tasks have all completed, from pool 
15/05/13 10:35:01 INFO DAGScheduler: Stage 169 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.165 s
15/05/13 10:35:01 INFO DAGScheduler: Job 173 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.208100 s
15/05/13 10:35:01 INFO JobScheduler: Finished job streaming job 1431527700000 ms.0 from job set of time 1431527700000 ms
15/05/13 10:35:01 INFO JobScheduler: Total delay: 1.157 s for time 1431527700000 ms (execution: 1.071 s)
15/05/13 10:35:01 INFO MapPartitionsRDD: Removing RDD 1188 from persistence list
15/05/13 10:35:01 INFO BlockManager: Removing RDD 1188
15/05/13 10:35:01 INFO UnionRDD: Removing RDD 1187 from persistence list
15/05/13 10:35:01 INFO BlockManager: Removing RDD 1187
15/05/13 10:35:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527640000 ms: 1431527580000 ms
15/05/13 10:35:01 INFO JobGenerator: Checkpointing graph for time 1431527700000 ms
15/05/13 10:35:01 INFO DStreamGraph: Updating checkpoint data for time 1431527700000 ms
15/05/13 10:35:01 INFO DStreamGraph: Updated checkpoint data for time 1431527700000 ms
15/05/13 10:35:01 INFO CheckpointWriter: Saving checkpoint for time 1431527700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000'
15/05/13 10:35:01 INFO CheckpointWriter: Checkpoint for time 1431527700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527700000', took 7654 bytes and 106 ms
15/05/13 10:35:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527700000 ms
15/05/13 10:35:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527700000 ms
15/05/13 10:35:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:36:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 10:36:00 INFO FileInputDStream: New files at time 1431527760000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527649378.json
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=21273353, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_255 stored as values in memory (estimated size 232.9 KB, free 244.9 MB)
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=21511885, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_255_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.9 MB)
15/05/13 10:36:00 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.7 MB)
15/05/13 10:36:00 INFO BlockManagerMaster: Updated info of block broadcast_255_piece0
15/05/13 10:36:00 INFO SparkContext: Created broadcast 255 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:36:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:36:00 INFO JobScheduler: Added jobs for time 1431527760000 ms
15/05/13 10:36:00 INFO JobGenerator: Checkpointing graph for time 1431527760000 ms
15/05/13 10:36:00 INFO JobScheduler: Starting job streaming job 1431527760000 ms.0 from job set of time 1431527760000 ms
15/05/13 10:36:00 INFO DStreamGraph: Updating checkpoint data for time 1431527760000 ms
15/05/13 10:36:00 INFO DStreamGraph: Updated checkpoint data for time 1431527760000 ms
15/05/13 10:36:00 INFO CheckpointWriter: Saving checkpoint for time 1431527760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000'
15/05/13 10:36:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83531): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:36:00 INFO CheckpointWriter: Saving checkpoint for time 1431527760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000'
15/05/13 10:36:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:36:00 INFO DAGScheduler: Got job 174 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:36:00 INFO DAGScheduler: Final stage: Stage 170(reduce at JsonRDD.scala:51)
15/05/13 10:36:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:36:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:36:00 INFO DAGScheduler: Submitting Stage 170 (MapPartitionsRDD[1220] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=21547593, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_256 stored as values in memory (estimated size 5.9 KB, free 244.9 MB)
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=21553633, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_256_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.9 MB)
15/05/13 10:36:00 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:36:00 INFO BlockManagerMaster: Updated info of block broadcast_256_piece0
15/05/13 10:36:00 INFO SparkContext: Created broadcast 256 from broadcast at DAGScheduler.scala:839
15/05/13 10:36:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 170 (MapPartitionsRDD[1220] at map at JsonRDD.scala:51)
15/05/13 10:36:00 INFO TaskSchedulerImpl: Adding task set 170.0 with 1 tasks
15/05/13 10:36:00 INFO TaskSetManager: Starting task 0.0 in stage 170.0 (TID 170, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:36:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527400000
15/05/13 10:36:00 INFO CheckpointWriter: Checkpoint for time 1431527760000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000', took 7662 bytes and 132 ms
15/05/13 10:36:00 INFO BlockManagerInfo: Added broadcast_256_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:36:00 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.4 MB)
15/05/13 10:36:00 INFO TaskSetManager: Finished task 0.0 in stage 170.0 (TID 170) in 379 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:36:00 INFO TaskSchedulerImpl: Removed TaskSet 170.0, whose tasks have all completed, from pool 
15/05/13 10:36:00 INFO DAGScheduler: Stage 170 (reduce at JsonRDD.scala:51) finished in 0.388 s
15/05/13 10:36:00 INFO DAGScheduler: Job 174 finished: reduce at JsonRDD.scala:51, took 0.419664 s
15/05/13 10:36:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:36:00 INFO DAGScheduler: Got job 175 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:36:00 INFO DAGScheduler: Final stage: Stage 171(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:36:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:36:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:36:00 INFO DAGScheduler: Submitting Stage 171 (MapPartitionsRDD[1227] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=21557860, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_257 stored as values in memory (estimated size 20.0 KB, free 244.8 MB)
15/05/13 10:36:00 INFO MemoryStore: ensureFreeSpace(10767) called with curMem=21578348, maxMem=278302556
15/05/13 10:36:00 INFO MemoryStore: Block broadcast_257_piece0 stored as bytes in memory (estimated size 10.5 KB, free 244.8 MB)
15/05/13 10:36:00 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 262.7 MB)
15/05/13 10:36:00 INFO BlockManagerMaster: Updated info of block broadcast_257_piece0
15/05/13 10:36:00 INFO SparkContext: Created broadcast 257 from broadcast at DAGScheduler.scala:839
15/05/13 10:36:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 171 (MapPartitionsRDD[1227] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:36:00 INFO TaskSchedulerImpl: Adding task set 171.0 with 1 tasks
15/05/13 10:36:00 INFO TaskSetManager: Starting task 0.0 in stage 171.0 (TID 171, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:36:01 INFO BlockManagerInfo: Added broadcast_257_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.5 KB, free: 264.4 MB)
15/05/13 10:36:01 INFO BlockManagerInfo: Added broadcast_255_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:36:01 INFO TaskSetManager: Finished task 0.0 in stage 171.0 (TID 171) in 171 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:36:01 INFO TaskSchedulerImpl: Removed TaskSet 171.0, whose tasks have all completed, from pool 
15/05/13 10:36:01 INFO DAGScheduler: Stage 171 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.173 s
15/05/13 10:36:01 INFO DAGScheduler: Job 175 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.206012 s
15/05/13 10:36:01 INFO JobScheduler: Finished job streaming job 1431527760000 ms.0 from job set of time 1431527760000 ms
15/05/13 10:36:01 INFO JobScheduler: Total delay: 1.221 s for time 1431527760000 ms (execution: 1.016 s)
15/05/13 10:36:01 INFO MapPartitionsRDD: Removing RDD 1202 from persistence list
15/05/13 10:36:01 INFO BlockManager: Removing RDD 1202
15/05/13 10:36:01 INFO UnionRDD: Removing RDD 1201 from persistence list
15/05/13 10:36:01 INFO BlockManager: Removing RDD 1201
15/05/13 10:36:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527700000 ms: 1431527640000 ms
15/05/13 10:36:01 INFO JobGenerator: Checkpointing graph for time 1431527760000 ms
15/05/13 10:36:01 INFO DStreamGraph: Updating checkpoint data for time 1431527760000 ms
15/05/13 10:36:01 INFO DStreamGraph: Updated checkpoint data for time 1431527760000 ms
15/05/13 10:36:01 INFO CheckpointWriter: Saving checkpoint for time 1431527760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000'
15/05/13 10:36:01 INFO CheckpointWriter: Checkpoint for time 1431527760000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000', took 7655 bytes and 53 ms
15/05/13 10:36:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527760000 ms
15/05/13 10:36:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527760000 ms
15/05/13 10:36:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:37:00 INFO FileInputDStream: Finding new files took 41 ms
15/05/13 10:37:00 INFO FileInputDStream: New files at time 1431527820000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527710469.json
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=21589115, maxMem=278302556
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_258 stored as values in memory (estimated size 232.9 KB, free 244.6 MB)
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=21827647, maxMem=278302556
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_258_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.6 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_258_piece0
15/05/13 10:37:00 INFO SparkContext: Created broadcast 258 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:37:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:37:00 INFO JobScheduler: Added jobs for time 1431527820000 ms
15/05/13 10:37:00 INFO JobGenerator: Checkpointing graph for time 1431527820000 ms
15/05/13 10:37:00 INFO JobScheduler: Starting job streaming job 1431527820000 ms.0 from job set of time 1431527820000 ms
15/05/13 10:37:00 INFO DStreamGraph: Updating checkpoint data for time 1431527820000 ms
15/05/13 10:37:00 INFO DStreamGraph: Updated checkpoint data for time 1431527820000 ms
15/05/13 10:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431527820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000'
15/05/13 10:37:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:37:00 INFO DAGScheduler: Got job 176 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:37:00 INFO DAGScheduler: Final stage: Stage 172(reduce at JsonRDD.scala:51)
15/05/13 10:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:37:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83545): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431527820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000'
15/05/13 10:37:00 INFO DAGScheduler: Submitting Stage 172 (MapPartitionsRDD[1234] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=21863355, maxMem=278302556
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_259 stored as values in memory (estimated size 5.9 KB, free 244.6 MB)
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=21869395, maxMem=278302556
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 251
15/05/13 10:37:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83547): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_251
15/05/13 10:37:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83547): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_259_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.5 MB)
15/05/13 10:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431527820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000'
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_251 of size 20920 dropped from memory (free 256449855)
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_251_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_251_piece0 of size 10895 dropped from memory (free 256460750)
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_251_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_251_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_251_piece0
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_259_piece0
15/05/13 10:37:00 INFO SparkContext: Created broadcast 259 from broadcast at DAGScheduler.scala:839
15/05/13 10:37:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 172 (MapPartitionsRDD[1234] at map at JsonRDD.scala:51)
15/05/13 10:37:00 INFO TaskSchedulerImpl: Adding task set 172.0 with 1 tasks
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 251
15/05/13 10:37:00 INFO TaskSetManager: Starting task 0.0 in stage 172.0 (TID 172, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 250
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_250_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_250_piece0 of size 4227 dropped from memory (free 256464977)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_250_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_250_piece0
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_250
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_250 of size 6040 dropped from memory (free 256471017)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_250_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 250
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 249
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_249_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_249_piece0 of size 35708 dropped from memory (free 256506725)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_249_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.7 MB)
15/05/13 10:37:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83549): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_249_piece0
15/05/13 10:37:00 WARN CheckpointWriter: Could not write checkpoint for time 1431527820000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000'
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_249
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_249 of size 238532 dropped from memory (free 256745257)
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_259_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_249_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 249
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 248
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_248
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_248 of size 20920 dropped from memory (free 256766177)
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_248_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_248_piece0 of size 10838 dropped from memory (free 256777015)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_248_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_248_piece0
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_258_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_248_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 248
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 256
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_256_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_256_piece0 of size 4227 dropped from memory (free 256781242)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_256_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_256_piece0
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_256
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_256 of size 6040 dropped from memory (free 256787282)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_256_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 256
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 254
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_254
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_254 of size 20488 dropped from memory (free 256807770)
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_254_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_254_piece0 of size 10758 dropped from memory (free 256818528)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_254_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_254_piece0
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_254_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 264.5 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 254
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 253
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_253
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_253 of size 6040 dropped from memory (free 256824568)
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_253_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_253_piece0 of size 4227 dropped from memory (free 256828795)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_253_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_253_piece0
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_253_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 253
15/05/13 10:37:00 INFO BlockManager: Removing broadcast 257
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_257_piece0
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_257_piece0 of size 10767 dropped from memory (free 256839562)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_257_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_257_piece0
15/05/13 10:37:00 INFO BlockManager: Removing block broadcast_257
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_257 of size 20488 dropped from memory (free 256860050)
15/05/13 10:37:00 INFO BlockManagerInfo: Removed broadcast_257_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.5 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO ContextCleaner: Cleaned broadcast 257
15/05/13 10:37:00 INFO TaskSetManager: Finished task 0.0 in stage 172.0 (TID 172) in 254 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:37:00 INFO TaskSchedulerImpl: Removed TaskSet 172.0, whose tasks have all completed, from pool 
15/05/13 10:37:00 INFO DAGScheduler: Stage 172 (reduce at JsonRDD.scala:51) finished in 0.263 s
15/05/13 10:37:00 INFO DAGScheduler: Job 176 finished: reduce at JsonRDD.scala:51, took 0.357962 s
15/05/13 10:37:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:37:00 INFO DAGScheduler: Got job 177 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:37:00 INFO DAGScheduler: Final stage: Stage 173(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:37:00 INFO DAGScheduler: Submitting Stage 173 (MapPartitionsRDD[1241] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=21442506, maxMem=278302556
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_260 stored as values in memory (estimated size 20.4 KB, free 244.9 MB)
15/05/13 10:37:00 INFO MemoryStore: ensureFreeSpace(10911) called with curMem=21463426, maxMem=278302556
15/05/13 10:37:00 INFO MemoryStore: Block broadcast_260_piece0 stored as bytes in memory (estimated size 10.7 KB, free 244.9 MB)
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 262.7 MB)
15/05/13 10:37:00 INFO BlockManagerMaster: Updated info of block broadcast_260_piece0
15/05/13 10:37:00 INFO SparkContext: Created broadcast 260 from broadcast at DAGScheduler.scala:839
15/05/13 10:37:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 173 (MapPartitionsRDD[1241] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:37:00 INFO TaskSchedulerImpl: Adding task set 173.0 with 1 tasks
15/05/13 10:37:00 INFO TaskSetManager: Starting task 0.0 in stage 173.0 (TID 173, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:37:00 INFO BlockManagerInfo: Added broadcast_260_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 264.4 MB)
15/05/13 10:37:00 INFO TaskSetManager: Finished task 0.0 in stage 173.0 (TID 173) in 87 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:37:00 INFO TaskSchedulerImpl: Removed TaskSet 173.0, whose tasks have all completed, from pool 
15/05/13 10:37:00 INFO DAGScheduler: Stage 173 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.088 s
15/05/13 10:37:00 INFO DAGScheduler: Job 177 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.120416 s
15/05/13 10:37:00 INFO JobScheduler: Finished job streaming job 1431527820000 ms.0 from job set of time 1431527820000 ms
15/05/13 10:37:00 INFO JobScheduler: Total delay: 0.929 s for time 1431527820000 ms (execution: 0.809 s)
15/05/13 10:37:00 INFO MapPartitionsRDD: Removing RDD 1216 from persistence list
15/05/13 10:37:00 INFO BlockManager: Removing RDD 1216
15/05/13 10:37:00 INFO UnionRDD: Removing RDD 1215 from persistence list
15/05/13 10:37:00 INFO BlockManager: Removing RDD 1215
15/05/13 10:37:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431527760000 ms: 1431527700000 ms
15/05/13 10:37:00 INFO JobGenerator: Checkpointing graph for time 1431527820000 ms
15/05/13 10:37:00 INFO DStreamGraph: Updating checkpoint data for time 1431527820000 ms
15/05/13 10:37:00 INFO DStreamGraph: Updated checkpoint data for time 1431527820000 ms
15/05/13 10:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431527820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000'
15/05/13 10:37:01 INFO CheckpointWriter: Checkpoint for time 1431527820000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000', took 7657 bytes and 58 ms
15/05/13 10:37:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527820000 ms
15/05/13 10:37:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527820000 ms
15/05/13 10:37:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:38:00 INFO FileInputDStream: Finding new files took 36 ms
15/05/13 10:38:00 INFO FileInputDStream: New files at time 1431527880000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527770933.json
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=21474337, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_261 stored as values in memory (estimated size 232.9 KB, free 244.7 MB)
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=21712869, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_261_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.7 MB)
15/05/13 10:38:00 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.7 MB)
15/05/13 10:38:00 INFO BlockManagerMaster: Updated info of block broadcast_261_piece0
15/05/13 10:38:00 INFO SparkContext: Created broadcast 261 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:38:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:38:00 INFO JobScheduler: Added jobs for time 1431527880000 ms
15/05/13 10:38:00 INFO JobScheduler: Starting job streaming job 1431527880000 ms.0 from job set of time 1431527880000 ms
15/05/13 10:38:00 INFO JobGenerator: Checkpointing graph for time 1431527880000 ms
15/05/13 10:38:00 INFO DStreamGraph: Updating checkpoint data for time 1431527880000 ms
15/05/13 10:38:00 INFO DStreamGraph: Updated checkpoint data for time 1431527880000 ms
15/05/13 10:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431527880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000'
15/05/13 10:38:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83556): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431527880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000'
15/05/13 10:38:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:38:00 INFO DAGScheduler: Got job 178 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:38:00 INFO DAGScheduler: Final stage: Stage 174(reduce at JsonRDD.scala:51)
15/05/13 10:38:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:38:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:38:00 INFO DAGScheduler: Submitting Stage 174 (MapPartitionsRDD[1248] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=21748577, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_262 stored as values in memory (estimated size 5.9 KB, free 244.7 MB)
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=21754617, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_262_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.7 MB)
15/05/13 10:38:00 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:38:00 INFO BlockManagerMaster: Updated info of block broadcast_262_piece0
15/05/13 10:38:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83558): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431527880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000'
15/05/13 10:38:00 INFO SparkContext: Created broadcast 262 from broadcast at DAGScheduler.scala:839
15/05/13 10:38:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 174 (MapPartitionsRDD[1248] at map at JsonRDD.scala:51)
15/05/13 10:38:00 INFO TaskSchedulerImpl: Adding task set 174.0 with 1 tasks
15/05/13 10:38:00 INFO TaskSetManager: Starting task 0.0 in stage 174.0 (TID 174, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:38:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527580000.bk
15/05/13 10:38:00 INFO CheckpointWriter: Checkpoint for time 1431527880000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000', took 7665 bytes and 181 ms
15/05/13 10:38:00 INFO BlockManagerInfo: Added broadcast_262_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.4 MB)
15/05/13 10:38:00 INFO BlockManagerInfo: Added broadcast_261_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:38:00 INFO TaskSetManager: Finished task 0.0 in stage 174.0 (TID 174) in 426 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:38:00 INFO TaskSchedulerImpl: Removed TaskSet 174.0, whose tasks have all completed, from pool 
15/05/13 10:38:00 INFO DAGScheduler: Stage 174 (reduce at JsonRDD.scala:51) finished in 0.441 s
15/05/13 10:38:00 INFO DAGScheduler: Job 178 finished: reduce at JsonRDD.scala:51, took 0.485568 s
15/05/13 10:38:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:38:00 INFO DAGScheduler: Got job 179 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:38:00 INFO DAGScheduler: Final stage: Stage 175(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:38:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:38:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:38:00 INFO DAGScheduler: Submitting Stage 175 (MapPartitionsRDD[1255] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=21758843, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_263 stored as values in memory (estimated size 20.4 KB, free 244.6 MB)
15/05/13 10:38:00 INFO MemoryStore: ensureFreeSpace(10912) called with curMem=21779763, maxMem=278302556
15/05/13 10:38:00 INFO MemoryStore: Block broadcast_263_piece0 stored as bytes in memory (estimated size 10.7 KB, free 244.6 MB)
15/05/13 10:38:00 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 262.7 MB)
15/05/13 10:38:00 INFO BlockManagerMaster: Updated info of block broadcast_263_piece0
15/05/13 10:38:00 INFO SparkContext: Created broadcast 263 from broadcast at DAGScheduler.scala:839
15/05/13 10:38:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 175 (MapPartitionsRDD[1255] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:38:00 INFO TaskSchedulerImpl: Adding task set 175.0 with 1 tasks
15/05/13 10:38:00 INFO TaskSetManager: Starting task 0.0 in stage 175.0 (TID 175, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:38:01 INFO BlockManagerInfo: Added broadcast_263_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.3 MB)
15/05/13 10:38:01 INFO TaskSetManager: Finished task 0.0 in stage 175.0 (TID 175) in 122 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:38:01 INFO TaskSchedulerImpl: Removed TaskSet 175.0, whose tasks have all completed, from pool 
15/05/13 10:38:01 INFO DAGScheduler: Stage 175 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.123 s
15/05/13 10:38:01 INFO DAGScheduler: Job 179 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.149235 s
15/05/13 10:38:01 INFO JobScheduler: Finished job streaming job 1431527880000 ms.0 from job set of time 1431527880000 ms
15/05/13 10:38:01 INFO JobScheduler: Total delay: 1.149 s for time 1431527880000 ms (execution: 0.970 s)
15/05/13 10:38:01 INFO MapPartitionsRDD: Removing RDD 1230 from persistence list
15/05/13 10:38:01 INFO BlockManager: Removing RDD 1230
15/05/13 10:38:01 INFO UnionRDD: Removing RDD 1229 from persistence list
15/05/13 10:38:01 INFO BlockManager: Removing RDD 1229
15/05/13 10:38:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527820000 ms: 1431527760000 ms
15/05/13 10:38:01 INFO JobGenerator: Checkpointing graph for time 1431527880000 ms
15/05/13 10:38:01 INFO DStreamGraph: Updating checkpoint data for time 1431527880000 ms
15/05/13 10:38:01 INFO DStreamGraph: Updated checkpoint data for time 1431527880000 ms
15/05/13 10:38:01 INFO CheckpointWriter: Saving checkpoint for time 1431527880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000'
15/05/13 10:38:01 INFO CheckpointWriter: Checkpoint for time 1431527880000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000', took 7656 bytes and 54 ms
15/05/13 10:38:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527880000 ms
15/05/13 10:38:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527880000 ms
15/05/13 10:38:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:39:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 10:39:00 INFO FileInputDStream: New files at time 1431527940000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527833157.json
15/05/13 10:39:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=21790675, maxMem=278302556
15/05/13 10:39:00 INFO MemoryStore: Block broadcast_264 stored as values in memory (estimated size 232.9 KB, free 244.4 MB)
15/05/13 10:39:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=22029207, maxMem=278302556
15/05/13 10:39:00 INFO MemoryStore: Block broadcast_264_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.4 MB)
15/05/13 10:39:00 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:39:00 INFO BlockManagerMaster: Updated info of block broadcast_264_piece0
15/05/13 10:39:00 INFO SparkContext: Created broadcast 264 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:39:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:39:00 INFO JobScheduler: Added jobs for time 1431527940000 ms
15/05/13 10:39:00 INFO JobGenerator: Checkpointing graph for time 1431527940000 ms
15/05/13 10:39:00 INFO JobScheduler: Starting job streaming job 1431527940000 ms.0 from job set of time 1431527940000 ms
15/05/13 10:39:00 INFO DStreamGraph: Updating checkpoint data for time 1431527940000 ms
15/05/13 10:39:00 INFO DStreamGraph: Updated checkpoint data for time 1431527940000 ms
15/05/13 10:39:00 INFO CheckpointWriter: Saving checkpoint for time 1431527940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000'
15/05/13 10:39:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:39:00 INFO DAGScheduler: Got job 180 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:39:00 INFO DAGScheduler: Final stage: Stage 176(reduce at JsonRDD.scala:51)
15/05/13 10:39:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:39:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:39:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83564): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:39:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83564): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:39:00 INFO CheckpointWriter: Saving checkpoint for time 1431527940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000'
15/05/13 10:39:00 INFO DAGScheduler: Submitting Stage 176 (MapPartitionsRDD[1262] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:39:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22064915, maxMem=278302556
15/05/13 10:39:00 INFO MemoryStore: Block broadcast_265 stored as values in memory (estimated size 5.9 KB, free 244.4 MB)
15/05/13 10:39:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=22070955, maxMem=278302556
15/05/13 10:39:00 INFO MemoryStore: Block broadcast_265_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.4 MB)
15/05/13 10:39:00 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:39:00 INFO BlockManagerMaster: Updated info of block broadcast_265_piece0
15/05/13 10:39:00 INFO SparkContext: Created broadcast 265 from broadcast at DAGScheduler.scala:839
15/05/13 10:39:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 176 (MapPartitionsRDD[1262] at map at JsonRDD.scala:51)
15/05/13 10:39:00 INFO TaskSchedulerImpl: Adding task set 176.0 with 1 tasks
15/05/13 10:39:00 INFO TaskSetManager: Starting task 0.0 in stage 176.0 (TID 176, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:39:00 INFO BlockManagerInfo: Added broadcast_265_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:39:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527640000.bk
15/05/13 10:39:00 INFO CheckpointWriter: Checkpoint for time 1431527940000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000', took 7662 bytes and 166 ms
15/05/13 10:39:00 INFO BlockManagerInfo: Added broadcast_264_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:39:01 INFO TaskSetManager: Finished task 0.0 in stage 176.0 (TID 176) in 832 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:39:01 INFO DAGScheduler: Stage 176 (reduce at JsonRDD.scala:51) finished in 0.884 s
15/05/13 10:39:01 INFO DAGScheduler: Job 180 finished: reduce at JsonRDD.scala:51, took 0.947224 s
15/05/13 10:39:01 INFO TaskSchedulerImpl: Removed TaskSet 176.0, whose tasks have all completed, from pool 
15/05/13 10:39:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:39:01 INFO DAGScheduler: Got job 181 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:39:01 INFO DAGScheduler: Final stage: Stage 177(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:39:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:39:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:39:01 INFO DAGScheduler: Submitting Stage 177 (MapPartitionsRDD[1269] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:39:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=22075182, maxMem=278302556
15/05/13 10:39:01 INFO MemoryStore: Block broadcast_266 stored as values in memory (estimated size 20.4 KB, free 244.3 MB)
15/05/13 10:39:01 INFO MemoryStore: ensureFreeSpace(10903) called with curMem=22096102, maxMem=278302556
15/05/13 10:39:01 INFO MemoryStore: Block broadcast_266_piece0 stored as bytes in memory (estimated size 10.6 KB, free 244.3 MB)
15/05/13 10:39:01 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.6 MB)
15/05/13 10:39:01 INFO BlockManagerMaster: Updated info of block broadcast_266_piece0
15/05/13 10:39:01 INFO SparkContext: Created broadcast 266 from broadcast at DAGScheduler.scala:839
15/05/13 10:39:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 177 (MapPartitionsRDD[1269] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:39:01 INFO TaskSchedulerImpl: Adding task set 177.0 with 1 tasks
15/05/13 10:39:01 INFO TaskSetManager: Starting task 0.0 in stage 177.0 (TID 177, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:39:01 INFO BlockManagerInfo: Added broadcast_266_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.5 MB)
15/05/13 10:39:01 INFO TaskSetManager: Finished task 0.0 in stage 177.0 (TID 177) in 176 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:39:01 INFO TaskSchedulerImpl: Removed TaskSet 177.0, whose tasks have all completed, from pool 
15/05/13 10:39:01 INFO DAGScheduler: Stage 177 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.178 s
15/05/13 10:39:01 INFO DAGScheduler: Job 181 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.212467 s
15/05/13 10:39:01 INFO JobScheduler: Finished job streaming job 1431527940000 ms.0 from job set of time 1431527940000 ms
15/05/13 10:39:01 INFO JobScheduler: Total delay: 1.730 s for time 1431527940000 ms (execution: 1.580 s)
15/05/13 10:39:01 INFO MapPartitionsRDD: Removing RDD 1244 from persistence list
15/05/13 10:39:01 INFO BlockManager: Removing RDD 1244
15/05/13 10:39:01 INFO UnionRDD: Removing RDD 1243 from persistence list
15/05/13 10:39:01 INFO BlockManager: Removing RDD 1243
15/05/13 10:39:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527880000 ms: 1431527820000 ms
15/05/13 10:39:01 INFO JobGenerator: Checkpointing graph for time 1431527940000 ms
15/05/13 10:39:01 INFO DStreamGraph: Updating checkpoint data for time 1431527940000 ms
15/05/13 10:39:01 INFO DStreamGraph: Updated checkpoint data for time 1431527940000 ms
15/05/13 10:39:01 INFO CheckpointWriter: Saving checkpoint for time 1431527940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000'
15/05/13 10:39:01 INFO CheckpointWriter: Checkpoint for time 1431527940000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000', took 7656 bytes and 70 ms
15/05/13 10:39:01 INFO DStreamGraph: Clearing checkpoint data for time 1431527940000 ms
15/05/13 10:39:01 INFO DStreamGraph: Cleared checkpoint data for time 1431527940000 ms
15/05/13 10:39:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:40:00 INFO FileInputDStream: Finding new files took 65 ms
15/05/13 10:40:00 INFO FileInputDStream: New files at time 1431528000000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527893605.json
15/05/13 10:40:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22107005, maxMem=278302556
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_267 stored as values in memory (estimated size 232.9 KB, free 244.1 MB)
15/05/13 10:40:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=22345537, maxMem=278302556
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_267_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.1 MB)
15/05/13 10:40:00 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_267_piece0
15/05/13 10:40:00 INFO SparkContext: Created broadcast 267 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:40:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:40:00 INFO JobScheduler: Added jobs for time 1431528000000 ms
15/05/13 10:40:00 INFO JobGenerator: Checkpointing graph for time 1431528000000 ms
15/05/13 10:40:00 INFO DStreamGraph: Updating checkpoint data for time 1431528000000 ms
15/05/13 10:40:00 INFO JobScheduler: Starting job streaming job 1431528000000 ms.0 from job set of time 1431528000000 ms
15/05/13 10:40:00 INFO DStreamGraph: Updated checkpoint data for time 1431528000000 ms
15/05/13 10:40:00 INFO CheckpointWriter: Saving checkpoint for time 1431528000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000'
15/05/13 10:40:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:40:00 INFO DAGScheduler: Got job 182 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:40:00 INFO DAGScheduler: Final stage: Stage 178(reduce at JsonRDD.scala:51)
15/05/13 10:40:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:40:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83572): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:40:00 INFO CheckpointWriter: Saving checkpoint for time 1431528000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000'
15/05/13 10:40:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:40:00 INFO DAGScheduler: Submitting Stage 178 (MapPartitionsRDD[1276] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:40:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22381245, maxMem=278302556
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_268 stored as values in memory (estimated size 5.9 KB, free 244.1 MB)
15/05/13 10:40:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=22387285, maxMem=278302556
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_268_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.1 MB)
15/05/13 10:40:00 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_268_piece0
15/05/13 10:40:00 INFO SparkContext: Created broadcast 268 from broadcast at DAGScheduler.scala:839
15/05/13 10:40:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 178 (MapPartitionsRDD[1276] at map at JsonRDD.scala:51)
15/05/13 10:40:00 INFO TaskSchedulerImpl: Adding task set 178.0 with 1 tasks
15/05/13 10:40:00 INFO TaskSetManager: Starting task 0.0 in stage 178.0 (TID 178, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:40:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83574): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:40:00 INFO CheckpointWriter: Saving checkpoint for time 1431528000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000'
15/05/13 10:40:00 INFO BlockManagerInfo: Added broadcast_268_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:40:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83576): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:40:00 WARN CheckpointWriter: Could not write checkpoint for time 1431528000000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000'
15/05/13 10:40:00 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:40:00 INFO TaskSetManager: Finished task 0.0 in stage 178.0 (TID 178) in 460 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:40:00 INFO TaskSchedulerImpl: Removed TaskSet 178.0, whose tasks have all completed, from pool 
15/05/13 10:40:00 INFO DAGScheduler: Stage 178 (reduce at JsonRDD.scala:51) finished in 0.472 s
15/05/13 10:40:00 INFO DAGScheduler: Job 182 finished: reduce at JsonRDD.scala:51, took 0.511074 s
15/05/13 10:40:00 INFO BlockManager: Removing broadcast 265
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_265_piece0
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_265_piece0 of size 4227 dropped from memory (free 255915273)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_265_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_265_piece0
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_265
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_265 of size 6040 dropped from memory (free 255921313)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_265_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:40:00 INFO ContextCleaner: Cleaned broadcast 265
15/05/13 10:40:00 INFO BlockManager: Removing broadcast 266
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_266_piece0
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_266_piece0 of size 10903 dropped from memory (free 255932216)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_266_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_266_piece0
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_266
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_266 of size 20920 dropped from memory (free 255953136)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_266_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.5 MB)
15/05/13 10:40:00 INFO ContextCleaner: Cleaned broadcast 266
15/05/13 10:40:00 INFO BlockManager: Removing broadcast 268
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_268
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_268 of size 6040 dropped from memory (free 255959176)
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_268_piece0
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_268_piece0 of size 4225 dropped from memory (free 255963401)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_268_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_268_piece0
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_268_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:40:00 INFO ContextCleaner: Cleaned broadcast 268
15/05/13 10:40:00 INFO BlockManager: Removing broadcast 261
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_261
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_261 of size 238532 dropped from memory (free 256201933)
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_261_piece0
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_261_piece0 of size 35708 dropped from memory (free 256237641)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_261_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_261_piece0
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_261_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:40:00 INFO ContextCleaner: Cleaned broadcast 261
15/05/13 10:40:00 INFO BlockManager: Removing broadcast 262
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_262_piece0
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_262_piece0 of size 4226 dropped from memory (free 256241867)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_262_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:40:00 INFO BlockManagerMaster: Updated info of block broadcast_262_piece0
15/05/13 10:40:00 INFO BlockManager: Removing block broadcast_262
15/05/13 10:40:00 INFO MemoryStore: Block broadcast_262 of size 6040 dropped from memory (free 256247907)
15/05/13 10:40:00 INFO BlockManagerInfo: Removed broadcast_262_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:40:01 INFO ContextCleaner: Cleaned broadcast 262
15/05/13 10:40:01 INFO BlockManager: Removing broadcast 263
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_263_piece0
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_263_piece0 of size 10912 dropped from memory (free 256258819)
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_263_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 262.7 MB)
15/05/13 10:40:01 INFO BlockManagerMaster: Updated info of block broadcast_263_piece0
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_263
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_263 of size 20920 dropped from memory (free 256279739)
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_263_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 263.3 MB)
15/05/13 10:40:01 INFO ContextCleaner: Cleaned broadcast 263
15/05/13 10:40:01 INFO BlockManager: Removing broadcast 259
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_259_piece0
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_259_piece0 of size 4226 dropped from memory (free 256283965)
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_259_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.7 MB)
15/05/13 10:40:01 INFO BlockManagerMaster: Updated info of block broadcast_259_piece0
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_259
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_259 of size 6040 dropped from memory (free 256290005)
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_259_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:40:01 INFO ContextCleaner: Cleaned broadcast 259
15/05/13 10:40:01 INFO BlockManager: Removing broadcast 260
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_260
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_260 of size 20920 dropped from memory (free 256310925)
15/05/13 10:40:01 INFO BlockManager: Removing block broadcast_260_piece0
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_260_piece0 of size 10911 dropped from memory (free 256321836)
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_260_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 262.7 MB)
15/05/13 10:40:01 INFO BlockManagerMaster: Updated info of block broadcast_260_piece0
15/05/13 10:40:01 INFO BlockManagerInfo: Removed broadcast_260_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 264.4 MB)
15/05/13 10:40:01 INFO ContextCleaner: Cleaned broadcast 260
15/05/13 10:40:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:40:01 INFO DAGScheduler: Got job 183 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:40:01 INFO DAGScheduler: Final stage: Stage 179(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:40:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:40:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:40:01 INFO DAGScheduler: Submitting Stage 179 (MapPartitionsRDD[1283] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:40:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=21980720, maxMem=278302556
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_269 stored as values in memory (estimated size 21.0 KB, free 244.4 MB)
15/05/13 10:40:01 INFO MemoryStore: ensureFreeSpace(11140) called with curMem=22002200, maxMem=278302556
15/05/13 10:40:01 INFO MemoryStore: Block broadcast_269_piece0 stored as bytes in memory (estimated size 10.9 KB, free 244.4 MB)
15/05/13 10:40:01 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.7 MB)
15/05/13 10:40:01 INFO BlockManagerMaster: Updated info of block broadcast_269_piece0
15/05/13 10:40:01 INFO SparkContext: Created broadcast 269 from broadcast at DAGScheduler.scala:839
15/05/13 10:40:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 179 (MapPartitionsRDD[1283] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:40:01 INFO TaskSchedulerImpl: Adding task set 179.0 with 1 tasks
15/05/13 10:40:01 INFO TaskSetManager: Starting task 0.0 in stage 179.0 (TID 179, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:40:01 INFO BlockManagerInfo: Added broadcast_269_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.4 MB)
15/05/13 10:40:01 INFO BlockManagerInfo: Added broadcast_267_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:40:01 INFO TaskSetManager: Finished task 0.0 in stage 179.0 (TID 179) in 176 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:40:01 INFO TaskSchedulerImpl: Removed TaskSet 179.0, whose tasks have all completed, from pool 
15/05/13 10:40:01 INFO DAGScheduler: Stage 179 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.179 s
15/05/13 10:40:01 INFO DAGScheduler: Job 183 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.213718 s
15/05/13 10:40:01 INFO JobScheduler: Finished job streaming job 1431528000000 ms.0 from job set of time 1431528000000 ms
15/05/13 10:40:01 INFO JobScheduler: Total delay: 1.346 s for time 1431528000000 ms (execution: 1.199 s)
15/05/13 10:40:01 INFO MapPartitionsRDD: Removing RDD 1258 from persistence list
15/05/13 10:40:01 INFO BlockManager: Removing RDD 1258
15/05/13 10:40:01 INFO UnionRDD: Removing RDD 1257 from persistence list
15/05/13 10:40:01 INFO BlockManager: Removing RDD 1257
15/05/13 10:40:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431527940000 ms: 1431527880000 ms
15/05/13 10:40:01 INFO JobGenerator: Checkpointing graph for time 1431528000000 ms
15/05/13 10:40:01 INFO DStreamGraph: Updating checkpoint data for time 1431528000000 ms
15/05/13 10:40:01 INFO DStreamGraph: Updated checkpoint data for time 1431528000000 ms
15/05/13 10:40:01 INFO CheckpointWriter: Saving checkpoint for time 1431528000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000'
15/05/13 10:40:01 INFO CheckpointWriter: Checkpoint for time 1431528000000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000', took 7656 bytes and 70 ms
15/05/13 10:40:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528000000 ms
15/05/13 10:40:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528000000 ms
15/05/13 10:40:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:41:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 10:41:00 INFO FileInputDStream: New files at time 1431528060000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431527954758.json
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22013340, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_270 stored as values in memory (estimated size 232.9 KB, free 244.2 MB)
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=22251872, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_270_piece0 stored as bytes in memory (estimated size 34.9 KB, free 244.2 MB)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:41:00 INFO BlockManagerMaster: Updated info of block broadcast_270_piece0
15/05/13 10:41:00 INFO SparkContext: Created broadcast 270 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:41:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:41:00 INFO JobScheduler: Added jobs for time 1431528060000 ms
15/05/13 10:41:00 INFO JobGenerator: Checkpointing graph for time 1431528060000 ms
15/05/13 10:41:00 INFO JobScheduler: Starting job streaming job 1431528060000 ms.0 from job set of time 1431528060000 ms
15/05/13 10:41:00 INFO DStreamGraph: Updating checkpoint data for time 1431528060000 ms
15/05/13 10:41:00 INFO DStreamGraph: Updated checkpoint data for time 1431528060000 ms
15/05/13 10:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431528060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528060000'
15/05/13 10:41:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:41:00 INFO DAGScheduler: Got job 184 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:41:00 INFO DAGScheduler: Final stage: Stage 180(reduce at JsonRDD.scala:51)
15/05/13 10:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:41:00 INFO DAGScheduler: Submitting Stage 180 (MapPartitionsRDD[1290] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:41:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527760000
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22287580, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_271 stored as values in memory (estimated size 5.9 KB, free 244.1 MB)
15/05/13 10:41:00 INFO CheckpointWriter: Checkpoint for time 1431528060000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528060000', took 7665 bytes and 68 ms
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=22293620, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_271_piece0 stored as bytes in memory (estimated size 4.1 KB, free 244.1 MB)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:41:00 INFO BlockManagerMaster: Updated info of block broadcast_271_piece0
15/05/13 10:41:00 INFO SparkContext: Created broadcast 271 from broadcast at DAGScheduler.scala:839
15/05/13 10:41:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 180 (MapPartitionsRDD[1290] at map at JsonRDD.scala:51)
15/05/13 10:41:00 INFO TaskSchedulerImpl: Adding task set 180.0 with 1 tasks
15/05/13 10:41:00 INFO TaskSetManager: Starting task 0.0 in stage 180.0 (TID 180, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_271_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_270_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:41:00 INFO TaskSetManager: Finished task 0.0 in stage 180.0 (TID 180) in 282 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:41:00 INFO TaskSchedulerImpl: Removed TaskSet 180.0, whose tasks have all completed, from pool 
15/05/13 10:41:00 INFO DAGScheduler: Stage 180 (reduce at JsonRDD.scala:51) finished in 0.296 s
15/05/13 10:41:00 INFO DAGScheduler: Job 184 finished: reduce at JsonRDD.scala:51, took 0.328146 s
15/05/13 10:41:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:41:00 INFO DAGScheduler: Got job 185 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:41:00 INFO DAGScheduler: Final stage: Stage 181(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:41:00 INFO DAGScheduler: Submitting Stage 181 (MapPartitionsRDD[1297] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=22297845, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_272 stored as values in memory (estimated size 20.4 KB, free 244.1 MB)
15/05/13 10:41:00 INFO MemoryStore: ensureFreeSpace(10904) called with curMem=22318765, maxMem=278302556
15/05/13 10:41:00 INFO MemoryStore: Block broadcast_272_piece0 stored as bytes in memory (estimated size 10.6 KB, free 244.1 MB)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.6 MB)
15/05/13 10:41:00 INFO BlockManagerMaster: Updated info of block broadcast_272_piece0
15/05/13 10:41:00 INFO SparkContext: Created broadcast 272 from broadcast at DAGScheduler.scala:839
15/05/13 10:41:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 181 (MapPartitionsRDD[1297] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:41:00 INFO TaskSchedulerImpl: Adding task set 181.0 with 1 tasks
15/05/13 10:41:00 INFO TaskSetManager: Starting task 0.0 in stage 181.0 (TID 181, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:41:00 INFO BlockManagerInfo: Added broadcast_272_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:41:00 INFO TaskSetManager: Finished task 0.0 in stage 181.0 (TID 181) in 82 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:41:00 INFO TaskSchedulerImpl: Removed TaskSet 181.0, whose tasks have all completed, from pool 
15/05/13 10:41:00 INFO DAGScheduler: Stage 181 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.084 s
15/05/13 10:41:00 INFO DAGScheduler: Job 185 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.120835 s
15/05/13 10:41:00 INFO JobScheduler: Finished job streaming job 1431528060000 ms.0 from job set of time 1431528060000 ms
15/05/13 10:41:00 INFO JobScheduler: Total delay: 0.933 s for time 1431528060000 ms (execution: 0.736 s)
15/05/13 10:41:00 INFO MapPartitionsRDD: Removing RDD 1272 from persistence list
15/05/13 10:41:00 INFO UnionRDD: Removing RDD 1271 from persistence list
15/05/13 10:41:00 INFO BlockManager: Removing RDD 1272
15/05/13 10:41:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528000000 ms: 1431527940000 ms
15/05/13 10:41:00 INFO JobGenerator: Checkpointing graph for time 1431528060000 ms
15/05/13 10:41:00 INFO DStreamGraph: Updating checkpoint data for time 1431528060000 ms
15/05/13 10:41:00 INFO DStreamGraph: Updated checkpoint data for time 1431528060000 ms
15/05/13 10:41:00 INFO BlockManager: Removing RDD 1271
15/05/13 10:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431528060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528060000'
15/05/13 10:41:01 INFO CheckpointWriter: Checkpoint for time 1431528060000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528060000', took 7657 bytes and 56 ms
15/05/13 10:41:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528060000 ms
15/05/13 10:41:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528060000 ms
15/05/13 10:41:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:42:00 INFO FileInputDStream: Finding new files took 27 ms
15/05/13 10:42:00 INFO FileInputDStream: New files at time 1431528120000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528019881.json
15/05/13 10:42:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22329669, maxMem=278302556
15/05/13 10:42:00 INFO MemoryStore: Block broadcast_273 stored as values in memory (estimated size 232.9 KB, free 243.9 MB)
15/05/13 10:42:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=22568201, maxMem=278302556
15/05/13 10:42:00 INFO MemoryStore: Block broadcast_273_piece0 stored as bytes in memory (estimated size 34.9 KB, free 243.9 MB)
15/05/13 10:42:00 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:42:00 INFO BlockManagerMaster: Updated info of block broadcast_273_piece0
15/05/13 10:42:00 INFO SparkContext: Created broadcast 273 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:42:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:42:00 INFO JobScheduler: Starting job streaming job 1431528120000 ms.0 from job set of time 1431528120000 ms
15/05/13 10:42:00 INFO JobScheduler: Added jobs for time 1431528120000 ms
15/05/13 10:42:00 INFO JobGenerator: Checkpointing graph for time 1431528120000 ms
15/05/13 10:42:00 INFO DStreamGraph: Updating checkpoint data for time 1431528120000 ms
15/05/13 10:42:00 INFO DStreamGraph: Updated checkpoint data for time 1431528120000 ms
15/05/13 10:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431528120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000'
15/05/13 10:42:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83588): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431528120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000'
15/05/13 10:42:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:42:00 INFO DAGScheduler: Got job 186 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:42:00 INFO DAGScheduler: Final stage: Stage 182(reduce at JsonRDD.scala:51)
15/05/13 10:42:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:42:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:42:00 INFO DAGScheduler: Submitting Stage 182 (MapPartitionsRDD[1304] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:42:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22603909, maxMem=278302556
15/05/13 10:42:00 INFO MemoryStore: Block broadcast_274 stored as values in memory (estimated size 5.9 KB, free 243.8 MB)
15/05/13 10:42:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=22609949, maxMem=278302556
15/05/13 10:42:00 INFO MemoryStore: Block broadcast_274_piece0 stored as bytes in memory (estimated size 4.1 KB, free 243.8 MB)
15/05/13 10:42:00 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:42:00 INFO BlockManagerMaster: Updated info of block broadcast_274_piece0
15/05/13 10:42:00 INFO SparkContext: Created broadcast 274 from broadcast at DAGScheduler.scala:839
15/05/13 10:42:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 182 (MapPartitionsRDD[1304] at map at JsonRDD.scala:51)
15/05/13 10:42:00 INFO TaskSchedulerImpl: Adding task set 182.0 with 1 tasks
15/05/13 10:42:00 INFO TaskSetManager: Starting task 0.0 in stage 182.0 (TID 182, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:42:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83590): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431528120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000'
15/05/13 10:42:00 INFO BlockManagerInfo: Added broadcast_274_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:42:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527820000.bk
15/05/13 10:42:00 INFO CheckpointWriter: Checkpoint for time 1431528120000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000', took 7664 bytes and 180 ms
15/05/13 10:42:00 INFO BlockManagerInfo: Added broadcast_273_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:42:00 INFO TaskSetManager: Finished task 0.0 in stage 182.0 (TID 182) in 547 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:42:00 INFO TaskSchedulerImpl: Removed TaskSet 182.0, whose tasks have all completed, from pool 
15/05/13 10:42:00 INFO DAGScheduler: Stage 182 (reduce at JsonRDD.scala:51) finished in 0.556 s
15/05/13 10:42:00 INFO DAGScheduler: Job 186 finished: reduce at JsonRDD.scala:51, took 0.593413 s
15/05/13 10:42:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:42:01 INFO DAGScheduler: Got job 187 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:42:01 INFO DAGScheduler: Final stage: Stage 183(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:42:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:42:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:42:01 INFO DAGScheduler: Submitting Stage 183 (MapPartitionsRDD[1311] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:42:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=22614175, maxMem=278302556
15/05/13 10:42:01 INFO MemoryStore: Block broadcast_275 stored as values in memory (estimated size 21.0 KB, free 243.8 MB)
15/05/13 10:42:01 INFO MemoryStore: ensureFreeSpace(11148) called with curMem=22635655, maxMem=278302556
15/05/13 10:42:01 INFO MemoryStore: Block broadcast_275_piece0 stored as bytes in memory (estimated size 10.9 KB, free 243.8 MB)
15/05/13 10:42:01 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.6 MB)
15/05/13 10:42:01 INFO BlockManagerMaster: Updated info of block broadcast_275_piece0
15/05/13 10:42:01 INFO SparkContext: Created broadcast 275 from broadcast at DAGScheduler.scala:839
15/05/13 10:42:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 183 (MapPartitionsRDD[1311] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:42:01 INFO TaskSchedulerImpl: Adding task set 183.0 with 1 tasks
15/05/13 10:42:01 INFO TaskSetManager: Starting task 0.0 in stage 183.0 (TID 183, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:42:01 INFO BlockManagerInfo: Added broadcast_275_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 264.4 MB)
15/05/13 10:42:01 INFO TaskSetManager: Finished task 0.0 in stage 183.0 (TID 183) in 211 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:42:01 INFO TaskSchedulerImpl: Removed TaskSet 183.0, whose tasks have all completed, from pool 
15/05/13 10:42:01 INFO DAGScheduler: Stage 183 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.212 s
15/05/13 10:42:01 INFO DAGScheduler: Job 187 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.230254 s
15/05/13 10:42:01 INFO JobScheduler: Finished job streaming job 1431528120000 ms.0 from job set of time 1431528120000 ms
15/05/13 10:42:01 INFO JobScheduler: Total delay: 1.320 s for time 1431528120000 ms (execution: 1.114 s)
15/05/13 10:42:01 INFO MapPartitionsRDD: Removing RDD 1286 from persistence list
15/05/13 10:42:01 INFO BlockManager: Removing RDD 1286
15/05/13 10:42:01 INFO UnionRDD: Removing RDD 1285 from persistence list
15/05/13 10:42:01 INFO BlockManager: Removing RDD 1285
15/05/13 10:42:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528060000 ms: 1431528000000 ms
15/05/13 10:42:01 INFO JobGenerator: Checkpointing graph for time 1431528120000 ms
15/05/13 10:42:01 INFO DStreamGraph: Updating checkpoint data for time 1431528120000 ms
15/05/13 10:42:01 INFO DStreamGraph: Updated checkpoint data for time 1431528120000 ms
15/05/13 10:42:01 INFO CheckpointWriter: Saving checkpoint for time 1431528120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000'
15/05/13 10:42:01 INFO CheckpointWriter: Checkpoint for time 1431528120000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000', took 7656 bytes and 62 ms
15/05/13 10:42:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528120000 ms
15/05/13 10:42:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528120000 ms
15/05/13 10:42:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:43:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 10:43:00 INFO FileInputDStream: New files at time 1431528180000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528081473.json
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22646803, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_276 stored as values in memory (estimated size 232.9 KB, free 243.6 MB)
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=22885335, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_276_piece0 stored as bytes in memory (estimated size 34.9 KB, free 243.6 MB)
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:43:00 INFO BlockManagerMaster: Updated info of block broadcast_276_piece0
15/05/13 10:43:00 INFO SparkContext: Created broadcast 276 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:43:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:43:00 INFO JobScheduler: Added jobs for time 1431528180000 ms
15/05/13 10:43:00 INFO JobGenerator: Checkpointing graph for time 1431528180000 ms
15/05/13 10:43:00 INFO DStreamGraph: Updating checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO JobScheduler: Starting job streaming job 1431528180000 ms.0 from job set of time 1431528180000 ms
15/05/13 10:43:00 INFO DStreamGraph: Updated checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431528180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000'
15/05/13 10:43:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83603): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:43:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83603): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431528180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000'
15/05/13 10:43:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:43:00 INFO DAGScheduler: Got job 188 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:43:00 INFO DAGScheduler: Final stage: Stage 184(reduce at JsonRDD.scala:51)
15/05/13 10:43:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:43:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:43:00 INFO DAGScheduler: Submitting Stage 184 (MapPartitionsRDD[1318] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22921043, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_277 stored as values in memory (estimated size 5.9 KB, free 243.5 MB)
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=22927083, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_277_piece0 stored as bytes in memory (estimated size 4.1 KB, free 243.5 MB)
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:43:00 INFO BlockManagerMaster: Updated info of block broadcast_277_piece0
15/05/13 10:43:00 INFO SparkContext: Created broadcast 277 from broadcast at DAGScheduler.scala:839
15/05/13 10:43:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527880000.bk
15/05/13 10:43:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 184 (MapPartitionsRDD[1318] at map at JsonRDD.scala:51)
15/05/13 10:43:00 INFO TaskSchedulerImpl: Adding task set 184.0 with 1 tasks
15/05/13 10:43:00 INFO TaskSetManager: Starting task 0.0 in stage 184.0 (TID 184, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:43:00 INFO CheckpointWriter: Checkpoint for time 1431528180000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000', took 7666 bytes and 116 ms
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_277_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_276_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:43:00 INFO TaskSetManager: Finished task 0.0 in stage 184.0 (TID 184) in 213 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:43:00 INFO TaskSchedulerImpl: Removed TaskSet 184.0, whose tasks have all completed, from pool 
15/05/13 10:43:00 INFO DAGScheduler: Stage 184 (reduce at JsonRDD.scala:51) finished in 0.225 s
15/05/13 10:43:00 INFO DAGScheduler: Job 188 finished: reduce at JsonRDD.scala:51, took 0.257118 s
15/05/13 10:43:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:43:00 INFO DAGScheduler: Got job 189 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:43:00 INFO DAGScheduler: Final stage: Stage 185(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:43:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:43:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:43:00 INFO DAGScheduler: Submitting Stage 185 (MapPartitionsRDD[1325] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=22931308, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_278 stored as values in memory (estimated size 20.4 KB, free 243.5 MB)
15/05/13 10:43:00 INFO MemoryStore: ensureFreeSpace(10904) called with curMem=22952228, maxMem=278302556
15/05/13 10:43:00 INFO MemoryStore: Block broadcast_278_piece0 stored as bytes in memory (estimated size 10.6 KB, free 243.5 MB)
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.5 MB)
15/05/13 10:43:00 INFO BlockManagerMaster: Updated info of block broadcast_278_piece0
15/05/13 10:43:00 INFO SparkContext: Created broadcast 278 from broadcast at DAGScheduler.scala:839
15/05/13 10:43:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 185 (MapPartitionsRDD[1325] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:43:00 INFO TaskSchedulerImpl: Adding task set 185.0 with 1 tasks
15/05/13 10:43:00 INFO TaskSetManager: Starting task 0.0 in stage 185.0 (TID 185, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:43:00 INFO BlockManagerInfo: Added broadcast_278_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:43:00 INFO TaskSetManager: Finished task 0.0 in stage 185.0 (TID 185) in 91 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:43:00 INFO TaskSchedulerImpl: Removed TaskSet 185.0, whose tasks have all completed, from pool 
15/05/13 10:43:00 INFO DAGScheduler: Stage 185 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.092 s
15/05/13 10:43:00 INFO DAGScheduler: Job 189 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.122305 s
15/05/13 10:43:00 INFO JobScheduler: Finished job streaming job 1431528180000 ms.0 from job set of time 1431528180000 ms
15/05/13 10:43:00 INFO JobScheduler: Total delay: 0.905 s for time 1431528180000 ms (execution: 0.719 s)
15/05/13 10:43:00 INFO MapPartitionsRDD: Removing RDD 1300 from persistence list
15/05/13 10:43:00 INFO BlockManager: Removing RDD 1300
15/05/13 10:43:00 INFO UnionRDD: Removing RDD 1299 from persistence list
15/05/13 10:43:00 INFO BlockManager: Removing RDD 1299
15/05/13 10:43:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528120000 ms: 1431528060000 ms
15/05/13 10:43:00 INFO JobGenerator: Checkpointing graph for time 1431528180000 ms
15/05/13 10:43:00 INFO DStreamGraph: Updating checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO DStreamGraph: Updated checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431528180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000'
15/05/13 10:43:00 INFO CheckpointWriter: Checkpoint for time 1431528180000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528180000', took 7656 bytes and 55 ms
15/05/13 10:43:00 INFO DStreamGraph: Clearing checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO DStreamGraph: Cleared checkpoint data for time 1431528180000 ms
15/05/13 10:43:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:44:00 INFO FileInputDStream: Finding new files took 61 ms
15/05/13 10:44:00 INFO FileInputDStream: New files at time 1431528240000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528142513.json
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 278
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_278_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_278_piece0 of size 10904 dropped from memory (free 255350328)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_278_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_278_piece0
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_278
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_278 of size 20920 dropped from memory (free 255371248)
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22931308, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_279 stored as values in memory (estimated size 232.9 KB, free 243.3 MB)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_278_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 278
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 274
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_274
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_274 of size 6040 dropped from memory (free 255138756)
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_274_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_274_piece0 of size 4226 dropped from memory (free 255142982)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_274_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_274_piece0
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_274_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 274
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 275
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_275_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_275_piece0 of size 11148 dropped from memory (free 255154130)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_275_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_275_piece0
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_275
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_275 of size 21480 dropped from memory (free 255175610)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_275_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 264.4 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 275
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 277
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_277
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_277 of size 6040 dropped from memory (free 255181650)
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_277_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_277_piece0 of size 4225 dropped from memory (free 255185875)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_277_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_277_piece0
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_277_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 277
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 269
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_269_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_269_piece0 of size 11140 dropped from memory (free 255197015)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_269_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.6 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_269_piece0
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_269
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_269 of size 21480 dropped from memory (free 255218495)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_269_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 269
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=23084061, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_279_piece0 stored as bytes in memory (estimated size 34.9 KB, free 243.4 MB)
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 271
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_271_piece0
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_279_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_271_piece0 of size 4225 dropped from memory (free 255187012)
15/05/13 10:44:00 INFO SparkContext: Created broadcast 279 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_271_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_271_piece0
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_271
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_271 of size 6040 dropped from memory (free 255193052)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_271_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 271
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 272
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_272
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_272 of size 20920 dropped from memory (free 255213972)
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_272_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_272_piece0 of size 10904 dropped from memory (free 255224876)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_272_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.5 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_272_piece0
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_272_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 272
15/05/13 10:44:00 INFO BlockManager: Removing broadcast 273
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_273_piece0
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_273_piece0 of size 35708 dropped from memory (free 255260584)
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_273_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.6 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_273_piece0
15/05/13 10:44:00 INFO BlockManager: Removing block broadcast_273
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_273 of size 238532 dropped from memory (free 255499116)
15/05/13 10:44:00 INFO JobScheduler: Starting job streaming job 1431528240000 ms.0 from job set of time 1431528240000 ms
15/05/13 10:44:00 INFO BlockManagerInfo: Removed broadcast_273_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 264.5 MB)
15/05/13 10:44:00 INFO JobScheduler: Added jobs for time 1431528240000 ms
15/05/13 10:44:00 INFO JobGenerator: Checkpointing graph for time 1431528240000 ms
15/05/13 10:44:00 INFO DStreamGraph: Updating checkpoint data for time 1431528240000 ms
15/05/13 10:44:00 INFO DStreamGraph: Updated checkpoint data for time 1431528240000 ms
15/05/13 10:44:00 INFO ContextCleaner: Cleaned broadcast 273
15/05/13 10:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431528240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000'
15/05/13 10:44:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83612): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:44:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83612): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431528240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000'
15/05/13 10:44:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:44:00 INFO DAGScheduler: Got job 190 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:44:00 INFO DAGScheduler: Final stage: Stage 186(reduce at JsonRDD.scala:51)
15/05/13 10:44:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:44:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83614): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431528240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000'
15/05/13 10:44:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:44:00 INFO DAGScheduler: Submitting Stage 186 (MapPartitionsRDD[1332] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=22803440, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_280 stored as values in memory (estimated size 5.9 KB, free 243.7 MB)
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=22809480, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_280_piece0 stored as bytes in memory (estimated size 4.1 KB, free 243.7 MB)
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.6 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_280_piece0
15/05/13 10:44:00 INFO SparkContext: Created broadcast 280 from broadcast at DAGScheduler.scala:839
15/05/13 10:44:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 186 (MapPartitionsRDD[1332] at map at JsonRDD.scala:51)
15/05/13 10:44:00 INFO TaskSchedulerImpl: Adding task set 186.0 with 1 tasks
15/05/13 10:44:00 INFO TaskSetManager: Starting task 0.0 in stage 186.0 (TID 186, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:44:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431527940000.bk
15/05/13 10:44:00 INFO CheckpointWriter: Checkpoint for time 1431528240000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000', took 7665 bytes and 144 ms
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_280_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_279_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO TaskSetManager: Finished task 0.0 in stage 186.0 (TID 186) in 243 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:44:00 INFO TaskSchedulerImpl: Removed TaskSet 186.0, whose tasks have all completed, from pool 
15/05/13 10:44:00 INFO DAGScheduler: Stage 186 (reduce at JsonRDD.scala:51) finished in 0.251 s
15/05/13 10:44:00 INFO DAGScheduler: Job 190 finished: reduce at JsonRDD.scala:51, took 0.284412 s
15/05/13 10:44:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:44:00 INFO DAGScheduler: Got job 191 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:44:00 INFO DAGScheduler: Final stage: Stage 187(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:44:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:44:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:44:00 INFO DAGScheduler: Submitting Stage 187 (MapPartitionsRDD[1339] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=22813705, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_281 stored as values in memory (estimated size 20.4 KB, free 243.6 MB)
15/05/13 10:44:00 INFO MemoryStore: ensureFreeSpace(10823) called with curMem=22834625, maxMem=278302556
15/05/13 10:44:00 INFO MemoryStore: Block broadcast_281_piece0 stored as bytes in memory (estimated size 10.6 KB, free 243.6 MB)
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.6 MB)
15/05/13 10:44:00 INFO BlockManagerMaster: Updated info of block broadcast_281_piece0
15/05/13 10:44:00 INFO SparkContext: Created broadcast 281 from broadcast at DAGScheduler.scala:839
15/05/13 10:44:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 187 (MapPartitionsRDD[1339] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:44:00 INFO TaskSchedulerImpl: Adding task set 187.0 with 1 tasks
15/05/13 10:44:00 INFO TaskSetManager: Starting task 0.0 in stage 187.0 (TID 187, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:44:00 INFO BlockManagerInfo: Added broadcast_281_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:44:00 INFO TaskSetManager: Finished task 0.0 in stage 187.0 (TID 187) in 89 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:44:00 INFO TaskSchedulerImpl: Removed TaskSet 187.0, whose tasks have all completed, from pool 
15/05/13 10:44:00 INFO DAGScheduler: Stage 187 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.091 s
15/05/13 10:44:00 INFO DAGScheduler: Job 191 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.125770 s
15/05/13 10:44:00 INFO JobScheduler: Finished job streaming job 1431528240000 ms.0 from job set of time 1431528240000 ms
15/05/13 10:44:00 INFO JobScheduler: Total delay: 0.959 s for time 1431528240000 ms (execution: 0.712 s)
15/05/13 10:44:00 INFO MapPartitionsRDD: Removing RDD 1314 from persistence list
15/05/13 10:44:00 INFO BlockManager: Removing RDD 1314
15/05/13 10:44:00 INFO UnionRDD: Removing RDD 1313 from persistence list
15/05/13 10:44:00 INFO BlockManager: Removing RDD 1313
15/05/13 10:44:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528180000 ms: 1431528120000 ms
15/05/13 10:44:00 INFO JobGenerator: Checkpointing graph for time 1431528240000 ms
15/05/13 10:44:00 INFO DStreamGraph: Updating checkpoint data for time 1431528240000 ms
15/05/13 10:44:00 INFO DStreamGraph: Updated checkpoint data for time 1431528240000 ms
15/05/13 10:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431528240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000'
15/05/13 10:44:01 INFO CheckpointWriter: Checkpoint for time 1431528240000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528240000', took 7654 bytes and 50 ms
15/05/13 10:44:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528240000 ms
15/05/13 10:44:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528240000 ms
15/05/13 10:44:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:45:00 INFO FileInputDStream: Finding new files took 30 ms
15/05/13 10:45:00 INFO FileInputDStream: New files at time 1431528300000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528208163.json
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=22845448, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_282 stored as values in memory (estimated size 232.9 KB, free 243.4 MB)
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=23083980, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_282_piece0 stored as bytes in memory (estimated size 34.9 KB, free 243.4 MB)
15/05/13 10:45:00 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:45:00 INFO BlockManagerMaster: Updated info of block broadcast_282_piece0
15/05/13 10:45:00 INFO SparkContext: Created broadcast 282 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:45:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:45:00 INFO JobScheduler: Added jobs for time 1431528300000 ms
15/05/13 10:45:00 INFO JobGenerator: Checkpointing graph for time 1431528300000 ms
15/05/13 10:45:00 INFO DStreamGraph: Updating checkpoint data for time 1431528300000 ms
15/05/13 10:45:00 INFO JobScheduler: Starting job streaming job 1431528300000 ms.0 from job set of time 1431528300000 ms
15/05/13 10:45:00 INFO DStreamGraph: Updated checkpoint data for time 1431528300000 ms
15/05/13 10:45:00 INFO CheckpointWriter: Saving checkpoint for time 1431528300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528300000'
15/05/13 10:45:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:45:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528000000.bk
15/05/13 10:45:00 INFO DAGScheduler: Got job 192 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:45:00 INFO DAGScheduler: Final stage: Stage 188(reduce at JsonRDD.scala:51)
15/05/13 10:45:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:45:00 INFO CheckpointWriter: Checkpoint for time 1431528300000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528300000', took 7663 bytes and 83 ms
15/05/13 10:45:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:45:00 INFO DAGScheduler: Submitting Stage 188 (MapPartitionsRDD[1346] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=23119688, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_283 stored as values in memory (estimated size 5.9 KB, free 243.4 MB)
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(4223) called with curMem=23125728, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_283_piece0 stored as bytes in memory (estimated size 4.1 KB, free 243.4 MB)
15/05/13 10:45:00 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:45:00 INFO BlockManagerMaster: Updated info of block broadcast_283_piece0
15/05/13 10:45:00 INFO SparkContext: Created broadcast 283 from broadcast at DAGScheduler.scala:839
15/05/13 10:45:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 188 (MapPartitionsRDD[1346] at map at JsonRDD.scala:51)
15/05/13 10:45:00 INFO TaskSchedulerImpl: Adding task set 188.0 with 1 tasks
15/05/13 10:45:00 INFO TaskSetManager: Starting task 0.0 in stage 188.0 (TID 188, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:45:00 INFO BlockManagerInfo: Added broadcast_283_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:45:00 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:45:00 INFO TaskSetManager: Finished task 0.0 in stage 188.0 (TID 188) in 431 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:45:00 INFO TaskSchedulerImpl: Removed TaskSet 188.0, whose tasks have all completed, from pool 
15/05/13 10:45:00 INFO DAGScheduler: Stage 188 (reduce at JsonRDD.scala:51) finished in 0.445 s
15/05/13 10:45:00 INFO DAGScheduler: Job 192 finished: reduce at JsonRDD.scala:51, took 0.477500 s
15/05/13 10:45:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:45:00 INFO DAGScheduler: Got job 193 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:45:00 INFO DAGScheduler: Final stage: Stage 189(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:45:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:45:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:45:00 INFO DAGScheduler: Submitting Stage 189 (MapPartitionsRDD[1353] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=23129951, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_284 stored as values in memory (estimated size 20.8 KB, free 243.3 MB)
15/05/13 10:45:00 INFO MemoryStore: ensureFreeSpace(11131) called with curMem=23151255, maxMem=278302556
15/05/13 10:45:00 INFO MemoryStore: Block broadcast_284_piece0 stored as bytes in memory (estimated size 10.9 KB, free 243.3 MB)
15/05/13 10:45:00 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.5 MB)
15/05/13 10:45:00 INFO BlockManagerMaster: Updated info of block broadcast_284_piece0
15/05/13 10:45:00 INFO SparkContext: Created broadcast 284 from broadcast at DAGScheduler.scala:839
15/05/13 10:45:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 189 (MapPartitionsRDD[1353] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:45:00 INFO TaskSchedulerImpl: Adding task set 189.0 with 1 tasks
15/05/13 10:45:00 INFO TaskSetManager: Starting task 0.0 in stage 189.0 (TID 189, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:45:01 INFO BlockManagerInfo: Added broadcast_284_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.3 MB)
15/05/13 10:45:01 INFO BlockManagerInfo: Added broadcast_282_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:45:01 INFO TaskSetManager: Finished task 0.0 in stage 189.0 (TID 189) in 216 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:45:01 INFO TaskSchedulerImpl: Removed TaskSet 189.0, whose tasks have all completed, from pool 
15/05/13 10:45:01 INFO DAGScheduler: Stage 189 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.219 s
15/05/13 10:45:01 INFO DAGScheduler: Job 193 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.262675 s
15/05/13 10:45:01 INFO JobScheduler: Finished job streaming job 1431528300000 ms.0 from job set of time 1431528300000 ms
15/05/13 10:45:01 INFO JobScheduler: Total delay: 1.252 s for time 1431528300000 ms (execution: 1.132 s)
15/05/13 10:45:01 INFO MapPartitionsRDD: Removing RDD 1328 from persistence list
15/05/13 10:45:01 INFO BlockManager: Removing RDD 1328
15/05/13 10:45:01 INFO UnionRDD: Removing RDD 1327 from persistence list
15/05/13 10:45:01 INFO BlockManager: Removing RDD 1327
15/05/13 10:45:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528240000 ms: 1431528180000 ms
15/05/13 10:45:01 INFO JobGenerator: Checkpointing graph for time 1431528300000 ms
15/05/13 10:45:01 INFO DStreamGraph: Updating checkpoint data for time 1431528300000 ms
15/05/13 10:45:01 INFO DStreamGraph: Updated checkpoint data for time 1431528300000 ms
15/05/13 10:45:01 INFO CheckpointWriter: Saving checkpoint for time 1431528300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528300000'
15/05/13 10:45:01 INFO CheckpointWriter: Checkpoint for time 1431528300000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528300000', took 7656 bytes and 63 ms
15/05/13 10:45:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528300000 ms
15/05/13 10:45:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528300000 ms
15/05/13 10:45:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:46:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 10:46:00 INFO FileInputDStream: New files at time 1431528360000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528269135.json
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=23162386, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_285 stored as values in memory (estimated size 232.9 KB, free 243.1 MB)
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=23400918, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_285_piece0 stored as bytes in memory (estimated size 34.9 KB, free 243.1 MB)
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:46:00 INFO BlockManagerMaster: Updated info of block broadcast_285_piece0
15/05/13 10:46:00 INFO SparkContext: Created broadcast 285 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:46:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:46:00 INFO JobScheduler: Added jobs for time 1431528360000 ms
15/05/13 10:46:00 INFO JobGenerator: Checkpointing graph for time 1431528360000 ms
15/05/13 10:46:00 INFO JobScheduler: Starting job streaming job 1431528360000 ms.0 from job set of time 1431528360000 ms
15/05/13 10:46:00 INFO DStreamGraph: Updating checkpoint data for time 1431528360000 ms
15/05/13 10:46:00 INFO DStreamGraph: Updated checkpoint data for time 1431528360000 ms
15/05/13 10:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431528360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000'
15/05/13 10:46:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83627): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431528360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000'
15/05/13 10:46:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:46:00 INFO DAGScheduler: Got job 194 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:46:00 INFO DAGScheduler: Final stage: Stage 190(reduce at JsonRDD.scala:51)
15/05/13 10:46:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:46:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:46:00 INFO DAGScheduler: Submitting Stage 190 (MapPartitionsRDD[1360] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=23436626, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_286 stored as values in memory (estimated size 5.9 KB, free 243.1 MB)
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=23442666, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_286_piece0 stored as bytes in memory (estimated size 4.1 KB, free 243.0 MB)
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:46:00 INFO BlockManagerMaster: Updated info of block broadcast_286_piece0
15/05/13 10:46:00 INFO SparkContext: Created broadcast 286 from broadcast at DAGScheduler.scala:839
15/05/13 10:46:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 190 (MapPartitionsRDD[1360] at map at JsonRDD.scala:51)
15/05/13 10:46:00 INFO TaskSchedulerImpl: Adding task set 190.0 with 1 tasks
15/05/13 10:46:00 INFO TaskSetManager: Starting task 0.0 in stage 190.0 (TID 190, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:46:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000
15/05/13 10:46:00 INFO CheckpointWriter: Checkpoint for time 1431528360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000', took 7665 bytes and 124 ms
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_286_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_285_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:46:00 INFO TaskSetManager: Finished task 0.0 in stage 190.0 (TID 190) in 227 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:46:00 INFO TaskSchedulerImpl: Removed TaskSet 190.0, whose tasks have all completed, from pool 
15/05/13 10:46:00 INFO DAGScheduler: Stage 190 (reduce at JsonRDD.scala:51) finished in 0.239 s
15/05/13 10:46:00 INFO DAGScheduler: Job 194 finished: reduce at JsonRDD.scala:51, took 0.270775 s
15/05/13 10:46:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:46:00 INFO DAGScheduler: Got job 195 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:46:00 INFO DAGScheduler: Final stage: Stage 191(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:46:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:46:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:46:00 INFO DAGScheduler: Submitting Stage 191 (MapPartitionsRDD[1367] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=23446891, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_287 stored as values in memory (estimated size 21.0 KB, free 243.0 MB)
15/05/13 10:46:00 INFO MemoryStore: ensureFreeSpace(11142) called with curMem=23468371, maxMem=278302556
15/05/13 10:46:00 INFO MemoryStore: Block broadcast_287_piece0 stored as bytes in memory (estimated size 10.9 KB, free 243.0 MB)
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.5 MB)
15/05/13 10:46:00 INFO BlockManagerMaster: Updated info of block broadcast_287_piece0
15/05/13 10:46:00 INFO SparkContext: Created broadcast 287 from broadcast at DAGScheduler.scala:839
15/05/13 10:46:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 191 (MapPartitionsRDD[1367] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:46:00 INFO TaskSchedulerImpl: Adding task set 191.0 with 1 tasks
15/05/13 10:46:00 INFO TaskSetManager: Starting task 0.0 in stage 191.0 (TID 191, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:46:00 INFO BlockManagerInfo: Added broadcast_287_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.2 MB)
15/05/13 10:46:00 INFO TaskSetManager: Finished task 0.0 in stage 191.0 (TID 191) in 106 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:46:00 INFO TaskSchedulerImpl: Removed TaskSet 191.0, whose tasks have all completed, from pool 
15/05/13 10:46:00 INFO DAGScheduler: Stage 191 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.109 s
15/05/13 10:46:00 INFO DAGScheduler: Job 195 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.139486 s
15/05/13 10:46:00 INFO JobScheduler: Finished job streaming job 1431528360000 ms.0 from job set of time 1431528360000 ms
15/05/13 10:46:00 INFO JobScheduler: Total delay: 0.990 s for time 1431528360000 ms (execution: 0.806 s)
15/05/13 10:46:00 INFO MapPartitionsRDD: Removing RDD 1342 from persistence list
15/05/13 10:46:00 INFO BlockManager: Removing RDD 1342
15/05/13 10:46:00 INFO UnionRDD: Removing RDD 1341 from persistence list
15/05/13 10:46:00 INFO BlockManager: Removing RDD 1341
15/05/13 10:46:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528300000 ms: 1431528240000 ms
15/05/13 10:46:00 INFO JobGenerator: Checkpointing graph for time 1431528360000 ms
15/05/13 10:46:00 INFO DStreamGraph: Updating checkpoint data for time 1431528360000 ms
15/05/13 10:46:00 INFO DStreamGraph: Updated checkpoint data for time 1431528360000 ms
15/05/13 10:46:01 INFO CheckpointWriter: Saving checkpoint for time 1431528360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000'
15/05/13 10:46:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83631): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:46:01 INFO CheckpointWriter: Saving checkpoint for time 1431528360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000'
15/05/13 10:46:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83633): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:46:01 INFO CheckpointWriter: Saving checkpoint for time 1431528360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000'
15/05/13 10:46:01 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528060000
15/05/13 10:46:01 INFO CheckpointWriter: Checkpoint for time 1431528360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000', took 7656 bytes and 163 ms
15/05/13 10:46:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528360000 ms
15/05/13 10:46:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528360000 ms
15/05/13 10:46:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:47:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 10:47:00 INFO FileInputDStream: New files at time 1431528420000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528330825.json
15/05/13 10:47:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=23479513, maxMem=278302556
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_288 stored as values in memory (estimated size 232.9 KB, free 242.8 MB)
15/05/13 10:47:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=23718045, maxMem=278302556
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_288_piece0 stored as bytes in memory (estimated size 34.9 KB, free 242.8 MB)
15/05/13 10:47:00 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.4 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_288_piece0
15/05/13 10:47:00 INFO SparkContext: Created broadcast 288 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:47:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:47:00 INFO JobScheduler: Added jobs for time 1431528420000 ms
15/05/13 10:47:00 INFO JobGenerator: Checkpointing graph for time 1431528420000 ms
15/05/13 10:47:00 INFO DStreamGraph: Updating checkpoint data for time 1431528420000 ms
15/05/13 10:47:00 INFO JobScheduler: Starting job streaming job 1431528420000 ms.0 from job set of time 1431528420000 ms
15/05/13 10:47:00 INFO DStreamGraph: Updated checkpoint data for time 1431528420000 ms
15/05/13 10:47:00 INFO CheckpointWriter: Saving checkpoint for time 1431528420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528420000'
15/05/13 10:47:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:47:00 INFO DAGScheduler: Got job 196 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:47:00 INFO DAGScheduler: Final stage: Stage 192(reduce at JsonRDD.scala:51)
15/05/13 10:47:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:47:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:47:00 INFO DAGScheduler: Submitting Stage 192 (MapPartitionsRDD[1374] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:47:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=23753755, maxMem=278302556
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_289 stored as values in memory (estimated size 5.9 KB, free 242.8 MB)
15/05/13 10:47:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528120000.bk
15/05/13 10:47:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=23759795, maxMem=278302556
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_289_piece0 stored as bytes in memory (estimated size 4.1 KB, free 242.7 MB)
15/05/13 10:47:00 INFO CheckpointWriter: Checkpoint for time 1431528420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528420000', took 7665 bytes and 92 ms
15/05/13 10:47:00 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_289_piece0
15/05/13 10:47:00 INFO SparkContext: Created broadcast 289 from broadcast at DAGScheduler.scala:839
15/05/13 10:47:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 192 (MapPartitionsRDD[1374] at map at JsonRDD.scala:51)
15/05/13 10:47:00 INFO TaskSchedulerImpl: Adding task set 192.0 with 1 tasks
15/05/13 10:47:00 INFO TaskSetManager: Starting task 0.0 in stage 192.0 (TID 192, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:47:00 INFO BlockManagerInfo: Added broadcast_289_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.5 MB)
15/05/13 10:47:00 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:47:00 INFO TaskSetManager: Finished task 0.0 in stage 192.0 (TID 192) in 403 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:47:00 INFO DAGScheduler: Stage 192 (reduce at JsonRDD.scala:51) finished in 0.412 s
15/05/13 10:47:00 INFO TaskSchedulerImpl: Removed TaskSet 192.0, whose tasks have all completed, from pool 
15/05/13 10:47:00 INFO DAGScheduler: Job 196 finished: reduce at JsonRDD.scala:51, took 0.452772 s
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 282
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_282
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_282 of size 238532 dropped from memory (free 254777068)
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_282_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_282_piece0 of size 35708 dropped from memory (free 254812776)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_282_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_282_piece0
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_282_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_282_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 282
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 281
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_281_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_281_piece0 of size 10823 dropped from memory (free 254823599)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_281_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_281_piece0
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_281
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_281 of size 20920 dropped from memory (free 254844519)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_281_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.2 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 281
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 280
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_280
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_280 of size 6040 dropped from memory (free 254850559)
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_280_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_280_piece0 of size 4225 dropped from memory (free 254854784)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_280_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_280_piece0
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_280_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 280
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 289
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_289
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_289 of size 6040 dropped from memory (free 254860824)
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_289_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_289_piece0 of size 4225 dropped from memory (free 254865049)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_289_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_289_piece0
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_289_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 289
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 287
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_287
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_287 of size 21480 dropped from memory (free 254886529)
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_287_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_287_piece0 of size 11142 dropped from memory (free 254897671)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_287_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_287_piece0
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_287_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.2 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 287
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 286
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_286_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_286_piece0 of size 4225 dropped from memory (free 254901896)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_286_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_286_piece0
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_286
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_286 of size 6040 dropped from memory (free 254907936)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_286_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 286
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 284
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_284_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_284_piece0 of size 11131 dropped from memory (free 254919067)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_284_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_284_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.3 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_284_piece0
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_284
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_284 of size 21304 dropped from memory (free 254940371)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 284
15/05/13 10:47:00 INFO BlockManager: Removing broadcast 283
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_283_piece0
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_283_piece0 of size 4223 dropped from memory (free 254944594)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_283_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:47:00 INFO BlockManagerMaster: Updated info of block broadcast_283_piece0
15/05/13 10:47:00 INFO BlockManager: Removing block broadcast_283
15/05/13 10:47:00 INFO MemoryStore: Block broadcast_283 of size 6040 dropped from memory (free 254950634)
15/05/13 10:47:00 INFO BlockManagerInfo: Removed broadcast_283_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:47:00 INFO ContextCleaner: Cleaned broadcast 283
15/05/13 10:47:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:47:01 INFO DAGScheduler: Got job 197 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:47:01 INFO DAGScheduler: Final stage: Stage 193(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:47:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:47:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:47:01 INFO DAGScheduler: Submitting Stage 193 (MapPartitionsRDD[1381] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:47:01 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=23351922, maxMem=278302556
15/05/13 10:47:01 INFO MemoryStore: Block broadcast_290 stored as values in memory (estimated size 21.0 KB, free 243.1 MB)
15/05/13 10:47:01 INFO MemoryStore: ensureFreeSpace(11294) called with curMem=23373426, maxMem=278302556
15/05/13 10:47:01 INFO MemoryStore: Block broadcast_290_piece0 stored as bytes in memory (estimated size 11.0 KB, free 243.1 MB)
15/05/13 10:47:01 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 262.5 MB)
15/05/13 10:47:01 INFO BlockManagerMaster: Updated info of block broadcast_290_piece0
15/05/13 10:47:01 INFO SparkContext: Created broadcast 290 from broadcast at DAGScheduler.scala:839
15/05/13 10:47:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 193 (MapPartitionsRDD[1381] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:47:01 INFO TaskSchedulerImpl: Adding task set 193.0 with 1 tasks
15/05/13 10:47:01 INFO TaskSetManager: Starting task 0.0 in stage 193.0 (TID 193, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:47:01 INFO BlockManagerInfo: Added broadcast_290_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 263.3 MB)
15/05/13 10:47:01 INFO BlockManagerInfo: Added broadcast_288_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.3 MB)
15/05/13 10:47:01 INFO TaskSetManager: Finished task 0.0 in stage 193.0 (TID 193) in 244 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:47:01 INFO TaskSchedulerImpl: Removed TaskSet 193.0, whose tasks have all completed, from pool 
15/05/13 10:47:01 INFO DAGScheduler: Stage 193 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.246 s
15/05/13 10:47:01 INFO DAGScheduler: Job 197 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.275376 s
15/05/13 10:47:01 INFO JobScheduler: Finished job streaming job 1431528420000 ms.0 from job set of time 1431528420000 ms
15/05/13 10:47:01 INFO JobScheduler: Total delay: 1.343 s for time 1431528420000 ms (execution: 1.144 s)
15/05/13 10:47:01 INFO MapPartitionsRDD: Removing RDD 1356 from persistence list
15/05/13 10:47:01 INFO BlockManager: Removing RDD 1356
15/05/13 10:47:01 INFO UnionRDD: Removing RDD 1355 from persistence list
15/05/13 10:47:01 INFO BlockManager: Removing RDD 1355
15/05/13 10:47:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528360000 ms: 1431528300000 ms
15/05/13 10:47:01 INFO JobGenerator: Checkpointing graph for time 1431528420000 ms
15/05/13 10:47:01 INFO DStreamGraph: Updating checkpoint data for time 1431528420000 ms
15/05/13 10:47:01 INFO DStreamGraph: Updated checkpoint data for time 1431528420000 ms
15/05/13 10:47:01 INFO CheckpointWriter: Saving checkpoint for time 1431528420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528420000'
15/05/13 10:47:01 INFO CheckpointWriter: Checkpoint for time 1431528420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528420000', took 7654 bytes and 52 ms
15/05/13 10:47:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528420000 ms
15/05/13 10:47:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528420000 ms
15/05/13 10:47:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:48:00 INFO FileInputDStream: Finding new files took 48 ms
15/05/13 10:48:00 INFO FileInputDStream: New files at time 1431528480000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528394809.json
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=23384720, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_291 stored as values in memory (estimated size 232.9 KB, free 242.9 MB)
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=23623252, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_291_piece0 stored as bytes in memory (estimated size 34.9 KB, free 242.8 MB)
15/05/13 10:48:00 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.5 MB)
15/05/13 10:48:00 INFO BlockManagerMaster: Updated info of block broadcast_291_piece0
15/05/13 10:48:00 INFO SparkContext: Created broadcast 291 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:48:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:48:00 INFO JobScheduler: Added jobs for time 1431528480000 ms
15/05/13 10:48:00 INFO JobGenerator: Checkpointing graph for time 1431528480000 ms
15/05/13 10:48:00 INFO DStreamGraph: Updating checkpoint data for time 1431528480000 ms
15/05/13 10:48:00 INFO JobScheduler: Starting job streaming job 1431528480000 ms.0 from job set of time 1431528480000 ms
15/05/13 10:48:00 INFO DStreamGraph: Updated checkpoint data for time 1431528480000 ms
15/05/13 10:48:00 INFO CheckpointWriter: Saving checkpoint for time 1431528480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000'
15/05/13 10:48:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83650): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:48:00 INFO CheckpointWriter: Saving checkpoint for time 1431528480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000'
15/05/13 10:48:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:48:00 INFO DAGScheduler: Got job 198 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:48:00 INFO DAGScheduler: Final stage: Stage 194(reduce at JsonRDD.scala:51)
15/05/13 10:48:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:48:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:48:00 INFO DAGScheduler: Submitting Stage 194 (MapPartitionsRDD[1388] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=23658960, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_292 stored as values in memory (estimated size 5.9 KB, free 242.8 MB)
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=23665000, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_292_piece0 stored as bytes in memory (estimated size 4.1 KB, free 242.8 MB)
15/05/13 10:48:00 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.5 MB)
15/05/13 10:48:00 INFO BlockManagerMaster: Updated info of block broadcast_292_piece0
15/05/13 10:48:00 INFO SparkContext: Created broadcast 292 from broadcast at DAGScheduler.scala:839
15/05/13 10:48:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 194 (MapPartitionsRDD[1388] at map at JsonRDD.scala:51)
15/05/13 10:48:00 INFO TaskSchedulerImpl: Adding task set 194.0 with 1 tasks
15/05/13 10:48:00 INFO TaskSetManager: Starting task 0.0 in stage 194.0 (TID 194, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:48:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83652): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:48:00 INFO CheckpointWriter: Saving checkpoint for time 1431528480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000'
15/05/13 10:48:00 INFO BlockManagerInfo: Added broadcast_292_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.3 MB)
15/05/13 10:48:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83654): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:48:00 WARN CheckpointWriter: Could not write checkpoint for time 1431528480000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000'
15/05/13 10:48:00 INFO BlockManagerInfo: Added broadcast_291_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:48:00 INFO TaskSetManager: Finished task 0.0 in stage 194.0 (TID 194) in 357 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:48:00 INFO TaskSchedulerImpl: Removed TaskSet 194.0, whose tasks have all completed, from pool 
15/05/13 10:48:00 INFO DAGScheduler: Stage 194 (reduce at JsonRDD.scala:51) finished in 0.368 s
15/05/13 10:48:00 INFO DAGScheduler: Job 198 finished: reduce at JsonRDD.scala:51, took 0.387866 s
15/05/13 10:48:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:48:00 INFO DAGScheduler: Got job 199 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:48:00 INFO DAGScheduler: Final stage: Stage 195(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:48:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:48:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:48:00 INFO DAGScheduler: Submitting Stage 195 (MapPartitionsRDD[1395] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=23669225, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_293 stored as values in memory (estimated size 20.6 KB, free 242.8 MB)
15/05/13 10:48:00 INFO MemoryStore: ensureFreeSpace(11027) called with curMem=23690321, maxMem=278302556
15/05/13 10:48:00 INFO MemoryStore: Block broadcast_293_piece0 stored as bytes in memory (estimated size 10.8 KB, free 242.8 MB)
15/05/13 10:48:00 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 262.4 MB)
15/05/13 10:48:00 INFO BlockManagerMaster: Updated info of block broadcast_293_piece0
15/05/13 10:48:00 INFO SparkContext: Created broadcast 293 from broadcast at DAGScheduler.scala:839
15/05/13 10:48:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 195 (MapPartitionsRDD[1395] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:48:00 INFO TaskSchedulerImpl: Adding task set 195.0 with 1 tasks
15/05/13 10:48:00 INFO TaskSetManager: Starting task 0.0 in stage 195.0 (TID 195, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:48:01 INFO BlockManagerInfo: Added broadcast_293_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 263.2 MB)
15/05/13 10:48:01 INFO TaskSetManager: Finished task 0.0 in stage 195.0 (TID 195) in 134 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:48:01 INFO TaskSchedulerImpl: Removed TaskSet 195.0, whose tasks have all completed, from pool 
15/05/13 10:48:01 INFO DAGScheduler: Stage 195 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.137 s
15/05/13 10:48:01 INFO DAGScheduler: Job 199 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.179959 s
15/05/13 10:48:01 INFO JobScheduler: Finished job streaming job 1431528480000 ms.0 from job set of time 1431528480000 ms
15/05/13 10:48:01 INFO JobScheduler: Total delay: 1.159 s for time 1431528480000 ms (execution: 0.970 s)
15/05/13 10:48:01 INFO MapPartitionsRDD: Removing RDD 1370 from persistence list
15/05/13 10:48:01 INFO BlockManager: Removing RDD 1370
15/05/13 10:48:01 INFO UnionRDD: Removing RDD 1369 from persistence list
15/05/13 10:48:01 INFO BlockManager: Removing RDD 1369
15/05/13 10:48:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528420000 ms: 1431528360000 ms
15/05/13 10:48:01 INFO JobGenerator: Checkpointing graph for time 1431528480000 ms
15/05/13 10:48:01 INFO DStreamGraph: Updating checkpoint data for time 1431528480000 ms
15/05/13 10:48:01 INFO DStreamGraph: Updated checkpoint data for time 1431528480000 ms
15/05/13 10:48:01 INFO CheckpointWriter: Saving checkpoint for time 1431528480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000'
15/05/13 10:48:01 INFO CheckpointWriter: Checkpoint for time 1431528480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000', took 7654 bytes and 59 ms
15/05/13 10:48:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528480000 ms
15/05/13 10:48:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528480000 ms
15/05/13 10:48:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:49:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 10:49:00 INFO FileInputDStream: New files at time 1431528540000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528455078.json
15/05/13 10:49:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=23701348, maxMem=278302556
15/05/13 10:49:00 INFO MemoryStore: Block broadcast_294 stored as values in memory (estimated size 232.9 KB, free 242.6 MB)
15/05/13 10:49:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=23939880, maxMem=278302556
15/05/13 10:49:00 INFO MemoryStore: Block broadcast_294_piece0 stored as bytes in memory (estimated size 34.9 KB, free 242.5 MB)
15/05/13 10:49:00 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.4 MB)
15/05/13 10:49:00 INFO BlockManagerMaster: Updated info of block broadcast_294_piece0
15/05/13 10:49:00 INFO SparkContext: Created broadcast 294 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:49:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:49:00 INFO JobScheduler: Starting job streaming job 1431528540000 ms.0 from job set of time 1431528540000 ms
15/05/13 10:49:00 INFO JobScheduler: Added jobs for time 1431528540000 ms
15/05/13 10:49:00 INFO JobGenerator: Checkpointing graph for time 1431528540000 ms
15/05/13 10:49:00 INFO DStreamGraph: Updating checkpoint data for time 1431528540000 ms
15/05/13 10:49:00 INFO DStreamGraph: Updated checkpoint data for time 1431528540000 ms
15/05/13 10:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431528540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000'
15/05/13 10:49:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:49:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83660): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431528540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000'
15/05/13 10:49:00 INFO DAGScheduler: Got job 200 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:49:00 INFO DAGScheduler: Final stage: Stage 196(reduce at JsonRDD.scala:51)
15/05/13 10:49:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:49:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:49:00 INFO DAGScheduler: Submitting Stage 196 (MapPartitionsRDD[1402] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:49:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=23975588, maxMem=278302556
15/05/13 10:49:00 INFO MemoryStore: Block broadcast_295 stored as values in memory (estimated size 5.9 KB, free 242.5 MB)
15/05/13 10:49:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=23981628, maxMem=278302556
15/05/13 10:49:00 INFO MemoryStore: Block broadcast_295_piece0 stored as bytes in memory (estimated size 4.1 KB, free 242.5 MB)
15/05/13 10:49:00 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:49:00 INFO BlockManagerMaster: Updated info of block broadcast_295_piece0
15/05/13 10:49:00 INFO SparkContext: Created broadcast 295 from broadcast at DAGScheduler.scala:839
15/05/13 10:49:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 196 (MapPartitionsRDD[1402] at map at JsonRDD.scala:51)
15/05/13 10:49:00 INFO TaskSchedulerImpl: Adding task set 196.0 with 1 tasks
15/05/13 10:49:00 INFO TaskSetManager: Starting task 0.0 in stage 196.0 (TID 196, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:49:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83662): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431528540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000'
15/05/13 10:49:00 INFO BlockManagerInfo: Added broadcast_295_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:49:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000
15/05/13 10:49:00 INFO CheckpointWriter: Checkpoint for time 1431528540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000', took 7665 bytes and 240 ms
15/05/13 10:49:00 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:49:00 INFO TaskSetManager: Finished task 0.0 in stage 196.0 (TID 196) in 478 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:49:00 INFO TaskSchedulerImpl: Removed TaskSet 196.0, whose tasks have all completed, from pool 
15/05/13 10:49:00 INFO DAGScheduler: Stage 196 (reduce at JsonRDD.scala:51) finished in 0.495 s
15/05/13 10:49:00 INFO DAGScheduler: Job 200 finished: reduce at JsonRDD.scala:51, took 0.537790 s
15/05/13 10:49:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:49:01 INFO DAGScheduler: Got job 201 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:49:01 INFO DAGScheduler: Final stage: Stage 197(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:49:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:49:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:49:01 INFO DAGScheduler: Submitting Stage 197 (MapPartitionsRDD[1409] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:49:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=23985853, maxMem=278302556
15/05/13 10:49:01 INFO MemoryStore: Block broadcast_296 stored as values in memory (estimated size 20.6 KB, free 242.5 MB)
15/05/13 10:49:01 INFO MemoryStore: ensureFreeSpace(11036) called with curMem=24006949, maxMem=278302556
15/05/13 10:49:01 INFO MemoryStore: Block broadcast_296_piece0 stored as bytes in memory (estimated size 10.8 KB, free 242.5 MB)
15/05/13 10:49:01 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 262.4 MB)
15/05/13 10:49:01 INFO BlockManagerMaster: Updated info of block broadcast_296_piece0
15/05/13 10:49:01 INFO SparkContext: Created broadcast 296 from broadcast at DAGScheduler.scala:839
15/05/13 10:49:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 197 (MapPartitionsRDD[1409] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:49:01 INFO TaskSchedulerImpl: Adding task set 197.0 with 1 tasks
15/05/13 10:49:01 INFO TaskSetManager: Starting task 0.0 in stage 197.0 (TID 197, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:49:01 INFO BlockManagerInfo: Added broadcast_296_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.8 KB, free: 264.4 MB)
15/05/13 10:49:01 INFO BlockManagerInfo: Added broadcast_294_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:49:01 INFO TaskSetManager: Finished task 0.0 in stage 197.0 (TID 197) in 369 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:49:01 INFO TaskSchedulerImpl: Removed TaskSet 197.0, whose tasks have all completed, from pool 
15/05/13 10:49:01 INFO DAGScheduler: Stage 197 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.373 s
15/05/13 10:49:01 INFO DAGScheduler: Job 201 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.400108 s
15/05/13 10:49:01 INFO JobScheduler: Finished job streaming job 1431528540000 ms.0 from job set of time 1431528540000 ms
15/05/13 10:49:01 INFO JobScheduler: Total delay: 1.437 s for time 1431528540000 ms (execution: 1.238 s)
15/05/13 10:49:01 INFO MapPartitionsRDD: Removing RDD 1384 from persistence list
15/05/13 10:49:01 INFO BlockManager: Removing RDD 1384
15/05/13 10:49:01 INFO UnionRDD: Removing RDD 1383 from persistence list
15/05/13 10:49:01 INFO BlockManager: Removing RDD 1383
15/05/13 10:49:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528480000 ms: 1431528420000 ms
15/05/13 10:49:01 INFO JobGenerator: Checkpointing graph for time 1431528540000 ms
15/05/13 10:49:01 INFO DStreamGraph: Updating checkpoint data for time 1431528540000 ms
15/05/13 10:49:01 INFO DStreamGraph: Updated checkpoint data for time 1431528540000 ms
15/05/13 10:49:01 INFO CheckpointWriter: Saving checkpoint for time 1431528540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000'
15/05/13 10:49:01 INFO CheckpointWriter: Checkpoint for time 1431528540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000', took 7657 bytes and 61 ms
15/05/13 10:49:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528540000 ms
15/05/13 10:49:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528540000 ms
15/05/13 10:49:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:50:00 INFO FileInputDStream: Finding new files took 45 ms
15/05/13 10:50:00 INFO FileInputDStream: New files at time 1431528600000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528516535.json
15/05/13 10:50:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=24017985, maxMem=278302556
15/05/13 10:50:00 INFO MemoryStore: Block broadcast_297 stored as values in memory (estimated size 232.9 KB, free 242.3 MB)
15/05/13 10:50:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=24256517, maxMem=278302556
15/05/13 10:50:00 INFO MemoryStore: Block broadcast_297_piece0 stored as bytes in memory (estimated size 34.9 KB, free 242.2 MB)
15/05/13 10:50:00 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.4 MB)
15/05/13 10:50:00 INFO BlockManagerMaster: Updated info of block broadcast_297_piece0
15/05/13 10:50:00 INFO SparkContext: Created broadcast 297 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:50:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:50:00 INFO JobScheduler: Added jobs for time 1431528600000 ms
15/05/13 10:50:00 INFO JobGenerator: Checkpointing graph for time 1431528600000 ms
15/05/13 10:50:00 INFO DStreamGraph: Updating checkpoint data for time 1431528600000 ms
15/05/13 10:50:00 INFO JobScheduler: Starting job streaming job 1431528600000 ms.0 from job set of time 1431528600000 ms
15/05/13 10:50:00 INFO DStreamGraph: Updated checkpoint data for time 1431528600000 ms
15/05/13 10:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431528600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528600000'
15/05/13 10:50:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:50:00 INFO DAGScheduler: Got job 202 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:50:00 INFO DAGScheduler: Final stage: Stage 198(reduce at JsonRDD.scala:51)
15/05/13 10:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:50:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528300000.bk
15/05/13 10:50:00 INFO CheckpointWriter: Checkpoint for time 1431528600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528600000', took 7666 bytes and 89 ms
15/05/13 10:50:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:50:00 INFO DAGScheduler: Submitting Stage 198 (MapPartitionsRDD[1416] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:50:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=24292225, maxMem=278302556
15/05/13 10:50:00 INFO MemoryStore: Block broadcast_298 stored as values in memory (estimated size 5.9 KB, free 242.2 MB)
15/05/13 10:50:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=24298265, maxMem=278302556
15/05/13 10:50:00 INFO MemoryStore: Block broadcast_298_piece0 stored as bytes in memory (estimated size 4.1 KB, free 242.2 MB)
15/05/13 10:50:00 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:50:00 INFO BlockManagerMaster: Updated info of block broadcast_298_piece0
15/05/13 10:50:00 INFO SparkContext: Created broadcast 298 from broadcast at DAGScheduler.scala:839
15/05/13 10:50:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 198 (MapPartitionsRDD[1416] at map at JsonRDD.scala:51)
15/05/13 10:50:00 INFO TaskSchedulerImpl: Adding task set 198.0 with 1 tasks
15/05/13 10:50:00 INFO TaskSetManager: Starting task 0.0 in stage 198.0 (TID 198, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:50:00 INFO BlockManagerInfo: Added broadcast_298_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:50:00 INFO BlockManagerInfo: Added broadcast_297_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.4 MB)
15/05/13 10:50:00 INFO TaskSetManager: Finished task 0.0 in stage 198.0 (TID 198) in 577 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:50:00 INFO TaskSchedulerImpl: Removed TaskSet 198.0, whose tasks have all completed, from pool 
15/05/13 10:50:00 INFO DAGScheduler: Stage 198 (reduce at JsonRDD.scala:51) finished in 0.596 s
15/05/13 10:50:00 INFO DAGScheduler: Job 202 finished: reduce at JsonRDD.scala:51, took 0.623088 s
15/05/13 10:50:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:50:00 INFO DAGScheduler: Got job 203 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:50:00 INFO DAGScheduler: Final stage: Stage 199(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:50:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:50:01 INFO DAGScheduler: Submitting Stage 199 (MapPartitionsRDD[1423] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:50:01 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=24302490, maxMem=278302556
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_299 stored as values in memory (estimated size 21.0 KB, free 242.2 MB)
15/05/13 10:50:01 INFO MemoryStore: ensureFreeSpace(11288) called with curMem=24323994, maxMem=278302556
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_299_piece0 stored as bytes in memory (estimated size 11.0 KB, free 242.2 MB)
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 290
15/05/13 10:50:01 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 262.3 MB)
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_290_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_290_piece0 of size 11294 dropped from memory (free 253978568)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_290_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_299_piece0
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_290_piece0
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_290
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_290 of size 21504 dropped from memory (free 254000072)
15/05/13 10:50:01 INFO SparkContext: Created broadcast 299 from broadcast at DAGScheduler.scala:839
15/05/13 10:50:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 199 (MapPartitionsRDD[1423] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:50:01 INFO TaskSchedulerImpl: Adding task set 199.0 with 1 tasks
15/05/13 10:50:01 INFO TaskSetManager: Starting task 0.0 in stage 199.0 (TID 199, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_290_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 263.2 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 290
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 298
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_298
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_298 of size 6040 dropped from memory (free 254006112)
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_298_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_298_piece0 of size 4225 dropped from memory (free 254010337)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_298_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_298_piece0
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_298_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:50:01 INFO BlockManagerInfo: Added broadcast_299_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 11.0 KB, free: 264.4 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 298
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 293
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_293_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_293_piece0 of size 11027 dropped from memory (free 254021364)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_293_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_293_piece0
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_293
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_293 of size 21096 dropped from memory (free 254042460)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_293_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 263.2 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 293
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 295
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_295_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_295_piece0 of size 4225 dropped from memory (free 254046685)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_295_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_295_piece0
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_295
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_295 of size 6040 dropped from memory (free 254052725)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_295_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 295
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 296
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_296_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_296_piece0 of size 11036 dropped from memory (free 254063761)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_296_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_296_piece0
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_296
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_296 of size 21096 dropped from memory (free 254084857)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_296_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.8 KB, free: 264.4 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 296
15/05/13 10:50:01 INFO BlockManager: Removing broadcast 292
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_292_piece0
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_292_piece0 of size 4225 dropped from memory (free 254089082)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_292_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:50:01 INFO BlockManagerMaster: Updated info of block broadcast_292_piece0
15/05/13 10:50:01 INFO BlockManager: Removing block broadcast_292
15/05/13 10:50:01 INFO MemoryStore: Block broadcast_292 of size 6040 dropped from memory (free 254095122)
15/05/13 10:50:01 INFO BlockManagerInfo: Removed broadcast_292_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:50:01 INFO ContextCleaner: Cleaned broadcast 292
15/05/13 10:50:01 INFO TaskSetManager: Finished task 0.0 in stage 199.0 (TID 199) in 175 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:50:01 INFO TaskSchedulerImpl: Removed TaskSet 199.0, whose tasks have all completed, from pool 
15/05/13 10:50:01 INFO DAGScheduler: Stage 199 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.176 s
15/05/13 10:50:01 INFO DAGScheduler: Job 203 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.259462 s
15/05/13 10:50:01 INFO JobScheduler: Finished job streaming job 1431528600000 ms.0 from job set of time 1431528600000 ms
15/05/13 10:50:01 INFO JobScheduler: Total delay: 1.289 s for time 1431528600000 ms (execution: 1.177 s)
15/05/13 10:50:01 INFO MapPartitionsRDD: Removing RDD 1398 from persistence list
15/05/13 10:50:01 INFO BlockManager: Removing RDD 1398
15/05/13 10:50:01 INFO UnionRDD: Removing RDD 1397 from persistence list
15/05/13 10:50:01 INFO BlockManager: Removing RDD 1397
15/05/13 10:50:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528540000 ms: 1431528480000 ms
15/05/13 10:50:01 INFO JobGenerator: Checkpointing graph for time 1431528600000 ms
15/05/13 10:50:01 INFO DStreamGraph: Updating checkpoint data for time 1431528600000 ms
15/05/13 10:50:01 INFO DStreamGraph: Updated checkpoint data for time 1431528600000 ms
15/05/13 10:50:01 INFO CheckpointWriter: Saving checkpoint for time 1431528600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528600000'
15/05/13 10:50:01 INFO CheckpointWriter: Checkpoint for time 1431528600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528600000', took 7657 bytes and 42 ms
15/05/13 10:50:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528600000 ms
15/05/13 10:50:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528600000 ms
15/05/13 10:50:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:51:00 INFO FileInputDStream: Finding new files took 48 ms
15/05/13 10:51:00 INFO FileInputDStream: New files at time 1431528660000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528576849.json
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=24207434, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_300 stored as values in memory (estimated size 232.9 KB, free 242.1 MB)
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=24445966, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_300_piece0 stored as bytes in memory (estimated size 34.9 KB, free 242.1 MB)
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.4 MB)
15/05/13 10:51:00 INFO BlockManagerMaster: Updated info of block broadcast_300_piece0
15/05/13 10:51:00 INFO SparkContext: Created broadcast 300 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:51:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:51:00 INFO JobScheduler: Added jobs for time 1431528660000 ms
15/05/13 10:51:00 INFO JobGenerator: Checkpointing graph for time 1431528660000 ms
15/05/13 10:51:00 INFO JobScheduler: Starting job streaming job 1431528660000 ms.0 from job set of time 1431528660000 ms
15/05/13 10:51:00 INFO DStreamGraph: Updating checkpoint data for time 1431528660000 ms
15/05/13 10:51:00 INFO DStreamGraph: Updated checkpoint data for time 1431528660000 ms
15/05/13 10:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431528660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000'
15/05/13 10:51:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83676): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431528660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000'
15/05/13 10:51:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:51:00 INFO DAGScheduler: Got job 204 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:51:00 INFO DAGScheduler: Final stage: Stage 200(reduce at JsonRDD.scala:51)
15/05/13 10:51:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:51:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:51:00 INFO DAGScheduler: Submitting Stage 200 (MapPartitionsRDD[1430] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=24481674, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_301 stored as values in memory (estimated size 5.9 KB, free 242.1 MB)
15/05/13 10:51:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83678): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=24487714, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_301_piece0 stored as bytes in memory (estimated size 4.1 KB, free 242.1 MB)
15/05/13 10:51:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83678): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431528660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000'
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.4 MB)
15/05/13 10:51:00 INFO BlockManagerMaster: Updated info of block broadcast_301_piece0
15/05/13 10:51:00 INFO SparkContext: Created broadcast 301 from broadcast at DAGScheduler.scala:839
15/05/13 10:51:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 200 (MapPartitionsRDD[1430] at map at JsonRDD.scala:51)
15/05/13 10:51:00 INFO TaskSchedulerImpl: Adding task set 200.0 with 1 tasks
15/05/13 10:51:00 INFO TaskSetManager: Starting task 0.0 in stage 200.0 (TID 200, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_301_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:51:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528360000.bk
15/05/13 10:51:00 INFO CheckpointWriter: Checkpoint for time 1431528660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000', took 7662 bytes and 158 ms
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_300_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:51:00 INFO TaskSetManager: Finished task 0.0 in stage 200.0 (TID 200) in 303 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:51:00 INFO TaskSchedulerImpl: Removed TaskSet 200.0, whose tasks have all completed, from pool 
15/05/13 10:51:00 INFO DAGScheduler: Stage 200 (reduce at JsonRDD.scala:51) finished in 0.307 s
15/05/13 10:51:00 INFO DAGScheduler: Job 204 finished: reduce at JsonRDD.scala:51, took 0.345240 s
15/05/13 10:51:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:51:00 INFO DAGScheduler: Got job 205 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:51:00 INFO DAGScheduler: Final stage: Stage 201(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:51:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:51:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:51:00 INFO DAGScheduler: Submitting Stage 201 (MapPartitionsRDD[1437] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=24491939, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_302 stored as values in memory (estimated size 21.0 KB, free 242.0 MB)
15/05/13 10:51:00 INFO MemoryStore: ensureFreeSpace(11138) called with curMem=24513419, maxMem=278302556
15/05/13 10:51:00 INFO MemoryStore: Block broadcast_302_piece0 stored as bytes in memory (estimated size 10.9 KB, free 242.0 MB)
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.3 MB)
15/05/13 10:51:00 INFO BlockManagerMaster: Updated info of block broadcast_302_piece0
15/05/13 10:51:00 INFO SparkContext: Created broadcast 302 from broadcast at DAGScheduler.scala:839
15/05/13 10:51:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 201 (MapPartitionsRDD[1437] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:51:00 INFO TaskSchedulerImpl: Adding task set 201.0 with 1 tasks
15/05/13 10:51:00 INFO TaskSetManager: Starting task 0.0 in stage 201.0 (TID 201, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:51:00 INFO BlockManagerInfo: Added broadcast_302_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.2 MB)
15/05/13 10:51:00 INFO TaskSetManager: Finished task 0.0 in stage 201.0 (TID 201) in 158 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:51:00 INFO TaskSchedulerImpl: Removed TaskSet 201.0, whose tasks have all completed, from pool 
15/05/13 10:51:00 INFO DAGScheduler: Stage 201 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.158 s
15/05/13 10:51:01 INFO DAGScheduler: Job 205 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.211420 s
15/05/13 10:51:01 INFO JobScheduler: Finished job streaming job 1431528660000 ms.0 from job set of time 1431528660000 ms
15/05/13 10:51:01 INFO JobScheduler: Total delay: 1.027 s for time 1431528660000 ms (execution: 0.834 s)
15/05/13 10:51:01 INFO MapPartitionsRDD: Removing RDD 1412 from persistence list
15/05/13 10:51:01 INFO BlockManager: Removing RDD 1412
15/05/13 10:51:01 INFO UnionRDD: Removing RDD 1411 from persistence list
15/05/13 10:51:01 INFO BlockManager: Removing RDD 1411
15/05/13 10:51:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528600000 ms: 1431528540000 ms
15/05/13 10:51:01 INFO JobGenerator: Checkpointing graph for time 1431528660000 ms
15/05/13 10:51:01 INFO DStreamGraph: Updating checkpoint data for time 1431528660000 ms
15/05/13 10:51:01 INFO DStreamGraph: Updated checkpoint data for time 1431528660000 ms
15/05/13 10:51:01 INFO CheckpointWriter: Saving checkpoint for time 1431528660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000'
15/05/13 10:51:01 INFO CheckpointWriter: Checkpoint for time 1431528660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000', took 7654 bytes and 53 ms
15/05/13 10:51:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528660000 ms
15/05/13 10:51:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528660000 ms
15/05/13 10:51:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:52:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 10:52:00 INFO FileInputDStream: New files at time 1431528720000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528637323.json
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=24524557, maxMem=278302556
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_303 stored as values in memory (estimated size 232.9 KB, free 241.8 MB)
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=24763089, maxMem=278302556
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_303_piece0 stored as bytes in memory (estimated size 34.9 KB, free 241.8 MB)
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.3 MB)
15/05/13 10:52:00 INFO BlockManagerMaster: Updated info of block broadcast_303_piece0
15/05/13 10:52:00 INFO SparkContext: Created broadcast 303 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:52:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:52:00 INFO JobScheduler: Added jobs for time 1431528720000 ms
15/05/13 10:52:00 INFO JobGenerator: Checkpointing graph for time 1431528720000 ms
15/05/13 10:52:00 INFO DStreamGraph: Updating checkpoint data for time 1431528720000 ms
15/05/13 10:52:00 INFO DStreamGraph: Updated checkpoint data for time 1431528720000 ms
15/05/13 10:52:00 INFO JobScheduler: Starting job streaming job 1431528720000 ms.0 from job set of time 1431528720000 ms
15/05/13 10:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431528720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000'
15/05/13 10:52:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83685): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431528720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000'
15/05/13 10:52:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:52:00 INFO DAGScheduler: Got job 206 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:52:00 INFO DAGScheduler: Final stage: Stage 202(reduce at JsonRDD.scala:51)
15/05/13 10:52:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:52:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:52:00 INFO DAGScheduler: Submitting Stage 202 (MapPartitionsRDD[1444] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=24798797, maxMem=278302556
15/05/13 10:52:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83687): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431528720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000'
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_304 stored as values in memory (estimated size 5.9 KB, free 241.8 MB)
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=24804837, maxMem=278302556
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_304_piece0 stored as bytes in memory (estimated size 4.1 KB, free 241.8 MB)
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.3 MB)
15/05/13 10:52:00 INFO BlockManagerMaster: Updated info of block broadcast_304_piece0
15/05/13 10:52:00 INFO SparkContext: Created broadcast 304 from broadcast at DAGScheduler.scala:839
15/05/13 10:52:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 202 (MapPartitionsRDD[1444] at map at JsonRDD.scala:51)
15/05/13 10:52:00 INFO TaskSchedulerImpl: Adding task set 202.0 with 1 tasks
15/05/13 10:52:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83689): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:52:00 INFO TaskSetManager: Starting task 0.0 in stage 202.0 (TID 202, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:52:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83689): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:52:00 WARN CheckpointWriter: Could not write checkpoint for time 1431528720000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000'
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_304_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_303_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.2 MB)
15/05/13 10:52:00 INFO DAGScheduler: Stage 202 (reduce at JsonRDD.scala:51) finished in 0.407 s
15/05/13 10:52:00 INFO DAGScheduler: Job 206 finished: reduce at JsonRDD.scala:51, took 0.448944 s
15/05/13 10:52:00 INFO TaskSetManager: Finished task 0.0 in stage 202.0 (TID 202) in 393 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:52:00 INFO TaskSchedulerImpl: Removed TaskSet 202.0, whose tasks have all completed, from pool 
15/05/13 10:52:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:52:00 INFO DAGScheduler: Got job 207 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:52:00 INFO DAGScheduler: Final stage: Stage 203(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:52:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:52:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:52:00 INFO DAGScheduler: Submitting Stage 203 (MapPartitionsRDD[1451] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=24809062, maxMem=278302556
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_305 stored as values in memory (estimated size 20.6 KB, free 241.7 MB)
15/05/13 10:52:00 INFO MemoryStore: ensureFreeSpace(11026) called with curMem=24830158, maxMem=278302556
15/05/13 10:52:00 INFO MemoryStore: Block broadcast_305_piece0 stored as bytes in memory (estimated size 10.8 KB, free 241.7 MB)
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 262.3 MB)
15/05/13 10:52:00 INFO BlockManagerMaster: Updated info of block broadcast_305_piece0
15/05/13 10:52:00 INFO SparkContext: Created broadcast 305 from broadcast at DAGScheduler.scala:839
15/05/13 10:52:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 203 (MapPartitionsRDD[1451] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:52:00 INFO TaskSchedulerImpl: Adding task set 203.0 with 1 tasks
15/05/13 10:52:00 INFO TaskSetManager: Starting task 0.0 in stage 203.0 (TID 203, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:52:00 INFO BlockManagerInfo: Added broadcast_305_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 263.2 MB)
15/05/13 10:52:01 INFO TaskSetManager: Finished task 0.0 in stage 203.0 (TID 203) in 134 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:52:01 INFO TaskSchedulerImpl: Removed TaskSet 203.0, whose tasks have all completed, from pool 
15/05/13 10:52:01 INFO DAGScheduler: Stage 203 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.136 s
15/05/13 10:52:01 INFO DAGScheduler: Job 207 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.154138 s
15/05/13 10:52:01 INFO JobScheduler: Finished job streaming job 1431528720000 ms.0 from job set of time 1431528720000 ms
15/05/13 10:52:01 INFO JobScheduler: Total delay: 1.056 s for time 1431528720000 ms (execution: 0.932 s)
15/05/13 10:52:01 INFO MapPartitionsRDD: Removing RDD 1426 from persistence list
15/05/13 10:52:01 INFO BlockManager: Removing RDD 1426
15/05/13 10:52:01 INFO UnionRDD: Removing RDD 1425 from persistence list
15/05/13 10:52:01 INFO BlockManager: Removing RDD 1425
15/05/13 10:52:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528660000 ms: 1431528600000 ms
15/05/13 10:52:01 INFO JobGenerator: Checkpointing graph for time 1431528720000 ms
15/05/13 10:52:01 INFO DStreamGraph: Updating checkpoint data for time 1431528720000 ms
15/05/13 10:52:01 INFO DStreamGraph: Updated checkpoint data for time 1431528720000 ms
15/05/13 10:52:01 INFO CheckpointWriter: Saving checkpoint for time 1431528720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000'
15/05/13 10:52:01 INFO CheckpointWriter: Checkpoint for time 1431528720000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000', took 7657 bytes and 51 ms
15/05/13 10:52:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528720000 ms
15/05/13 10:52:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528720000 ms
15/05/13 10:52:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:53:00 INFO FileInputDStream: Finding new files took 25 ms
15/05/13 10:53:00 INFO FileInputDStream: New files at time 1431528780000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528701854.json
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=24841184, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_306 stored as values in memory (estimated size 232.9 KB, free 241.5 MB)
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=25079716, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_306_piece0 stored as bytes in memory (estimated size 34.9 KB, free 241.5 MB)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.3 MB)
15/05/13 10:53:00 INFO BlockManagerMaster: Updated info of block broadcast_306_piece0
15/05/13 10:53:00 INFO SparkContext: Created broadcast 306 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:53:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:53:00 INFO JobScheduler: Added jobs for time 1431528780000 ms
15/05/13 10:53:00 INFO JobGenerator: Checkpointing graph for time 1431528780000 ms
15/05/13 10:53:00 INFO DStreamGraph: Updating checkpoint data for time 1431528780000 ms
15/05/13 10:53:00 INFO JobScheduler: Starting job streaming job 1431528780000 ms.0 from job set of time 1431528780000 ms
15/05/13 10:53:00 INFO DStreamGraph: Updated checkpoint data for time 1431528780000 ms
15/05/13 10:53:00 INFO CheckpointWriter: Saving checkpoint for time 1431528780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528780000'
15/05/13 10:53:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:53:00 INFO DAGScheduler: Got job 208 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:53:00 INFO DAGScheduler: Final stage: Stage 204(reduce at JsonRDD.scala:51)
15/05/13 10:53:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:53:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528480000.bk
15/05/13 10:53:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:53:00 INFO CheckpointWriter: Checkpoint for time 1431528780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528780000', took 7666 bytes and 74 ms
15/05/13 10:53:00 INFO DAGScheduler: Submitting Stage 204 (MapPartitionsRDD[1458] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=25115425, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_307 stored as values in memory (estimated size 5.9 KB, free 241.5 MB)
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=25121465, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_307_piece0 stored as bytes in memory (estimated size 4.1 KB, free 241.4 MB)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.3 MB)
15/05/13 10:53:00 INFO BlockManagerMaster: Updated info of block broadcast_307_piece0
15/05/13 10:53:00 INFO SparkContext: Created broadcast 307 from broadcast at DAGScheduler.scala:839
15/05/13 10:53:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 204 (MapPartitionsRDD[1458] at map at JsonRDD.scala:51)
15/05/13 10:53:00 INFO TaskSchedulerImpl: Adding task set 204.0 with 1 tasks
15/05/13 10:53:00 INFO TaskSetManager: Starting task 0.0 in stage 204.0 (TID 204, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_307_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.4 MB)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:53:00 INFO TaskSetManager: Finished task 0.0 in stage 204.0 (TID 204) in 444 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:53:00 INFO TaskSchedulerImpl: Removed TaskSet 204.0, whose tasks have all completed, from pool 
15/05/13 10:53:00 INFO DAGScheduler: Stage 204 (reduce at JsonRDD.scala:51) finished in 0.458 s
15/05/13 10:53:00 INFO DAGScheduler: Job 208 finished: reduce at JsonRDD.scala:51, took 0.490493 s
15/05/13 10:53:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:53:00 INFO DAGScheduler: Got job 209 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:53:00 INFO DAGScheduler: Final stage: Stage 205(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:53:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:53:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:53:00 INFO DAGScheduler: Submitting Stage 205 (MapPartitionsRDD[1465] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=25125690, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_308 stored as values in memory (estimated size 20.4 KB, free 241.4 MB)
15/05/13 10:53:00 INFO MemoryStore: ensureFreeSpace(10824) called with curMem=25146610, maxMem=278302556
15/05/13 10:53:00 INFO MemoryStore: Block broadcast_308_piece0 stored as bytes in memory (estimated size 10.6 KB, free 241.4 MB)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.2 MB)
15/05/13 10:53:00 INFO BlockManagerMaster: Updated info of block broadcast_308_piece0
15/05/13 10:53:00 INFO SparkContext: Created broadcast 308 from broadcast at DAGScheduler.scala:839
15/05/13 10:53:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 205 (MapPartitionsRDD[1465] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:53:00 INFO TaskSchedulerImpl: Adding task set 205.0 with 1 tasks
15/05/13 10:53:00 INFO TaskSetManager: Starting task 0.0 in stage 205.0 (TID 205, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:53:00 INFO BlockManagerInfo: Added broadcast_308_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 263.2 MB)
15/05/13 10:53:01 INFO BlockManagerInfo: Added broadcast_306_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:53:01 INFO TaskSetManager: Finished task 0.0 in stage 205.0 (TID 205) in 325 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:53:01 INFO TaskSchedulerImpl: Removed TaskSet 205.0, whose tasks have all completed, from pool 
15/05/13 10:53:01 INFO DAGScheduler: Stage 205 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.327 s
15/05/13 10:53:01 INFO DAGScheduler: Job 209 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.358030 s
15/05/13 10:53:01 INFO JobScheduler: Finished job streaming job 1431528780000 ms.0 from job set of time 1431528780000 ms
15/05/13 10:53:01 INFO MapPartitionsRDD: Removing RDD 1440 from persistence list
15/05/13 10:53:01 INFO JobScheduler: Total delay: 1.337 s for time 1431528780000 ms (execution: 1.225 s)
15/05/13 10:53:01 INFO BlockManager: Removing RDD 1440
15/05/13 10:53:01 INFO UnionRDD: Removing RDD 1439 from persistence list
15/05/13 10:53:01 INFO BlockManager: Removing RDD 1439
15/05/13 10:53:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528720000 ms: 1431528660000 ms
15/05/13 10:53:01 INFO JobGenerator: Checkpointing graph for time 1431528780000 ms
15/05/13 10:53:01 INFO DStreamGraph: Updating checkpoint data for time 1431528780000 ms
15/05/13 10:53:01 INFO DStreamGraph: Updated checkpoint data for time 1431528780000 ms
15/05/13 10:53:01 INFO CheckpointWriter: Saving checkpoint for time 1431528780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528780000'
15/05/13 10:53:01 INFO CheckpointWriter: Checkpoint for time 1431528780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528780000', took 7655 bytes and 54 ms
15/05/13 10:53:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528780000 ms
15/05/13 10:53:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528780000 ms
15/05/13 10:53:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:54:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 10:54:00 INFO FileInputDStream: New files at time 1431528840000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528769157.json
15/05/13 10:54:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=25157434, maxMem=278302556
15/05/13 10:54:00 INFO MemoryStore: Block broadcast_309 stored as values in memory (estimated size 232.9 KB, free 241.2 MB)
15/05/13 10:54:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=25395966, maxMem=278302556
15/05/13 10:54:00 INFO MemoryStore: Block broadcast_309_piece0 stored as bytes in memory (estimated size 34.9 KB, free 241.2 MB)
15/05/13 10:54:00 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.2 MB)
15/05/13 10:54:00 INFO BlockManagerMaster: Updated info of block broadcast_309_piece0
15/05/13 10:54:00 INFO SparkContext: Created broadcast 309 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:54:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:54:00 INFO JobScheduler: Added jobs for time 1431528840000 ms
15/05/13 10:54:00 INFO JobScheduler: Starting job streaming job 1431528840000 ms.0 from job set of time 1431528840000 ms
15/05/13 10:54:00 INFO JobGenerator: Checkpointing graph for time 1431528840000 ms
15/05/13 10:54:00 INFO DStreamGraph: Updating checkpoint data for time 1431528840000 ms
15/05/13 10:54:00 INFO DStreamGraph: Updated checkpoint data for time 1431528840000 ms
15/05/13 10:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431528840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528840000'
15/05/13 10:54:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:54:00 INFO DAGScheduler: Got job 210 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:54:00 INFO DAGScheduler: Final stage: Stage 206(reduce at JsonRDD.scala:51)
15/05/13 10:54:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:54:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:54:00 INFO DAGScheduler: Submitting Stage 206 (MapPartitionsRDD[1472] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:54:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=25431674, maxMem=278302556
15/05/13 10:54:00 INFO MemoryStore: Block broadcast_310 stored as values in memory (estimated size 5.9 KB, free 241.2 MB)
15/05/13 10:54:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=25437714, maxMem=278302556
15/05/13 10:54:00 INFO MemoryStore: Block broadcast_310_piece0 stored as bytes in memory (estimated size 4.1 KB, free 241.1 MB)
15/05/13 10:54:00 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.2 MB)
15/05/13 10:54:00 INFO BlockManagerMaster: Updated info of block broadcast_310_piece0
15/05/13 10:54:00 INFO SparkContext: Created broadcast 310 from broadcast at DAGScheduler.scala:839
15/05/13 10:54:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 206 (MapPartitionsRDD[1472] at map at JsonRDD.scala:51)
15/05/13 10:54:00 INFO TaskSchedulerImpl: Adding task set 206.0 with 1 tasks
15/05/13 10:54:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528540000.bk
15/05/13 10:54:00 INFO TaskSetManager: Starting task 0.0 in stage 206.0 (TID 206, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:54:00 INFO CheckpointWriter: Checkpoint for time 1431528840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528840000', took 7666 bytes and 68 ms
15/05/13 10:54:00 INFO BlockManagerInfo: Added broadcast_310_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:54:00 INFO BlockManagerInfo: Added broadcast_309_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:54:01 INFO TaskSetManager: Finished task 0.0 in stage 206.0 (TID 206) in 985 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:54:01 INFO TaskSchedulerImpl: Removed TaskSet 206.0, whose tasks have all completed, from pool 
15/05/13 10:54:01 INFO DAGScheduler: Stage 206 (reduce at JsonRDD.scala:51) finished in 0.996 s
15/05/13 10:54:01 INFO DAGScheduler: Job 210 finished: reduce at JsonRDD.scala:51, took 1.016956 s
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 299
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_299
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_299 of size 21504 dropped from memory (free 252882121)
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_299_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_299_piece0 of size 11288 dropped from memory (free 252893409)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_299_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 262.2 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_299_piece0
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_299_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 11.0 KB, free: 264.3 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 299
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 302
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_302
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_302 of size 21480 dropped from memory (free 252914889)
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_302_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_302_piece0 of size 11138 dropped from memory (free 252926027)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_302_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.2 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_302_piece0
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_302_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.2 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 302
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 308
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_308
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_308 of size 20920 dropped from memory (free 252946947)
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_308_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_308_piece0 of size 10824 dropped from memory (free 252957771)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_308_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.2 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_308_piece0
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_308_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 263.1 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 308
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 307
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_307
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_307 of size 6040 dropped from memory (free 252963811)
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_307_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_307_piece0 of size 4225 dropped from memory (free 252968036)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_307_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.2 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_307_piece0
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_307_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 307
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 305
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_305_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_305_piece0 of size 11026 dropped from memory (free 252979062)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_305_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 262.3 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_305_piece0
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_305
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_305 of size 21096 dropped from memory (free 253000158)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_305_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 263.2 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 305
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 304
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_304_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_304_piece0 of size 4225 dropped from memory (free 253004383)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_304_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_304_piece0
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_304
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_304 of size 6040 dropped from memory (free 253010423)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_304_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 304
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 301
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_301
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_301 of size 6040 dropped from memory (free 253016463)
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_301_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_301_piece0 of size 4225 dropped from memory (free 253020688)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_301_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_301_piece0
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_301_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 301
15/05/13 10:54:01 INFO BlockManager: Removing broadcast 310
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_310_piece0
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_310_piece0 of size 4225 dropped from memory (free 253024913)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_310_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_310_piece0
15/05/13 10:54:01 INFO BlockManager: Removing block broadcast_310
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_310 of size 6040 dropped from memory (free 253030953)
15/05/13 10:54:01 INFO BlockManagerInfo: Removed broadcast_310_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:54:01 INFO ContextCleaner: Cleaned broadcast 310
15/05/13 10:54:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:54:01 INFO DAGScheduler: Got job 211 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:54:01 INFO DAGScheduler: Final stage: Stage 207(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:54:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:54:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:54:01 INFO DAGScheduler: Submitting Stage 207 (MapPartitionsRDD[1479] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:54:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=25271603, maxMem=278302556
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_311 stored as values in memory (estimated size 20.4 KB, free 241.3 MB)
15/05/13 10:54:01 INFO MemoryStore: ensureFreeSpace(10899) called with curMem=25292523, maxMem=278302556
15/05/13 10:54:01 INFO MemoryStore: Block broadcast_311_piece0 stored as bytes in memory (estimated size 10.6 KB, free 241.3 MB)
15/05/13 10:54:01 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.3 MB)
15/05/13 10:54:01 INFO BlockManagerMaster: Updated info of block broadcast_311_piece0
15/05/13 10:54:01 INFO SparkContext: Created broadcast 311 from broadcast at DAGScheduler.scala:839
15/05/13 10:54:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 207 (MapPartitionsRDD[1479] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:54:01 INFO TaskSchedulerImpl: Adding task set 207.0 with 1 tasks
15/05/13 10:54:01 INFO TaskSetManager: Starting task 0.0 in stage 207.0 (TID 207, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:54:01 INFO BlockManagerInfo: Added broadcast_311_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:54:01 INFO TaskSetManager: Finished task 0.0 in stage 207.0 (TID 207) in 117 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:54:01 INFO TaskSchedulerImpl: Removed TaskSet 207.0, whose tasks have all completed, from pool 
15/05/13 10:54:01 INFO DAGScheduler: Stage 207 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.120 s
15/05/13 10:54:01 INFO DAGScheduler: Job 211 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.152214 s
15/05/13 10:54:01 INFO JobScheduler: Finished job streaming job 1431528840000 ms.0 from job set of time 1431528840000 ms
15/05/13 10:54:01 INFO JobScheduler: Total delay: 1.672 s for time 1431528840000 ms (execution: 1.536 s)
15/05/13 10:54:01 INFO MapPartitionsRDD: Removing RDD 1454 from persistence list
15/05/13 10:54:01 INFO BlockManager: Removing RDD 1454
15/05/13 10:54:01 INFO UnionRDD: Removing RDD 1453 from persistence list
15/05/13 10:54:01 INFO BlockManager: Removing RDD 1453
15/05/13 10:54:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528780000 ms: 1431528720000 ms
15/05/13 10:54:01 INFO JobGenerator: Checkpointing graph for time 1431528840000 ms
15/05/13 10:54:01 INFO DStreamGraph: Updating checkpoint data for time 1431528840000 ms
15/05/13 10:54:01 INFO DStreamGraph: Updated checkpoint data for time 1431528840000 ms
15/05/13 10:54:01 INFO CheckpointWriter: Saving checkpoint for time 1431528840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528840000'
15/05/13 10:54:01 INFO CheckpointWriter: Checkpoint for time 1431528840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528840000', took 7656 bytes and 60 ms
15/05/13 10:54:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528840000 ms
15/05/13 10:54:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528840000 ms
15/05/13 10:54:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:55:00 INFO FileInputDStream: Finding new files took 38 ms
15/05/13 10:55:00 INFO FileInputDStream: New files at time 1431528900000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528834062.json
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=25303422, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_312 stored as values in memory (estimated size 232.9 KB, free 241.1 MB)
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=25541954, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_312_piece0 stored as bytes in memory (estimated size 34.9 KB, free 241.0 MB)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.2 MB)
15/05/13 10:55:00 INFO BlockManagerMaster: Updated info of block broadcast_312_piece0
15/05/13 10:55:00 INFO SparkContext: Created broadcast 312 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:55:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:55:00 INFO JobScheduler: Added jobs for time 1431528900000 ms
15/05/13 10:55:00 INFO JobGenerator: Checkpointing graph for time 1431528900000 ms
15/05/13 10:55:00 INFO DStreamGraph: Updating checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO JobScheduler: Starting job streaming job 1431528900000 ms.0 from job set of time 1431528900000 ms
15/05/13 10:55:00 INFO DStreamGraph: Updated checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431528900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000'
15/05/13 10:55:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:55:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431528900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000'
15/05/13 10:55:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:55:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528600000.bk
15/05/13 10:55:00 INFO DAGScheduler: Got job 212 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:55:00 INFO DAGScheduler: Final stage: Stage 208(reduce at JsonRDD.scala:51)
15/05/13 10:55:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:55:00 INFO CheckpointWriter: Checkpoint for time 1431528900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000', took 7666 bytes and 100 ms
15/05/13 10:55:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:55:00 INFO DAGScheduler: Submitting Stage 208 (MapPartitionsRDD[1486] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=25577662, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_313 stored as values in memory (estimated size 5.9 KB, free 241.0 MB)
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=25583702, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_313_piece0 stored as bytes in memory (estimated size 4.1 KB, free 241.0 MB)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.2 MB)
15/05/13 10:55:00 INFO BlockManagerMaster: Updated info of block broadcast_313_piece0
15/05/13 10:55:00 INFO SparkContext: Created broadcast 313 from broadcast at DAGScheduler.scala:839
15/05/13 10:55:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 208 (MapPartitionsRDD[1486] at map at JsonRDD.scala:51)
15/05/13 10:55:00 INFO TaskSchedulerImpl: Adding task set 208.0 with 1 tasks
15/05/13 10:55:00 INFO TaskSetManager: Starting task 0.0 in stage 208.0 (TID 208, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_313_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_312_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:55:00 INFO DAGScheduler: Stage 208 (reduce at JsonRDD.scala:51) finished in 0.292 s
15/05/13 10:55:00 INFO DAGScheduler: Job 212 finished: reduce at JsonRDD.scala:51, took 0.326918 s
15/05/13 10:55:00 INFO TaskSetManager: Finished task 0.0 in stage 208.0 (TID 208) in 280 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:55:00 INFO TaskSchedulerImpl: Removed TaskSet 208.0, whose tasks have all completed, from pool 
15/05/13 10:55:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:55:00 INFO DAGScheduler: Got job 213 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:55:00 INFO DAGScheduler: Final stage: Stage 209(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:55:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:55:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:55:00 INFO DAGScheduler: Submitting Stage 209 (MapPartitionsRDD[1493] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=25587927, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_314 stored as values in memory (estimated size 20.4 KB, free 241.0 MB)
15/05/13 10:55:00 INFO MemoryStore: ensureFreeSpace(10820) called with curMem=25608847, maxMem=278302556
15/05/13 10:55:00 INFO MemoryStore: Block broadcast_314_piece0 stored as bytes in memory (estimated size 10.6 KB, free 241.0 MB)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.2 MB)
15/05/13 10:55:00 INFO BlockManagerMaster: Updated info of block broadcast_314_piece0
15/05/13 10:55:00 INFO SparkContext: Created broadcast 314 from broadcast at DAGScheduler.scala:839
15/05/13 10:55:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 209 (MapPartitionsRDD[1493] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:55:00 INFO TaskSchedulerImpl: Adding task set 209.0 with 1 tasks
15/05/13 10:55:00 INFO TaskSetManager: Starting task 0.0 in stage 209.0 (TID 209, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:55:00 INFO BlockManagerInfo: Added broadcast_314_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.2 MB)
15/05/13 10:55:00 INFO TaskSetManager: Finished task 0.0 in stage 209.0 (TID 209) in 88 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:55:00 INFO DAGScheduler: Stage 209 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.092 s
15/05/13 10:55:00 INFO TaskSchedulerImpl: Removed TaskSet 209.0, whose tasks have all completed, from pool 
15/05/13 10:55:00 INFO DAGScheduler: Job 213 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.133454 s
15/05/13 10:55:00 INFO JobScheduler: Finished job streaming job 1431528900000 ms.0 from job set of time 1431528900000 ms
15/05/13 10:55:00 INFO JobScheduler: Total delay: 0.883 s for time 1431528900000 ms (execution: 0.770 s)
15/05/13 10:55:00 INFO MapPartitionsRDD: Removing RDD 1468 from persistence list
15/05/13 10:55:00 INFO BlockManager: Removing RDD 1468
15/05/13 10:55:00 INFO UnionRDD: Removing RDD 1467 from persistence list
15/05/13 10:55:00 INFO BlockManager: Removing RDD 1467
15/05/13 10:55:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528840000 ms: 1431528780000 ms
15/05/13 10:55:00 INFO JobGenerator: Checkpointing graph for time 1431528900000 ms
15/05/13 10:55:00 INFO DStreamGraph: Updating checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO DStreamGraph: Updated checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431528900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000'
15/05/13 10:55:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000
15/05/13 10:55:00 INFO CheckpointWriter: Checkpoint for time 1431528900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528900000', took 7655 bytes and 78 ms
15/05/13 10:55:00 INFO DStreamGraph: Clearing checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO DStreamGraph: Cleared checkpoint data for time 1431528900000 ms
15/05/13 10:55:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:56:00 INFO FileInputDStream: Finding new files took 47 ms
15/05/13 10:56:00 INFO FileInputDStream: New files at time 1431528960000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528895801.json
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=25619667, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_315 stored as values in memory (estimated size 232.9 KB, free 240.7 MB)
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=25858199, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_315_piece0 stored as bytes in memory (estimated size 34.9 KB, free 240.7 MB)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.2 MB)
15/05/13 10:56:00 INFO BlockManagerMaster: Updated info of block broadcast_315_piece0
15/05/13 10:56:00 INFO SparkContext: Created broadcast 315 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:56:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:56:00 INFO JobScheduler: Starting job streaming job 1431528960000 ms.0 from job set of time 1431528960000 ms
15/05/13 10:56:00 INFO JobScheduler: Added jobs for time 1431528960000 ms
15/05/13 10:56:00 INFO JobGenerator: Checkpointing graph for time 1431528960000 ms
15/05/13 10:56:00 INFO DStreamGraph: Updating checkpoint data for time 1431528960000 ms
15/05/13 10:56:00 INFO DStreamGraph: Updated checkpoint data for time 1431528960000 ms
15/05/13 10:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431528960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000'
15/05/13 10:56:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:56:00 INFO DAGScheduler: Got job 214 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:56:00 INFO DAGScheduler: Final stage: Stage 210(reduce at JsonRDD.scala:51)
15/05/13 10:56:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:56:00 INFO CheckpointWriter: Checkpoint for time 1431528960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000', took 7662 bytes and 80 ms
15/05/13 10:56:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:56:00 INFO DAGScheduler: Submitting Stage 210 (MapPartitionsRDD[1500] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=25893907, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_316 stored as values in memory (estimated size 5.9 KB, free 240.7 MB)
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=25899947, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_316_piece0 stored as bytes in memory (estimated size 4.1 KB, free 240.7 MB)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.2 MB)
15/05/13 10:56:00 INFO BlockManagerMaster: Updated info of block broadcast_316_piece0
15/05/13 10:56:00 INFO SparkContext: Created broadcast 316 from broadcast at DAGScheduler.scala:839
15/05/13 10:56:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 210 (MapPartitionsRDD[1500] at map at JsonRDD.scala:51)
15/05/13 10:56:00 INFO TaskSchedulerImpl: Adding task set 210.0 with 1 tasks
15/05/13 10:56:00 INFO TaskSetManager: Starting task 0.0 in stage 210.0 (TID 210, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_316_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.2 MB)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_315_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.1 MB)
15/05/13 10:56:00 INFO TaskSetManager: Finished task 0.0 in stage 210.0 (TID 210) in 263 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:56:00 INFO TaskSchedulerImpl: Removed TaskSet 210.0, whose tasks have all completed, from pool 
15/05/13 10:56:00 INFO DAGScheduler: Stage 210 (reduce at JsonRDD.scala:51) finished in 0.278 s
15/05/13 10:56:00 INFO DAGScheduler: Job 214 finished: reduce at JsonRDD.scala:51, took 0.313546 s
15/05/13 10:56:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:56:00 INFO DAGScheduler: Got job 215 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:56:00 INFO DAGScheduler: Final stage: Stage 211(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:56:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:56:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:56:00 INFO DAGScheduler: Submitting Stage 211 (MapPartitionsRDD[1507] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=25904172, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_317 stored as values in memory (estimated size 20.4 KB, free 240.7 MB)
15/05/13 10:56:00 INFO MemoryStore: ensureFreeSpace(10848) called with curMem=25925092, maxMem=278302556
15/05/13 10:56:00 INFO MemoryStore: Block broadcast_317_piece0 stored as bytes in memory (estimated size 10.6 KB, free 240.7 MB)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.2 MB)
15/05/13 10:56:00 INFO BlockManagerMaster: Updated info of block broadcast_317_piece0
15/05/13 10:56:00 INFO SparkContext: Created broadcast 317 from broadcast at DAGScheduler.scala:839
15/05/13 10:56:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 211 (MapPartitionsRDD[1507] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:56:00 INFO TaskSchedulerImpl: Adding task set 211.0 with 1 tasks
15/05/13 10:56:00 INFO TaskSetManager: Starting task 0.0 in stage 211.0 (TID 211, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:56:00 INFO BlockManagerInfo: Added broadcast_317_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 264.1 MB)
15/05/13 10:56:00 INFO TaskSetManager: Finished task 0.0 in stage 211.0 (TID 211) in 78 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 10:56:00 INFO TaskSchedulerImpl: Removed TaskSet 211.0, whose tasks have all completed, from pool 
15/05/13 10:56:00 INFO DAGScheduler: Stage 211 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.080 s
15/05/13 10:56:00 INFO DAGScheduler: Job 215 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.112851 s
15/05/13 10:56:00 INFO JobScheduler: Finished job streaming job 1431528960000 ms.0 from job set of time 1431528960000 ms
15/05/13 10:56:00 INFO JobScheduler: Total delay: 0.927 s for time 1431528960000 ms (execution: 0.782 s)
15/05/13 10:56:00 INFO MapPartitionsRDD: Removing RDD 1482 from persistence list
15/05/13 10:56:00 INFO BlockManager: Removing RDD 1482
15/05/13 10:56:00 INFO UnionRDD: Removing RDD 1481 from persistence list
15/05/13 10:56:00 INFO BlockManager: Removing RDD 1481
15/05/13 10:56:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431528900000 ms: 1431528840000 ms
15/05/13 10:56:00 INFO JobGenerator: Checkpointing graph for time 1431528960000 ms
15/05/13 10:56:00 INFO DStreamGraph: Updating checkpoint data for time 1431528960000 ms
15/05/13 10:56:00 INFO DStreamGraph: Updated checkpoint data for time 1431528960000 ms
15/05/13 10:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431528960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000'
15/05/13 10:56:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83723): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:56:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83723): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 10:56:01 INFO CheckpointWriter: Saving checkpoint for time 1431528960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000'
15/05/13 10:56:01 INFO CheckpointWriter: Checkpoint for time 1431528960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000', took 7654 bytes and 286 ms
15/05/13 10:56:01 INFO DStreamGraph: Clearing checkpoint data for time 1431528960000 ms
15/05/13 10:56:01 INFO DStreamGraph: Cleared checkpoint data for time 1431528960000 ms
15/05/13 10:56:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:57:00 INFO FileInputDStream: Finding new files took 42 ms
15/05/13 10:57:00 INFO FileInputDStream: New files at time 1431529020000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431528956649.json
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=25935940, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_318 stored as values in memory (estimated size 232.9 KB, free 240.4 MB)
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=26174472, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_318_piece0 stored as bytes in memory (estimated size 34.9 KB, free 240.4 MB)
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_318_piece0
15/05/13 10:57:00 INFO SparkContext: Created broadcast 318 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:57:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:57:00 INFO JobScheduler: Added jobs for time 1431529020000 ms
15/05/13 10:57:00 INFO JobGenerator: Checkpointing graph for time 1431529020000 ms
15/05/13 10:57:00 INFO DStreamGraph: Updating checkpoint data for time 1431529020000 ms
15/05/13 10:57:00 INFO JobScheduler: Starting job streaming job 1431529020000 ms.0 from job set of time 1431529020000 ms
15/05/13 10:57:00 INFO DStreamGraph: Updated checkpoint data for time 1431529020000 ms
15/05/13 10:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431529020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529020000'
15/05/13 10:57:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:57:00 INFO DAGScheduler: Got job 216 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:57:00 INFO DAGScheduler: Final stage: Stage 212(reduce at JsonRDD.scala:51)
15/05/13 10:57:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:57:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528660000
15/05/13 10:57:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:57:00 INFO DAGScheduler: Submitting Stage 212 (MapPartitionsRDD[1514] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:57:00 INFO CheckpointWriter: Checkpoint for time 1431529020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529020000', took 7668 bytes and 71 ms
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=26210180, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_319 stored as values in memory (estimated size 5.9 KB, free 240.4 MB)
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=26216220, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_319_piece0 stored as bytes in memory (estimated size 4.1 KB, free 240.4 MB)
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_319_piece0
15/05/13 10:57:00 INFO SparkContext: Created broadcast 319 from broadcast at DAGScheduler.scala:839
15/05/13 10:57:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 212 (MapPartitionsRDD[1514] at map at JsonRDD.scala:51)
15/05/13 10:57:00 INFO TaskSchedulerImpl: Adding task set 212.0 with 1 tasks
15/05/13 10:57:00 INFO TaskSetManager: Starting task 0.0 in stage 212.0 (TID 212, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_319_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_318_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.3 MB)
15/05/13 10:57:00 INFO DAGScheduler: Stage 212 (reduce at JsonRDD.scala:51) finished in 0.416 s
15/05/13 10:57:00 INFO TaskSetManager: Finished task 0.0 in stage 212.0 (TID 212) in 404 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:57:00 INFO DAGScheduler: Job 216 finished: reduce at JsonRDD.scala:51, took 0.447872 s
15/05/13 10:57:00 INFO TaskSchedulerImpl: Removed TaskSet 212.0, whose tasks have all completed, from pool 
15/05/13 10:57:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:57:00 INFO DAGScheduler: Got job 217 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:57:00 INFO DAGScheduler: Final stage: Stage 213(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:57:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:57:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:57:00 INFO DAGScheduler: Submitting Stage 213 (MapPartitionsRDD[1521] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(20328) called with curMem=26220445, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_320 stored as values in memory (estimated size 19.9 KB, free 240.4 MB)
15/05/13 10:57:00 INFO MemoryStore: ensureFreeSpace(10746) called with curMem=26240773, maxMem=278302556
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_320_piece0 stored as bytes in memory (estimated size 10.5 KB, free 240.4 MB)
15/05/13 10:57:00 INFO BlockManager: Removing broadcast 319
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_319
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_319 of size 6040 dropped from memory (free 252057077)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_320_piece0
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_319_piece0
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_319_piece0 of size 4225 dropped from memory (free 252061302)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_319_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_319_piece0
15/05/13 10:57:00 INFO SparkContext: Created broadcast 320 from broadcast at DAGScheduler.scala:839
15/05/13 10:57:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 213 (MapPartitionsRDD[1521] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:57:00 INFO TaskSchedulerImpl: Adding task set 213.0 with 1 tasks
15/05/13 10:57:00 INFO TaskSetManager: Starting task 0.0 in stage 213.0 (TID 213, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_319_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.3 MB)
15/05/13 10:57:00 INFO ContextCleaner: Cleaned broadcast 319
15/05/13 10:57:00 INFO BlockManager: Removing broadcast 313
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_313
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_313 of size 6040 dropped from memory (free 252067342)
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_313_piece0
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_313_piece0 of size 4225 dropped from memory (free 252071567)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_313_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_313_piece0
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_313_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 10:57:00 INFO ContextCleaner: Cleaned broadcast 313
15/05/13 10:57:00 INFO BlockManager: Removing broadcast 316
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_316
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_316 of size 6040 dropped from memory (free 252077607)
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_316_piece0
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_316_piece0 of size 4225 dropped from memory (free 252081832)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_316_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_316_piece0
15/05/13 10:57:00 INFO BlockManagerInfo: Added broadcast_320_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 264.3 MB)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_316_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 10:57:00 INFO ContextCleaner: Cleaned broadcast 316
15/05/13 10:57:00 INFO BlockManager: Removing broadcast 317
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_317_piece0
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_317_piece0 of size 10848 dropped from memory (free 252092680)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_317_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.1 MB)
15/05/13 10:57:00 INFO BlockManagerMaster: Updated info of block broadcast_317_piece0
15/05/13 10:57:00 INFO BlockManager: Removing block broadcast_317
15/05/13 10:57:00 INFO MemoryStore: Block broadcast_317 of size 20920 dropped from memory (free 252113600)
15/05/13 10:57:00 INFO BlockManagerInfo: Removed broadcast_317_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.1 MB)
15/05/13 10:57:01 INFO ContextCleaner: Cleaned broadcast 317
15/05/13 10:57:01 INFO BlockManager: Removing broadcast 311
15/05/13 10:57:01 INFO BlockManager: Removing block broadcast_311_piece0
15/05/13 10:57:01 INFO MemoryStore: Block broadcast_311_piece0 of size 10899 dropped from memory (free 252124499)
15/05/13 10:57:01 INFO BlockManagerInfo: Removed broadcast_311_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.1 MB)
15/05/13 10:57:01 INFO BlockManagerMaster: Updated info of block broadcast_311_piece0
15/05/13 10:57:01 INFO BlockManager: Removing block broadcast_311
15/05/13 10:57:01 INFO MemoryStore: Block broadcast_311 of size 20920 dropped from memory (free 252145419)
15/05/13 10:57:01 INFO BlockManagerInfo: Removed broadcast_311_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:57:01 INFO ContextCleaner: Cleaned broadcast 311
15/05/13 10:57:01 INFO BlockManager: Removing broadcast 314
15/05/13 10:57:01 INFO BlockManager: Removing block broadcast_314_piece0
15/05/13 10:57:01 INFO MemoryStore: Block broadcast_314_piece0 of size 10820 dropped from memory (free 252156239)
15/05/13 10:57:01 INFO BlockManagerInfo: Removed broadcast_314_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.2 MB)
15/05/13 10:57:01 INFO BlockManagerMaster: Updated info of block broadcast_314_piece0
15/05/13 10:57:01 INFO BlockManager: Removing block broadcast_314
15/05/13 10:57:01 INFO MemoryStore: Block broadcast_314 of size 20920 dropped from memory (free 252177159)
15/05/13 10:57:01 INFO BlockManagerInfo: Removed broadcast_314_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.6 KB, free: 264.2 MB)
15/05/13 10:57:01 INFO ContextCleaner: Cleaned broadcast 314
15/05/13 10:57:01 INFO TaskSetManager: Finished task 0.0 in stage 213.0 (TID 213) in 129 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:57:01 INFO TaskSchedulerImpl: Removed TaskSet 213.0, whose tasks have all completed, from pool 
15/05/13 10:57:01 INFO DAGScheduler: Stage 213 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.129 s
15/05/13 10:57:01 INFO DAGScheduler: Job 217 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.178763 s
15/05/13 10:57:01 INFO JobScheduler: Finished job streaming job 1431529020000 ms.0 from job set of time 1431529020000 ms
15/05/13 10:57:01 INFO JobScheduler: Total delay: 1.108 s for time 1431529020000 ms (execution: 0.946 s)
15/05/13 10:57:01 INFO MapPartitionsRDD: Removing RDD 1496 from persistence list
15/05/13 10:57:01 INFO BlockManager: Removing RDD 1496
15/05/13 10:57:01 INFO UnionRDD: Removing RDD 1495 from persistence list
15/05/13 10:57:01 INFO BlockManager: Removing RDD 1495
15/05/13 10:57:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431528960000 ms: 1431528900000 ms
15/05/13 10:57:01 INFO JobGenerator: Checkpointing graph for time 1431529020000 ms
15/05/13 10:57:01 INFO DStreamGraph: Updating checkpoint data for time 1431529020000 ms
15/05/13 10:57:01 INFO DStreamGraph: Updated checkpoint data for time 1431529020000 ms
15/05/13 10:57:01 INFO CheckpointWriter: Saving checkpoint for time 1431529020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529020000'
15/05/13 10:57:01 INFO CheckpointWriter: Checkpoint for time 1431529020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529020000', took 7659 bytes and 41 ms
15/05/13 10:57:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529020000 ms
15/05/13 10:57:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529020000 ms
15/05/13 10:57:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:58:00 INFO FileInputDStream: Finding new files took 33 ms
15/05/13 10:58:00 INFO FileInputDStream: New files at time 1431529080000 ms:

15/05/13 10:58:00 INFO JobScheduler: Added jobs for time 1431529080000 ms
15/05/13 10:58:00 INFO JobScheduler: Starting job streaming job 1431529080000 ms.0 from job set of time 1431529080000 ms
15/05/13 10:58:00 INFO JobGenerator: Checkpointing graph for time 1431529080000 ms
15/05/13 10:58:00 INFO DStreamGraph: Updating checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 INFO DStreamGraph: Updated checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431529080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000'
15/05/13 10:58:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:58:00 INFO DAGScheduler: Job 218 finished: reduce at JsonRDD.scala:51, took 0.000117 s
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 10:58:00 INFO JobScheduler: Finished job streaming job 1431529080000 ms.0 from job set of time 1431529080000 ms
15/05/13 10:58:00 INFO JobScheduler: Total delay: 0.159 s for time 1431529080000 ms (execution: 0.083 s)
15/05/13 10:58:00 INFO MapPartitionsRDD: Removing RDD 1510 from persistence list
15/05/13 10:58:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83741): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:58:00 INFO BlockManager: Removing RDD 1510
15/05/13 10:58:00 INFO UnionRDD: Removing RDD 1509 from persistence list
15/05/13 10:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431529080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000'
15/05/13 10:58:00 INFO BlockManager: Removing RDD 1509
15/05/13 10:58:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431529020000 ms: 1431528960000 ms
15/05/13 10:58:00 INFO JobGenerator: Checkpointing graph for time 1431529080000 ms
15/05/13 10:58:00 INFO DStreamGraph: Updating checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 INFO DStreamGraph: Updated checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83743): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431529080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000'
15/05/13 10:58:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000
15/05/13 10:58:00 INFO CheckpointWriter: Checkpoint for time 1431529080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000', took 7642 bytes and 130 ms
15/05/13 10:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431529080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000'
15/05/13 10:58:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83747): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431529080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000'
15/05/13 10:58:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528720000
15/05/13 10:58:00 INFO CheckpointWriter: Checkpoint for time 1431529080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000', took 7644 bytes and 98 ms
15/05/13 10:58:00 INFO DStreamGraph: Clearing checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 INFO DStreamGraph: Cleared checkpoint data for time 1431529080000 ms
15/05/13 10:58:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 10:59:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 10:59:00 INFO FileInputDStream: New files at time 1431529140000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529023948.json
15/05/13 10:59:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=26125397, maxMem=278302556
15/05/13 10:59:00 INFO MemoryStore: Block broadcast_321 stored as values in memory (estimated size 232.9 KB, free 240.3 MB)
15/05/13 10:59:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=26363929, maxMem=278302556
15/05/13 10:59:00 INFO MemoryStore: Block broadcast_321_piece0 stored as bytes in memory (estimated size 34.9 KB, free 240.2 MB)
15/05/13 10:59:00 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.1 MB)
15/05/13 10:59:00 INFO BlockManagerMaster: Updated info of block broadcast_321_piece0
15/05/13 10:59:00 INFO SparkContext: Created broadcast 321 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 10:59:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 10:59:00 INFO JobScheduler: Added jobs for time 1431529140000 ms
15/05/13 10:59:00 INFO JobGenerator: Checkpointing graph for time 1431529140000 ms
15/05/13 10:59:00 INFO JobScheduler: Starting job streaming job 1431529140000 ms.0 from job set of time 1431529140000 ms
15/05/13 10:59:00 INFO DStreamGraph: Updating checkpoint data for time 1431529140000 ms
15/05/13 10:59:00 INFO DStreamGraph: Updated checkpoint data for time 1431529140000 ms
15/05/13 10:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431529140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000'
15/05/13 10:59:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83753): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431529140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000'
15/05/13 10:59:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 10:59:00 INFO DAGScheduler: Got job 219 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 10:59:00 INFO DAGScheduler: Final stage: Stage 214(reduce at JsonRDD.scala:51)
15/05/13 10:59:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:59:00 INFO DAGScheduler: Missing parents: List()
15/05/13 10:59:00 INFO DAGScheduler: Submitting Stage 214 (MapPartitionsRDD[1534] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 10:59:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=26399637, maxMem=278302556
15/05/13 10:59:00 INFO MemoryStore: Block broadcast_322 stored as values in memory (estimated size 5.9 KB, free 240.2 MB)
15/05/13 10:59:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=26405677, maxMem=278302556
15/05/13 10:59:00 INFO MemoryStore: Block broadcast_322_piece0 stored as bytes in memory (estimated size 4.1 KB, free 240.2 MB)
15/05/13 10:59:00 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.1 MB)
15/05/13 10:59:00 INFO BlockManagerMaster: Updated info of block broadcast_322_piece0
15/05/13 10:59:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83755): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:59:00 INFO SparkContext: Created broadcast 322 from broadcast at DAGScheduler.scala:839
15/05/13 10:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431529140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000'
15/05/13 10:59:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 214 (MapPartitionsRDD[1534] at map at JsonRDD.scala:51)
15/05/13 10:59:00 INFO TaskSchedulerImpl: Adding task set 214.0 with 1 tasks
15/05/13 10:59:00 INFO TaskSetManager: Starting task 0.0 in stage 214.0 (TID 214, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:59:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83757): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 10:59:00 WARN CheckpointWriter: Could not write checkpoint for time 1431529140000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000'
15/05/13 10:59:00 INFO BlockManagerInfo: Added broadcast_322_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.2 MB)
15/05/13 10:59:00 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.1 MB)
15/05/13 10:59:00 INFO TaskSetManager: Finished task 0.0 in stage 214.0 (TID 214) in 487 ms on pti-base.insafanalytics.com (1/1)
15/05/13 10:59:00 INFO TaskSchedulerImpl: Removed TaskSet 214.0, whose tasks have all completed, from pool 
15/05/13 10:59:00 INFO DAGScheduler: Stage 214 (reduce at JsonRDD.scala:51) finished in 0.502 s
15/05/13 10:59:00 INFO DAGScheduler: Job 219 finished: reduce at JsonRDD.scala:51, took 0.537797 s
15/05/13 10:59:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 10:59:01 INFO DAGScheduler: Got job 220 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 10:59:01 INFO DAGScheduler: Final stage: Stage 215(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:59:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 10:59:01 INFO DAGScheduler: Missing parents: List()
15/05/13 10:59:01 INFO DAGScheduler: Submitting Stage 215 (MapPartitionsRDD[1541] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 10:59:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=26409902, maxMem=278302556
15/05/13 10:59:01 INFO MemoryStore: Block broadcast_323 stored as values in memory (estimated size 20.4 KB, free 240.2 MB)
15/05/13 10:59:01 INFO MemoryStore: ensureFreeSpace(10846) called with curMem=26430822, maxMem=278302556
15/05/13 10:59:01 INFO MemoryStore: Block broadcast_323_piece0 stored as bytes in memory (estimated size 10.6 KB, free 240.2 MB)
15/05/13 10:59:01 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.1 MB)
15/05/13 10:59:01 INFO BlockManagerMaster: Updated info of block broadcast_323_piece0
15/05/13 10:59:01 INFO SparkContext: Created broadcast 323 from broadcast at DAGScheduler.scala:839
15/05/13 10:59:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 215 (MapPartitionsRDD[1541] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 10:59:01 INFO TaskSchedulerImpl: Adding task set 215.0 with 1 tasks
15/05/13 10:59:01 INFO TaskSetManager: Starting task 0.0 in stage 215.0 (TID 215, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 10:59:01 INFO BlockManagerInfo: Added broadcast_323_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.3 MB)
15/05/13 10:59:01 INFO BlockManagerInfo: Added broadcast_321_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.2 MB)
15/05/13 10:59:01 INFO TaskSetManager: Finished task 0.0 in stage 215.0 (TID 215) in 265 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 10:59:01 INFO TaskSchedulerImpl: Removed TaskSet 215.0, whose tasks have all completed, from pool 
15/05/13 10:59:01 INFO DAGScheduler: Stage 215 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.267 s
15/05/13 10:59:01 INFO DAGScheduler: Job 220 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.298678 s
15/05/13 10:59:01 INFO JobScheduler: Finished job streaming job 1431529140000 ms.0 from job set of time 1431529140000 ms
15/05/13 10:59:01 INFO JobScheduler: Total delay: 1.345 s for time 1431529140000 ms (execution: 1.133 s)
15/05/13 10:59:01 INFO MapPartitionsRDD: Removing RDD 1523 from persistence list
15/05/13 10:59:01 INFO BlockManager: Removing RDD 1523
15/05/13 10:59:01 INFO UnionRDD: Removing RDD 1522 from persistence list
15/05/13 10:59:01 INFO BlockManager: Removing RDD 1522
15/05/13 10:59:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529080000 ms: 1431529020000 ms
15/05/13 10:59:01 INFO JobGenerator: Checkpointing graph for time 1431529140000 ms
15/05/13 10:59:01 INFO DStreamGraph: Updating checkpoint data for time 1431529140000 ms
15/05/13 10:59:01 INFO DStreamGraph: Updated checkpoint data for time 1431529140000 ms
15/05/13 10:59:01 INFO CheckpointWriter: Saving checkpoint for time 1431529140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000'
15/05/13 10:59:01 INFO CheckpointWriter: Checkpoint for time 1431529140000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000', took 7643 bytes and 43 ms
15/05/13 10:59:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529140000 ms
15/05/13 10:59:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529140000 ms
15/05/13 10:59:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:00:00 INFO FileInputDStream: Finding new files took 30 ms
15/05/13 11:00:00 INFO FileInputDStream: New files at time 1431529200000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529085660.json
15/05/13 11:00:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=26441668, maxMem=278302556
15/05/13 11:00:00 INFO MemoryStore: Block broadcast_324 stored as values in memory (estimated size 232.9 KB, free 240.0 MB)
15/05/13 11:00:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=26680200, maxMem=278302556
15/05/13 11:00:00 INFO MemoryStore: Block broadcast_324_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.9 MB)
15/05/13 11:00:00 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:00:00 INFO BlockManagerMaster: Updated info of block broadcast_324_piece0
15/05/13 11:00:00 INFO SparkContext: Created broadcast 324 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:00:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:00:00 INFO JobScheduler: Added jobs for time 1431529200000 ms
15/05/13 11:00:00 INFO JobGenerator: Checkpointing graph for time 1431529200000 ms
15/05/13 11:00:00 INFO JobScheduler: Starting job streaming job 1431529200000 ms.0 from job set of time 1431529200000 ms
15/05/13 11:00:00 INFO DStreamGraph: Updating checkpoint data for time 1431529200000 ms
15/05/13 11:00:00 INFO DStreamGraph: Updated checkpoint data for time 1431529200000 ms
15/05/13 11:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431529200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000'
15/05/13 11:00:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83763): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431529200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000'
15/05/13 11:00:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:00:00 INFO DAGScheduler: Got job 221 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:00:00 INFO DAGScheduler: Final stage: Stage 216(reduce at JsonRDD.scala:51)
15/05/13 11:00:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:00:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:00:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83765): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:00:00 INFO DAGScheduler: Submitting Stage 216 (MapPartitionsRDD[1548] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:00:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83765): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431529200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000'
15/05/13 11:00:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=26715908, maxMem=278302556
15/05/13 11:00:00 INFO MemoryStore: Block broadcast_325 stored as values in memory (estimated size 5.9 KB, free 239.9 MB)
15/05/13 11:00:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=26721948, maxMem=278302556
15/05/13 11:00:00 INFO MemoryStore: Block broadcast_325_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.9 MB)
15/05/13 11:00:00 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:00:00 INFO BlockManagerMaster: Updated info of block broadcast_325_piece0
15/05/13 11:00:00 INFO SparkContext: Created broadcast 325 from broadcast at DAGScheduler.scala:839
15/05/13 11:00:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 216 (MapPartitionsRDD[1548] at map at JsonRDD.scala:51)
15/05/13 11:00:00 INFO TaskSchedulerImpl: Adding task set 216.0 with 1 tasks
15/05/13 11:00:00 INFO TaskSetManager: Starting task 0.0 in stage 216.0 (TID 216, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:00:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528840000
15/05/13 11:00:00 INFO CheckpointWriter: Checkpoint for time 1431529200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000', took 7638 bytes and 193 ms
15/05/13 11:00:00 INFO BlockManagerInfo: Added broadcast_325_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:00:00 INFO BlockManagerInfo: Added broadcast_324_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.1 MB)
15/05/13 11:00:00 INFO DAGScheduler: Stage 216 (reduce at JsonRDD.scala:51) finished in 0.488 s
15/05/13 11:00:00 INFO TaskSetManager: Finished task 0.0 in stage 216.0 (TID 216) in 474 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:00:00 INFO TaskSchedulerImpl: Removed TaskSet 216.0, whose tasks have all completed, from pool 
15/05/13 11:00:00 INFO DAGScheduler: Job 221 finished: reduce at JsonRDD.scala:51, took 0.559807 s
15/05/13 11:00:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:00:01 INFO DAGScheduler: Got job 222 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:00:01 INFO DAGScheduler: Final stage: Stage 217(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:00:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:00:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:00:01 INFO DAGScheduler: Submitting Stage 217 (MapPartitionsRDD[1555] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:00:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=26726175, maxMem=278302556
15/05/13 11:00:01 INFO MemoryStore: Block broadcast_326 stored as values in memory (estimated size 20.8 KB, free 239.9 MB)
15/05/13 11:00:01 INFO MemoryStore: ensureFreeSpace(11129) called with curMem=26747479, maxMem=278302556
15/05/13 11:00:01 INFO MemoryStore: Block broadcast_326_piece0 stored as bytes in memory (estimated size 10.9 KB, free 239.9 MB)
15/05/13 11:00:01 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.1 MB)
15/05/13 11:00:01 INFO BlockManagerMaster: Updated info of block broadcast_326_piece0
15/05/13 11:00:01 INFO SparkContext: Created broadcast 326 from broadcast at DAGScheduler.scala:839
15/05/13 11:00:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 217 (MapPartitionsRDD[1555] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:00:01 INFO TaskSchedulerImpl: Adding task set 217.0 with 1 tasks
15/05/13 11:00:01 INFO TaskSetManager: Starting task 0.0 in stage 217.0 (TID 217, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:00:01 INFO BlockManagerInfo: Added broadcast_326_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 263.1 MB)
15/05/13 11:00:01 INFO TaskSetManager: Finished task 0.0 in stage 217.0 (TID 217) in 153 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:00:01 INFO TaskSchedulerImpl: Removed TaskSet 217.0, whose tasks have all completed, from pool 
15/05/13 11:00:01 INFO DAGScheduler: Stage 217 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.155 s
15/05/13 11:00:01 INFO DAGScheduler: Job 222 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.204727 s
15/05/13 11:00:01 INFO JobScheduler: Finished job streaming job 1431529200000 ms.0 from job set of time 1431529200000 ms
15/05/13 11:00:01 INFO JobScheduler: Total delay: 1.264 s for time 1431529200000 ms (execution: 1.120 s)
15/05/13 11:00:01 INFO MapPartitionsRDD: Removing RDD 1530 from persistence list
15/05/13 11:00:01 INFO BlockManager: Removing RDD 1530
15/05/13 11:00:01 INFO UnionRDD: Removing RDD 1529 from persistence list
15/05/13 11:00:01 INFO BlockManager: Removing RDD 1529
15/05/13 11:00:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529140000 ms: 1431529080000 ms
15/05/13 11:00:01 INFO JobGenerator: Checkpointing graph for time 1431529200000 ms
15/05/13 11:00:01 INFO DStreamGraph: Updating checkpoint data for time 1431529200000 ms
15/05/13 11:00:01 INFO DStreamGraph: Updated checkpoint data for time 1431529200000 ms
15/05/13 11:00:01 INFO CheckpointWriter: Saving checkpoint for time 1431529200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000'
15/05/13 11:00:01 INFO CheckpointWriter: Checkpoint for time 1431529200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000', took 7654 bytes and 55 ms
15/05/13 11:00:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529200000 ms
15/05/13 11:00:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529200000 ms
15/05/13 11:00:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:01:00 INFO FileInputDStream: Finding new files took 41 ms
15/05/13 11:01:00 INFO FileInputDStream: New files at time 1431529260000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529145820.json
15/05/13 11:01:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=26758608, maxMem=278302556
15/05/13 11:01:00 INFO MemoryStore: Block broadcast_327 stored as values in memory (estimated size 232.9 KB, free 239.7 MB)
15/05/13 11:01:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=26997140, maxMem=278302556
15/05/13 11:01:00 INFO MemoryStore: Block broadcast_327_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.6 MB)
15/05/13 11:01:00 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:01:00 INFO BlockManagerMaster: Updated info of block broadcast_327_piece0
15/05/13 11:01:00 INFO SparkContext: Created broadcast 327 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:01:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:01:00 INFO JobScheduler: Added jobs for time 1431529260000 ms
15/05/13 11:01:00 INFO JobScheduler: Starting job streaming job 1431529260000 ms.0 from job set of time 1431529260000 ms
15/05/13 11:01:00 INFO JobGenerator: Checkpointing graph for time 1431529260000 ms
15/05/13 11:01:00 INFO DStreamGraph: Updating checkpoint data for time 1431529260000 ms
15/05/13 11:01:00 INFO DStreamGraph: Updated checkpoint data for time 1431529260000 ms
15/05/13 11:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431529260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529260000'
15/05/13 11:01:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:01:00 INFO DAGScheduler: Got job 223 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:01:00 INFO DAGScheduler: Final stage: Stage 218(reduce at JsonRDD.scala:51)
15/05/13 11:01:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:01:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:01:00 INFO DAGScheduler: Submitting Stage 218 (MapPartitionsRDD[1562] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:01:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=27032848, maxMem=278302556
15/05/13 11:01:00 INFO MemoryStore: Block broadcast_328 stored as values in memory (estimated size 5.9 KB, free 239.6 MB)
15/05/13 11:01:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=27038888, maxMem=278302556
15/05/13 11:01:00 INFO MemoryStore: Block broadcast_328_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.6 MB)
15/05/13 11:01:00 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:01:00 INFO BlockManagerMaster: Updated info of block broadcast_328_piece0
15/05/13 11:01:00 INFO SparkContext: Created broadcast 328 from broadcast at DAGScheduler.scala:839
15/05/13 11:01:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 218 (MapPartitionsRDD[1562] at map at JsonRDD.scala:51)
15/05/13 11:01:00 INFO TaskSchedulerImpl: Adding task set 218.0 with 1 tasks
15/05/13 11:01:00 INFO TaskSetManager: Starting task 0.0 in stage 218.0 (TID 218, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:01:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431528960000.bk
15/05/13 11:01:00 INFO CheckpointWriter: Checkpoint for time 1431529260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529260000', took 7661 bytes and 85 ms
15/05/13 11:01:00 INFO BlockManagerInfo: Added broadcast_328_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:01:00 INFO BlockManagerInfo: Added broadcast_327_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.2 MB)
15/05/13 11:01:00 INFO TaskSetManager: Finished task 0.0 in stage 218.0 (TID 218) in 556 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:01:00 INFO TaskSchedulerImpl: Removed TaskSet 218.0, whose tasks have all completed, from pool 
15/05/13 11:01:00 INFO DAGScheduler: Stage 218 (reduce at JsonRDD.scala:51) finished in 0.566 s
15/05/13 11:01:00 INFO DAGScheduler: Job 223 finished: reduce at JsonRDD.scala:51, took 0.590809 s
15/05/13 11:01:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:01:00 INFO DAGScheduler: Got job 224 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:01:00 INFO DAGScheduler: Final stage: Stage 219(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:01:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:01:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:01:01 INFO DAGScheduler: Submitting Stage 219 (MapPartitionsRDD[1569] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:01:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=27043115, maxMem=278302556
15/05/13 11:01:01 INFO MemoryStore: Block broadcast_329 stored as values in memory (estimated size 20.8 KB, free 239.6 MB)
15/05/13 11:01:01 INFO MemoryStore: ensureFreeSpace(11105) called with curMem=27064419, maxMem=278302556
15/05/13 11:01:01 INFO MemoryStore: Block broadcast_329_piece0 stored as bytes in memory (estimated size 10.8 KB, free 239.6 MB)
15/05/13 11:01:01 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 262.0 MB)
15/05/13 11:01:01 INFO BlockManagerMaster: Updated info of block broadcast_329_piece0
15/05/13 11:01:01 INFO SparkContext: Created broadcast 329 from broadcast at DAGScheduler.scala:839
15/05/13 11:01:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 219 (MapPartitionsRDD[1569] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:01:01 INFO TaskSchedulerImpl: Adding task set 219.0 with 1 tasks
15/05/13 11:01:01 INFO TaskSetManager: Starting task 0.0 in stage 219.0 (TID 219, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:01:01 INFO BlockManagerInfo: Added broadcast_329_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.8 KB, free: 264.2 MB)
15/05/13 11:01:01 INFO TaskSetManager: Finished task 0.0 in stage 219.0 (TID 219) in 168 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:01:01 INFO TaskSchedulerImpl: Removed TaskSet 219.0, whose tasks have all completed, from pool 
15/05/13 11:01:01 INFO DAGScheduler: Stage 219 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.170 s
15/05/13 11:01:01 INFO DAGScheduler: Job 224 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.213839 s
15/05/13 11:01:01 INFO JobScheduler: Finished job streaming job 1431529260000 ms.0 from job set of time 1431529260000 ms
15/05/13 11:01:01 INFO JobScheduler: Total delay: 1.285 s for time 1431529260000 ms (execution: 1.160 s)
15/05/13 11:01:01 INFO MapPartitionsRDD: Removing RDD 1544 from persistence list
15/05/13 11:01:01 INFO BlockManager: Removing RDD 1544
15/05/13 11:01:01 INFO UnionRDD: Removing RDD 1543 from persistence list
15/05/13 11:01:01 INFO BlockManager: Removing RDD 1543
15/05/13 11:01:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529200000 ms: 1431529140000 ms
15/05/13 11:01:01 INFO JobGenerator: Checkpointing graph for time 1431529260000 ms
15/05/13 11:01:01 INFO DStreamGraph: Updating checkpoint data for time 1431529260000 ms
15/05/13 11:01:01 INFO DStreamGraph: Updated checkpoint data for time 1431529260000 ms
15/05/13 11:01:01 INFO CheckpointWriter: Saving checkpoint for time 1431529260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529260000'
15/05/13 11:01:01 INFO CheckpointWriter: Checkpoint for time 1431529260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529260000', took 7654 bytes and 54 ms
15/05/13 11:01:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529260000 ms
15/05/13 11:01:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529260000 ms
15/05/13 11:01:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:02:00 INFO FileInputDStream: Finding new files took 63 ms
15/05/13 11:02:00 INFO FileInputDStream: New files at time 1431529320000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529208919.json
15/05/13 11:02:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=27075524, maxMem=278302556
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_330 stored as values in memory (estimated size 232.9 KB, free 239.4 MB)
15/05/13 11:02:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=27314056, maxMem=278302556
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_330_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.3 MB)
15/05/13 11:02:00 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_330_piece0
15/05/13 11:02:00 INFO SparkContext: Created broadcast 330 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:02:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:02:00 INFO JobScheduler: Starting job streaming job 1431529320000 ms.0 from job set of time 1431529320000 ms
15/05/13 11:02:00 INFO JobScheduler: Added jobs for time 1431529320000 ms
15/05/13 11:02:00 INFO JobGenerator: Checkpointing graph for time 1431529320000 ms
15/05/13 11:02:00 INFO DStreamGraph: Updating checkpoint data for time 1431529320000 ms
15/05/13 11:02:00 INFO DStreamGraph: Updated checkpoint data for time 1431529320000 ms
15/05/13 11:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431529320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000'
15/05/13 11:02:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83778): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:02:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83778): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431529320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000'
15/05/13 11:02:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83780): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:02:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83780): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431529320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000'
15/05/13 11:02:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:02:00 INFO DAGScheduler: Got job 225 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:02:00 INFO DAGScheduler: Final stage: Stage 220(reduce at JsonRDD.scala:51)
15/05/13 11:02:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:02:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:02:00 INFO DAGScheduler: Submitting Stage 220 (MapPartitionsRDD[1576] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:02:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=27349764, maxMem=278302556
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_331 stored as values in memory (estimated size 5.9 KB, free 239.3 MB)
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 325
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_325_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_325_piece0 of size 4227 dropped from memory (free 250950979)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_325_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_325_piece0
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_325
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_325 of size 6040 dropped from memory (free 250957019)
15/05/13 11:02:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=27345537, maxMem=278302556
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_331_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.3 MB)
15/05/13 11:02:00 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_331_piece0
15/05/13 11:02:00 INFO SparkContext: Created broadcast 331 from broadcast at DAGScheduler.scala:839
15/05/13 11:02:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83782): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:02:00 WARN CheckpointWriter: Could not write checkpoint for time 1431529320000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000'
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_325_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:02:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 220 (MapPartitionsRDD[1576] at map at JsonRDD.scala:51)
15/05/13 11:02:00 INFO TaskSchedulerImpl: Adding task set 220.0 with 1 tasks
15/05/13 11:02:00 INFO TaskSetManager: Starting task 0.0 in stage 220.0 (TID 220, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 325
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 326
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_326
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_326 of size 21304 dropped from memory (free 250974098)
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_326_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_326_piece0 of size 11129 dropped from memory (free 250985227)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_326_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_326_piece0
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_326_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 263.1 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 326
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 329
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_329_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_329_piece0 of size 11105 dropped from memory (free 250996332)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_329_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_329_piece0
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_329
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_329 of size 21304 dropped from memory (free 251017636)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_329_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.8 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO BlockManagerInfo: Added broadcast_331_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 329
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 320
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_320_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_320_piece0 of size 10746 dropped from memory (free 251028382)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_320_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_320_piece0
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_320
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_320 of size 20328 dropped from memory (free 251048710)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_320_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 320
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 321
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_321
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_321 of size 238532 dropped from memory (free 251287242)
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_321_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_321_piece0 of size 35708 dropped from memory (free 251322950)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_321_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_321_piece0
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_321_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.1 MB)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_321_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 321
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 322
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_322
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_322 of size 6040 dropped from memory (free 251328990)
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_322_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_322_piece0 of size 4225 dropped from memory (free 251333215)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_322_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_322_piece0
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_322_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 322
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 324
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_324_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_324_piece0 of size 35708 dropped from memory (free 251368923)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_324_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_324_piece0
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_324
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_324 of size 238532 dropped from memory (free 251607455)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_324_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 324
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 323
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_323_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_323_piece0 of size 10846 dropped from memory (free 251618301)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_323_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 262.1 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_323_piece0
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_323
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_323 of size 20920 dropped from memory (free 251639221)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_323_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 323
15/05/13 11:02:00 INFO BlockManager: Removing broadcast 328
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_328
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_328 of size 6040 dropped from memory (free 251645261)
15/05/13 11:02:00 INFO BlockManager: Removing block broadcast_328_piece0
15/05/13 11:02:00 INFO MemoryStore: Block broadcast_328_piece0 of size 4227 dropped from memory (free 251649488)
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_328_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:02:00 INFO BlockManagerMaster: Updated info of block broadcast_328_piece0
15/05/13 11:02:00 INFO BlockManagerInfo: Removed broadcast_328_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:02:00 INFO ContextCleaner: Cleaned broadcast 328
15/05/13 11:02:00 INFO TaskSetManager: Finished task 0.0 in stage 220.0 (TID 220) in 396 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:02:00 INFO TaskSchedulerImpl: Removed TaskSet 220.0, whose tasks have all completed, from pool 
15/05/13 11:02:00 INFO DAGScheduler: Stage 220 (reduce at JsonRDD.scala:51) finished in 0.410 s
15/05/13 11:02:00 INFO DAGScheduler: Job 225 finished: reduce at JsonRDD.scala:51, took 0.504196 s
15/05/13 11:02:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:02:01 INFO DAGScheduler: Got job 226 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:02:01 INFO DAGScheduler: Final stage: Stage 221(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:02:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:02:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:02:01 INFO DAGScheduler: Submitting Stage 221 (MapPartitionsRDD[1583] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:02:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=26653068, maxMem=278302556
15/05/13 11:02:01 INFO MemoryStore: Block broadcast_332 stored as values in memory (estimated size 21.0 KB, free 240.0 MB)
15/05/13 11:02:01 INFO MemoryStore: ensureFreeSpace(11141) called with curMem=26674548, maxMem=278302556
15/05/13 11:02:01 INFO MemoryStore: Block broadcast_332_piece0 stored as bytes in memory (estimated size 10.9 KB, free 240.0 MB)
15/05/13 11:02:01 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 262.1 MB)
15/05/13 11:02:01 INFO BlockManagerMaster: Updated info of block broadcast_332_piece0
15/05/13 11:02:01 INFO SparkContext: Created broadcast 332 from broadcast at DAGScheduler.scala:839
15/05/13 11:02:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 221 (MapPartitionsRDD[1583] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:02:01 INFO TaskSchedulerImpl: Adding task set 221.0 with 1 tasks
15/05/13 11:02:01 INFO TaskSetManager: Starting task 0.0 in stage 221.0 (TID 221, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:02:01 INFO BlockManagerInfo: Added broadcast_332_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 263.2 MB)
15/05/13 11:02:01 INFO BlockManagerInfo: Added broadcast_330_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.1 MB)
15/05/13 11:02:01 INFO TaskSetManager: Finished task 0.0 in stage 221.0 (TID 221) in 221 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:02:01 INFO TaskSchedulerImpl: Removed TaskSet 221.0, whose tasks have all completed, from pool 
15/05/13 11:02:01 INFO DAGScheduler: Stage 221 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.223 s
15/05/13 11:02:01 INFO DAGScheduler: Job 226 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.253883 s
15/05/13 11:02:01 INFO JobScheduler: Finished job streaming job 1431529320000 ms.0 from job set of time 1431529320000 ms
15/05/13 11:02:01 INFO JobScheduler: Total delay: 1.377 s for time 1431529320000 ms (execution: 1.168 s)
15/05/13 11:02:01 INFO MapPartitionsRDD: Removing RDD 1558 from persistence list
15/05/13 11:02:01 INFO BlockManager: Removing RDD 1558
15/05/13 11:02:01 INFO UnionRDD: Removing RDD 1557 from persistence list
15/05/13 11:02:01 INFO BlockManager: Removing RDD 1557
15/05/13 11:02:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529260000 ms: 1431529200000 ms
15/05/13 11:02:01 INFO JobGenerator: Checkpointing graph for time 1431529320000 ms
15/05/13 11:02:01 INFO DStreamGraph: Updating checkpoint data for time 1431529320000 ms
15/05/13 11:02:01 INFO DStreamGraph: Updated checkpoint data for time 1431529320000 ms
15/05/13 11:02:01 INFO CheckpointWriter: Saving checkpoint for time 1431529320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000'
15/05/13 11:02:01 INFO CheckpointWriter: Checkpoint for time 1431529320000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000', took 7655 bytes and 54 ms
15/05/13 11:02:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529320000 ms
15/05/13 11:02:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529320000 ms
15/05/13 11:02:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:03:00 INFO FileInputDStream: Finding new files took 60 ms
15/05/13 11:03:00 INFO FileInputDStream: New files at time 1431529380000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529269129.json
15/05/13 11:03:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=26685689, maxMem=278302556
15/05/13 11:03:00 INFO MemoryStore: Block broadcast_333 stored as values in memory (estimated size 232.9 KB, free 239.7 MB)
15/05/13 11:03:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=26924221, maxMem=278302556
15/05/13 11:03:00 INFO MemoryStore: Block broadcast_333_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.7 MB)
15/05/13 11:03:00 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:03:00 INFO BlockManagerMaster: Updated info of block broadcast_333_piece0
15/05/13 11:03:00 INFO SparkContext: Created broadcast 333 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:03:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:03:00 INFO JobScheduler: Starting job streaming job 1431529380000 ms.0 from job set of time 1431529380000 ms
15/05/13 11:03:00 INFO JobScheduler: Added jobs for time 1431529380000 ms
15/05/13 11:03:00 INFO JobGenerator: Checkpointing graph for time 1431529380000 ms
15/05/13 11:03:00 INFO DStreamGraph: Updating checkpoint data for time 1431529380000 ms
15/05/13 11:03:00 INFO DStreamGraph: Updated checkpoint data for time 1431529380000 ms
15/05/13 11:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431529380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000'
15/05/13 11:03:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83794): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:03:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83794): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431529380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000'
15/05/13 11:03:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:03:00 INFO DAGScheduler: Got job 227 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:03:00 INFO DAGScheduler: Final stage: Stage 222(reduce at JsonRDD.scala:51)
15/05/13 11:03:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:03:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:03:00 INFO DAGScheduler: Submitting Stage 222 (MapPartitionsRDD[1590] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:03:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=26959929, maxMem=278302556
15/05/13 11:03:00 INFO MemoryStore: Block broadcast_334 stored as values in memory (estimated size 5.9 KB, free 239.7 MB)
15/05/13 11:03:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=26965969, maxMem=278302556
15/05/13 11:03:00 INFO MemoryStore: Block broadcast_334_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.7 MB)
15/05/13 11:03:00 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:03:00 INFO BlockManagerMaster: Updated info of block broadcast_334_piece0
15/05/13 11:03:00 INFO SparkContext: Created broadcast 334 from broadcast at DAGScheduler.scala:839
15/05/13 11:03:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 222 (MapPartitionsRDD[1590] at map at JsonRDD.scala:51)
15/05/13 11:03:00 INFO TaskSchedulerImpl: Adding task set 222.0 with 1 tasks
15/05/13 11:03:00 INFO TaskSetManager: Starting task 0.0 in stage 222.0 (TID 222, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:03:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529080000.bk
15/05/13 11:03:00 INFO CheckpointWriter: Checkpoint for time 1431529380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000', took 7665 bytes and 154 ms
15/05/13 11:03:00 INFO BlockManagerInfo: Added broadcast_334_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:03:00 INFO BlockManagerInfo: Added broadcast_333_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.2 MB)
15/05/13 11:03:00 INFO TaskSetManager: Finished task 0.0 in stage 222.0 (TID 222) in 537 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:03:00 INFO TaskSchedulerImpl: Removed TaskSet 222.0, whose tasks have all completed, from pool 
15/05/13 11:03:00 INFO DAGScheduler: Stage 222 (reduce at JsonRDD.scala:51) finished in 0.552 s
15/05/13 11:03:00 INFO DAGScheduler: Job 227 finished: reduce at JsonRDD.scala:51, took 0.589130 s
15/05/13 11:03:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:03:01 INFO DAGScheduler: Got job 228 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:03:01 INFO DAGScheduler: Final stage: Stage 223(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:03:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:03:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:03:01 INFO DAGScheduler: Submitting Stage 223 (MapPartitionsRDD[1597] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:03:01 INFO MemoryStore: ensureFreeSpace(21888) called with curMem=26970193, maxMem=278302556
15/05/13 11:03:01 INFO MemoryStore: Block broadcast_335 stored as values in memory (estimated size 21.4 KB, free 239.7 MB)
15/05/13 11:03:01 INFO MemoryStore: ensureFreeSpace(11350) called with curMem=26992081, maxMem=278302556
15/05/13 11:03:01 INFO MemoryStore: Block broadcast_335_piece0 stored as bytes in memory (estimated size 11.1 KB, free 239.7 MB)
15/05/13 11:03:01 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.1 KB, free: 262.0 MB)
15/05/13 11:03:01 INFO BlockManagerMaster: Updated info of block broadcast_335_piece0
15/05/13 11:03:01 INFO SparkContext: Created broadcast 335 from broadcast at DAGScheduler.scala:839
15/05/13 11:03:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 223 (MapPartitionsRDD[1597] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:03:01 INFO TaskSchedulerImpl: Adding task set 223.0 with 1 tasks
15/05/13 11:03:01 INFO TaskSetManager: Starting task 0.0 in stage 223.0 (TID 223, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:03:01 INFO BlockManagerInfo: Added broadcast_335_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 11.1 KB, free: 264.2 MB)
15/05/13 11:03:01 INFO TaskSetManager: Finished task 0.0 in stage 223.0 (TID 223) in 189 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:03:01 INFO TaskSchedulerImpl: Removed TaskSet 223.0, whose tasks have all completed, from pool 
15/05/13 11:03:01 INFO DAGScheduler: Stage 223 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.191 s
15/05/13 11:03:01 INFO DAGScheduler: Job 228 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.226374 s
15/05/13 11:03:01 INFO JobScheduler: Finished job streaming job 1431529380000 ms.0 from job set of time 1431529380000 ms
15/05/13 11:03:01 INFO JobScheduler: Total delay: 1.395 s for time 1431529380000 ms (execution: 1.167 s)
15/05/13 11:03:01 INFO MapPartitionsRDD: Removing RDD 1572 from persistence list
15/05/13 11:03:01 INFO BlockManager: Removing RDD 1572
15/05/13 11:03:01 INFO UnionRDD: Removing RDD 1571 from persistence list
15/05/13 11:03:01 INFO BlockManager: Removing RDD 1571
15/05/13 11:03:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529320000 ms: 1431529260000 ms
15/05/13 11:03:01 INFO JobGenerator: Checkpointing graph for time 1431529380000 ms
15/05/13 11:03:01 INFO DStreamGraph: Updating checkpoint data for time 1431529380000 ms
15/05/13 11:03:01 INFO DStreamGraph: Updated checkpoint data for time 1431529380000 ms
15/05/13 11:03:01 INFO CheckpointWriter: Saving checkpoint for time 1431529380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000'
15/05/13 11:03:01 INFO CheckpointWriter: Checkpoint for time 1431529380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529380000', took 7655 bytes and 52 ms
15/05/13 11:03:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529380000 ms
15/05/13 11:03:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529380000 ms
15/05/13 11:03:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:04:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 11:04:00 INFO FileInputDStream: New files at time 1431529440000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529337431.json
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=27003431, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_336 stored as values in memory (estimated size 232.9 KB, free 239.4 MB)
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=27241963, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_336_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.4 MB)
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:04:00 INFO BlockManagerMaster: Updated info of block broadcast_336_piece0
15/05/13 11:04:00 INFO SparkContext: Created broadcast 336 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:04:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:04:00 INFO JobScheduler: Added jobs for time 1431529440000 ms
15/05/13 11:04:00 INFO JobGenerator: Checkpointing graph for time 1431529440000 ms
15/05/13 11:04:00 INFO JobScheduler: Starting job streaming job 1431529440000 ms.0 from job set of time 1431529440000 ms
15/05/13 11:04:00 INFO DStreamGraph: Updating checkpoint data for time 1431529440000 ms
15/05/13 11:04:00 INFO DStreamGraph: Updated checkpoint data for time 1431529440000 ms
15/05/13 11:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431529440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529440000'
15/05/13 11:04:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:04:00 INFO DAGScheduler: Got job 229 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:04:00 INFO DAGScheduler: Final stage: Stage 224(reduce at JsonRDD.scala:51)
15/05/13 11:04:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:04:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:04:00 INFO DAGScheduler: Submitting Stage 224 (MapPartitionsRDD[1604] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=27277671, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_337 stored as values in memory (estimated size 5.9 KB, free 239.4 MB)
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=27283711, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_337_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.4 MB)
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:04:00 INFO BlockManagerMaster: Updated info of block broadcast_337_piece0
15/05/13 11:04:00 INFO SparkContext: Created broadcast 337 from broadcast at DAGScheduler.scala:839
15/05/13 11:04:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 224 (MapPartitionsRDD[1604] at map at JsonRDD.scala:51)
15/05/13 11:04:00 INFO TaskSchedulerImpl: Adding task set 224.0 with 1 tasks
15/05/13 11:04:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529140000.bk
15/05/13 11:04:00 INFO TaskSetManager: Starting task 0.0 in stage 224.0 (TID 224, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:04:00 INFO CheckpointWriter: Checkpoint for time 1431529440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529440000', took 7661 bytes and 77 ms
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_337_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_336_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.1 MB)
15/05/13 11:04:00 INFO TaskSetManager: Finished task 0.0 in stage 224.0 (TID 224) in 345 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:04:00 INFO TaskSchedulerImpl: Removed TaskSet 224.0, whose tasks have all completed, from pool 
15/05/13 11:04:00 INFO DAGScheduler: Stage 224 (reduce at JsonRDD.scala:51) finished in 0.353 s
15/05/13 11:04:00 INFO DAGScheduler: Job 229 finished: reduce at JsonRDD.scala:51, took 0.373912 s
15/05/13 11:04:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:04:00 INFO DAGScheduler: Got job 230 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:04:00 INFO DAGScheduler: Final stage: Stage 225(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:04:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:04:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:04:00 INFO DAGScheduler: Submitting Stage 225 (MapPartitionsRDD[1611] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(21888) called with curMem=27287936, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_338 stored as values in memory (estimated size 21.4 KB, free 239.4 MB)
15/05/13 11:04:00 INFO MemoryStore: ensureFreeSpace(11347) called with curMem=27309824, maxMem=278302556
15/05/13 11:04:00 INFO MemoryStore: Block broadcast_338_piece0 stored as bytes in memory (estimated size 11.1 KB, free 239.4 MB)
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.1 KB, free: 262.0 MB)
15/05/13 11:04:00 INFO BlockManagerMaster: Updated info of block broadcast_338_piece0
15/05/13 11:04:00 INFO SparkContext: Created broadcast 338 from broadcast at DAGScheduler.scala:839
15/05/13 11:04:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 225 (MapPartitionsRDD[1611] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:04:00 INFO TaskSchedulerImpl: Adding task set 225.0 with 1 tasks
15/05/13 11:04:00 INFO TaskSetManager: Starting task 0.0 in stage 225.0 (TID 225, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:04:00 INFO BlockManagerInfo: Added broadcast_338_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.1 KB, free: 263.1 MB)
15/05/13 11:04:00 INFO TaskSetManager: Finished task 0.0 in stage 225.0 (TID 225) in 148 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:04:00 INFO TaskSchedulerImpl: Removed TaskSet 225.0, whose tasks have all completed, from pool 
15/05/13 11:04:00 INFO DAGScheduler: Stage 225 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.150 s
15/05/13 11:04:00 INFO DAGScheduler: Job 230 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.187661 s
15/05/13 11:04:01 INFO JobScheduler: Finished job streaming job 1431529440000 ms.0 from job set of time 1431529440000 ms
15/05/13 11:04:01 INFO JobScheduler: Total delay: 1.042 s for time 1431529440000 ms (execution: 0.906 s)
15/05/13 11:04:01 INFO MapPartitionsRDD: Removing RDD 1586 from persistence list
15/05/13 11:04:01 INFO BlockManager: Removing RDD 1586
15/05/13 11:04:01 INFO UnionRDD: Removing RDD 1585 from persistence list
15/05/13 11:04:01 INFO BlockManager: Removing RDD 1585
15/05/13 11:04:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529380000 ms: 1431529320000 ms
15/05/13 11:04:01 INFO JobGenerator: Checkpointing graph for time 1431529440000 ms
15/05/13 11:04:01 INFO DStreamGraph: Updating checkpoint data for time 1431529440000 ms
15/05/13 11:04:01 INFO DStreamGraph: Updated checkpoint data for time 1431529440000 ms
15/05/13 11:04:01 INFO CheckpointWriter: Saving checkpoint for time 1431529440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529440000'
15/05/13 11:04:01 INFO CheckpointWriter: Checkpoint for time 1431529440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529440000', took 7655 bytes and 49 ms
15/05/13 11:04:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529440000 ms
15/05/13 11:04:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529440000 ms
15/05/13 11:04:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:05:00 INFO FileInputDStream: Finding new files took 49 ms
15/05/13 11:05:00 INFO FileInputDStream: New files at time 1431529500000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529398189.json
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=27321171, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_339 stored as values in memory (estimated size 232.9 KB, free 239.1 MB)
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=27559703, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_339_piece0 stored as bytes in memory (estimated size 34.9 KB, free 239.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_339_piece0
15/05/13 11:05:00 INFO SparkContext: Created broadcast 339 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:05:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:05:00 INFO JobScheduler: Added jobs for time 1431529500000 ms
15/05/13 11:05:00 INFO JobScheduler: Starting job streaming job 1431529500000 ms.0 from job set of time 1431529500000 ms
15/05/13 11:05:00 INFO JobGenerator: Checkpointing graph for time 1431529500000 ms
15/05/13 11:05:00 INFO DStreamGraph: Updating checkpoint data for time 1431529500000 ms
15/05/13 11:05:00 INFO DStreamGraph: Updated checkpoint data for time 1431529500000 ms
15/05/13 11:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431529500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000'
15/05/13 11:05:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83807): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:05:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83807): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431529500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000'
15/05/13 11:05:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:05:00 INFO DAGScheduler: Got job 231 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:05:00 INFO DAGScheduler: Final stage: Stage 226(reduce at JsonRDD.scala:51)
15/05/13 11:05:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:05:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:05:00 INFO DAGScheduler: Submitting Stage 226 (MapPartitionsRDD[1618] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=27595411, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_340 stored as values in memory (estimated size 5.9 KB, free 239.1 MB)
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=27601451, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_340_piece0 stored as bytes in memory (estimated size 4.1 KB, free 239.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_340_piece0
15/05/13 11:05:00 INFO SparkContext: Created broadcast 340 from broadcast at DAGScheduler.scala:839
15/05/13 11:05:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 226 (MapPartitionsRDD[1618] at map at JsonRDD.scala:51)
15/05/13 11:05:00 INFO TaskSchedulerImpl: Adding task set 226.0 with 1 tasks
15/05/13 11:05:00 INFO TaskSetManager: Starting task 0.0 in stage 226.0 (TID 226, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:05:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529200000.bk
15/05/13 11:05:00 INFO CheckpointWriter: Checkpoint for time 1431529500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000', took 7661 bytes and 79 ms
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_340_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.0 MB)
15/05/13 11:05:00 INFO TaskSetManager: Finished task 0.0 in stage 226.0 (TID 226) in 237 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:05:00 INFO TaskSchedulerImpl: Removed TaskSet 226.0, whose tasks have all completed, from pool 
15/05/13 11:05:00 INFO DAGScheduler: Stage 226 (reduce at JsonRDD.scala:51) finished in 0.244 s
15/05/13 11:05:00 INFO DAGScheduler: Job 231 finished: reduce at JsonRDD.scala:51, took 0.261376 s
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 334
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_334
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_334 of size 6040 dropped from memory (free 250702920)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_334_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_334_piece0 of size 4224 dropped from memory (free 250707144)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_334_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_334_piece0
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_334_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:05:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:05:00 INFO DAGScheduler: Got job 232 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:05:00 INFO DAGScheduler: Final stage: Stage 227(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:05:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 334
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 332
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_332_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_332_piece0 of size 11141 dropped from memory (free 250718285)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_332_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_332_piece0
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_332
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_332 of size 21480 dropped from memory (free 250739765)
15/05/13 11:05:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:05:00 INFO DAGScheduler: Submitting Stage 227 (MapPartitionsRDD[1625] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_332_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 263.0 MB)
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=27562791, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_341 stored as values in memory (estimated size 20.4 KB, free 239.1 MB)
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 332
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 331
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_331_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_331_piece0 of size 4225 dropped from memory (free 250723070)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_331_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_331_piece0
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_331
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_331 of size 6040 dropped from memory (free 250729110)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_331_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.2 MB)
15/05/13 11:05:00 INFO MemoryStore: ensureFreeSpace(10846) called with curMem=27573446, maxMem=278302556
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_341_piece0 stored as bytes in memory (estimated size 10.6 KB, free 239.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_341_piece0
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 331
15/05/13 11:05:00 INFO SparkContext: Created broadcast 341 from broadcast at DAGScheduler.scala:839
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 338
15/05/13 11:05:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 227 (MapPartitionsRDD[1625] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_338
15/05/13 11:05:00 INFO TaskSchedulerImpl: Adding task set 227.0 with 1 tasks
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_338 of size 21888 dropped from memory (free 250740152)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_338_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_338_piece0 of size 11347 dropped from memory (free 250751499)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_338_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.1 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_338_piece0
15/05/13 11:05:00 INFO TaskSetManager: Starting task 0.0 in stage 227.0 (TID 227, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_338_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.1 KB, free: 263.1 MB)
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 338
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 337
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_337
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_337 of size 6040 dropped from memory (free 250757539)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_337_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_337_piece0 of size 4225 dropped from memory (free 250761764)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_337_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_337_piece0
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_337_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 337
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 335
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_335
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_335 of size 21888 dropped from memory (free 250783652)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_335_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_335_piece0 of size 11350 dropped from memory (free 250795002)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_335_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.1 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_335_piece0
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_341_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 264.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_335_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 11.1 KB, free: 264.2 MB)
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 335
15/05/13 11:05:00 INFO BlockManager: Removing broadcast 340
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_340
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_340 of size 6040 dropped from memory (free 250801042)
15/05/13 11:05:00 INFO BlockManager: Removing block broadcast_340_piece0
15/05/13 11:05:00 INFO MemoryStore: Block broadcast_340_piece0 of size 4225 dropped from memory (free 250805267)
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_340_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:05:00 INFO BlockManagerMaster: Updated info of block broadcast_340_piece0
15/05/13 11:05:00 INFO BlockManagerInfo: Removed broadcast_340_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.1 MB)
15/05/13 11:05:00 INFO BlockManagerInfo: Added broadcast_339_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.1 MB)
15/05/13 11:05:00 INFO ContextCleaner: Cleaned broadcast 340
15/05/13 11:05:00 INFO TaskSetManager: Finished task 0.0 in stage 227.0 (TID 227) in 275 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:05:00 INFO TaskSchedulerImpl: Removed TaskSet 227.0, whose tasks have all completed, from pool 
15/05/13 11:05:00 INFO DAGScheduler: Stage 227 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.276 s
15/05/13 11:05:00 INFO DAGScheduler: Job 232 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.320066 s
15/05/13 11:05:00 INFO JobScheduler: Finished job streaming job 1431529500000 ms.0 from job set of time 1431529500000 ms
15/05/13 11:05:00 INFO JobScheduler: Total delay: 0.960 s for time 1431529500000 ms (execution: 0.809 s)
15/05/13 11:05:00 INFO MapPartitionsRDD: Removing RDD 1600 from persistence list
15/05/13 11:05:00 INFO BlockManager: Removing RDD 1600
15/05/13 11:05:00 INFO UnionRDD: Removing RDD 1599 from persistence list
15/05/13 11:05:00 INFO BlockManager: Removing RDD 1599
15/05/13 11:05:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431529440000 ms: 1431529380000 ms
15/05/13 11:05:00 INFO JobGenerator: Checkpointing graph for time 1431529500000 ms
15/05/13 11:05:00 INFO DStreamGraph: Updating checkpoint data for time 1431529500000 ms
15/05/13 11:05:00 INFO DStreamGraph: Updated checkpoint data for time 1431529500000 ms
15/05/13 11:05:00 INFO CheckpointWriter: Saving checkpoint for time 1431529500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000'
15/05/13 11:05:01 INFO CheckpointWriter: Checkpoint for time 1431529500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529500000', took 7654 bytes and 74 ms
15/05/13 11:05:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529500000 ms
15/05/13 11:05:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529500000 ms
15/05/13 11:05:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:06:00 INFO FileInputDStream: Finding new files took 29 ms
15/05/13 11:06:00 INFO FileInputDStream: New files at time 1431529560000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529458388.json
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=27497289, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_342 stored as values in memory (estimated size 232.9 KB, free 239.0 MB)
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=27735821, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_342_piece0 stored as bytes in memory (estimated size 34.9 KB, free 238.9 MB)
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:06:00 INFO BlockManagerMaster: Updated info of block broadcast_342_piece0
15/05/13 11:06:00 INFO SparkContext: Created broadcast 342 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:06:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:06:00 INFO JobScheduler: Added jobs for time 1431529560000 ms
15/05/13 11:06:00 INFO JobGenerator: Checkpointing graph for time 1431529560000 ms
15/05/13 11:06:00 INFO DStreamGraph: Updating checkpoint data for time 1431529560000 ms
15/05/13 11:06:00 INFO DStreamGraph: Updated checkpoint data for time 1431529560000 ms
15/05/13 11:06:00 INFO JobScheduler: Starting job streaming job 1431529560000 ms.0 from job set of time 1431529560000 ms
15/05/13 11:06:00 INFO CheckpointWriter: Saving checkpoint for time 1431529560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000'
15/05/13 11:06:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83816): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:06:00 INFO CheckpointWriter: Saving checkpoint for time 1431529560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000'
15/05/13 11:06:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:06:00 INFO DAGScheduler: Got job 233 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:06:00 INFO DAGScheduler: Final stage: Stage 228(reduce at JsonRDD.scala:51)
15/05/13 11:06:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:06:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:06:00 INFO DAGScheduler: Submitting Stage 228 (MapPartitionsRDD[1632] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=27771529, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_343 stored as values in memory (estimated size 5.9 KB, free 238.9 MB)
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=27777569, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_343_piece0 stored as bytes in memory (estimated size 4.1 KB, free 238.9 MB)
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:06:00 INFO BlockManagerMaster: Updated info of block broadcast_343_piece0
15/05/13 11:06:00 INFO SparkContext: Created broadcast 343 from broadcast at DAGScheduler.scala:839
15/05/13 11:06:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 228 (MapPartitionsRDD[1632] at map at JsonRDD.scala:51)
15/05/13 11:06:00 INFO TaskSchedulerImpl: Adding task set 228.0 with 1 tasks
15/05/13 11:06:00 INFO TaskSetManager: Starting task 0.0 in stage 228.0 (TID 228, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:06:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83818): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:06:00 INFO CheckpointWriter: Saving checkpoint for time 1431529560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000'
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_343_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:06:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529260000.bk
15/05/13 11:06:00 INFO CheckpointWriter: Checkpoint for time 1431529560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000', took 7665 bytes and 170 ms
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.1 MB)
15/05/13 11:06:00 INFO TaskSetManager: Finished task 0.0 in stage 228.0 (TID 228) in 400 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:06:00 INFO TaskSchedulerImpl: Removed TaskSet 228.0, whose tasks have all completed, from pool 
15/05/13 11:06:00 INFO DAGScheduler: Stage 228 (reduce at JsonRDD.scala:51) finished in 0.413 s
15/05/13 11:06:00 INFO DAGScheduler: Job 233 finished: reduce at JsonRDD.scala:51, took 0.440276 s
15/05/13 11:06:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:06:00 INFO DAGScheduler: Got job 234 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:06:00 INFO DAGScheduler: Final stage: Stage 229(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:06:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:06:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:06:00 INFO DAGScheduler: Submitting Stage 229 (MapPartitionsRDD[1639] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=27781794, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_344 stored as values in memory (estimated size 20.0 KB, free 238.9 MB)
15/05/13 11:06:00 INFO MemoryStore: ensureFreeSpace(10771) called with curMem=27802282, maxMem=278302556
15/05/13 11:06:00 INFO MemoryStore: Block broadcast_344_piece0 stored as bytes in memory (estimated size 10.5 KB, free 238.9 MB)
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 261.9 MB)
15/05/13 11:06:00 INFO BlockManagerMaster: Updated info of block broadcast_344_piece0
15/05/13 11:06:00 INFO SparkContext: Created broadcast 344 from broadcast at DAGScheduler.scala:839
15/05/13 11:06:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 229 (MapPartitionsRDD[1639] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:06:00 INFO TaskSchedulerImpl: Adding task set 229.0 with 1 tasks
15/05/13 11:06:00 INFO TaskSetManager: Starting task 0.0 in stage 229.0 (TID 229, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:06:00 INFO BlockManagerInfo: Added broadcast_344_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.5 KB, free: 263.1 MB)
15/05/13 11:06:01 INFO BlockManagerInfo: Added broadcast_342_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.0 MB)
15/05/13 11:06:01 INFO TaskSetManager: Finished task 0.0 in stage 229.0 (TID 229) in 304 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:06:01 INFO TaskSchedulerImpl: Removed TaskSet 229.0, whose tasks have all completed, from pool 
15/05/13 11:06:01 INFO DAGScheduler: Stage 229 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.306 s
15/05/13 11:06:01 INFO DAGScheduler: Job 234 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.332905 s
15/05/13 11:06:01 INFO JobScheduler: Finished job streaming job 1431529560000 ms.0 from job set of time 1431529560000 ms
15/05/13 11:06:01 INFO JobScheduler: Total delay: 1.289 s for time 1431529560000 ms (execution: 1.136 s)
15/05/13 11:06:01 INFO MapPartitionsRDD: Removing RDD 1614 from persistence list
15/05/13 11:06:01 INFO BlockManager: Removing RDD 1614
15/05/13 11:06:01 INFO UnionRDD: Removing RDD 1613 from persistence list
15/05/13 11:06:01 INFO BlockManager: Removing RDD 1613
15/05/13 11:06:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529500000 ms: 1431529440000 ms
15/05/13 11:06:01 INFO JobGenerator: Checkpointing graph for time 1431529560000 ms
15/05/13 11:06:01 INFO DStreamGraph: Updating checkpoint data for time 1431529560000 ms
15/05/13 11:06:01 INFO DStreamGraph: Updated checkpoint data for time 1431529560000 ms
15/05/13 11:06:01 INFO CheckpointWriter: Saving checkpoint for time 1431529560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000'
15/05/13 11:06:01 INFO CheckpointWriter: Checkpoint for time 1431529560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000', took 7657 bytes and 68 ms
15/05/13 11:06:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529560000 ms
15/05/13 11:06:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529560000 ms
15/05/13 11:06:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:07:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 11:07:00 INFO FileInputDStream: New files at time 1431529620000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529518817.json
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=27813053, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_345 stored as values in memory (estimated size 232.9 KB, free 238.7 MB)
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=28051585, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_345_piece0 stored as bytes in memory (estimated size 34.9 KB, free 238.6 MB)
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:07:00 INFO BlockManagerMaster: Updated info of block broadcast_345_piece0
15/05/13 11:07:00 INFO SparkContext: Created broadcast 345 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:07:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:07:00 INFO JobScheduler: Added jobs for time 1431529620000 ms
15/05/13 11:07:00 INFO JobScheduler: Starting job streaming job 1431529620000 ms.0 from job set of time 1431529620000 ms
15/05/13 11:07:00 INFO JobGenerator: Checkpointing graph for time 1431529620000 ms
15/05/13 11:07:00 INFO DStreamGraph: Updating checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO DStreamGraph: Updated checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431529620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000'
15/05/13 11:07:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:07:00 INFO DAGScheduler: Got job 235 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:07:00 INFO DAGScheduler: Final stage: Stage 230(reduce at JsonRDD.scala:51)
15/05/13 11:07:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:07:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:07:00 INFO DAGScheduler: Submitting Stage 230 (MapPartitionsRDD[1646] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=28087293, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_346 stored as values in memory (estimated size 5.9 KB, free 238.6 MB)
15/05/13 11:07:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83825): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431529620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000'
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=28093333, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_346_piece0 stored as bytes in memory (estimated size 4.1 KB, free 238.6 MB)
15/05/13 11:07:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83827): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:07:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83827): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:07:00 INFO BlockManagerMaster: Updated info of block broadcast_346_piece0
15/05/13 11:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431529620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000'
15/05/13 11:07:00 INFO SparkContext: Created broadcast 346 from broadcast at DAGScheduler.scala:839
15/05/13 11:07:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 230 (MapPartitionsRDD[1646] at map at JsonRDD.scala:51)
15/05/13 11:07:00 INFO TaskSchedulerImpl: Adding task set 230.0 with 1 tasks
15/05/13 11:07:00 INFO TaskSetManager: Starting task 0.0 in stage 230.0 (TID 230, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_346_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:07:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529320000.bk
15/05/13 11:07:00 INFO CheckpointWriter: Checkpoint for time 1431529620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000', took 7666 bytes and 167 ms
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_345_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.1 MB)
15/05/13 11:07:00 INFO TaskSetManager: Finished task 0.0 in stage 230.0 (TID 230) in 271 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:07:00 INFO DAGScheduler: Stage 230 (reduce at JsonRDD.scala:51) finished in 0.279 s
15/05/13 11:07:00 INFO DAGScheduler: Job 235 finished: reduce at JsonRDD.scala:51, took 0.324152 s
15/05/13 11:07:00 INFO TaskSchedulerImpl: Removed TaskSet 230.0, whose tasks have all completed, from pool 
15/05/13 11:07:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:07:00 INFO DAGScheduler: Got job 236 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:07:00 INFO DAGScheduler: Final stage: Stage 231(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:07:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:07:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:07:00 INFO DAGScheduler: Submitting Stage 231 (MapPartitionsRDD[1653] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=28097560, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_347 stored as values in memory (estimated size 21.0 KB, free 238.6 MB)
15/05/13 11:07:00 INFO MemoryStore: ensureFreeSpace(11144) called with curMem=28119040, maxMem=278302556
15/05/13 11:07:00 INFO MemoryStore: Block broadcast_347_piece0 stored as bytes in memory (estimated size 10.9 KB, free 238.6 MB)
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:07:00 INFO BlockManagerMaster: Updated info of block broadcast_347_piece0
15/05/13 11:07:00 INFO SparkContext: Created broadcast 347 from broadcast at DAGScheduler.scala:839
15/05/13 11:07:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 231 (MapPartitionsRDD[1653] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:07:00 INFO TaskSchedulerImpl: Adding task set 231.0 with 1 tasks
15/05/13 11:07:00 INFO TaskSetManager: Starting task 0.0 in stage 231.0 (TID 231, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:07:00 INFO BlockManagerInfo: Added broadcast_347_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.1 MB)
15/05/13 11:07:00 INFO TaskSetManager: Finished task 0.0 in stage 231.0 (TID 231) in 72 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:07:00 INFO TaskSchedulerImpl: Removed TaskSet 231.0, whose tasks have all completed, from pool 
15/05/13 11:07:00 INFO DAGScheduler: Stage 231 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.074 s
15/05/13 11:07:00 INFO DAGScheduler: Job 236 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.101727 s
15/05/13 11:07:00 INFO JobScheduler: Finished job streaming job 1431529620000 ms.0 from job set of time 1431529620000 ms
15/05/13 11:07:00 INFO JobScheduler: Total delay: 0.905 s for time 1431529620000 ms (execution: 0.737 s)
15/05/13 11:07:00 INFO MapPartitionsRDD: Removing RDD 1628 from persistence list
15/05/13 11:07:00 INFO BlockManager: Removing RDD 1628
15/05/13 11:07:00 INFO UnionRDD: Removing RDD 1627 from persistence list
15/05/13 11:07:00 INFO BlockManager: Removing RDD 1627
15/05/13 11:07:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431529560000 ms: 1431529500000 ms
15/05/13 11:07:00 INFO JobGenerator: Checkpointing graph for time 1431529620000 ms
15/05/13 11:07:00 INFO DStreamGraph: Updating checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO DStreamGraph: Updated checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO CheckpointWriter: Saving checkpoint for time 1431529620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000'
15/05/13 11:07:00 INFO CheckpointWriter: Checkpoint for time 1431529620000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000', took 7657 bytes and 52 ms
15/05/13 11:07:00 INFO DStreamGraph: Clearing checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO DStreamGraph: Cleared checkpoint data for time 1431529620000 ms
15/05/13 11:07:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:08:00 INFO FileInputDStream: Finding new files took 55 ms
15/05/13 11:08:00 INFO FileInputDStream: New files at time 1431529680000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529579796.json
15/05/13 11:08:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28130184, maxMem=278302556
15/05/13 11:08:00 INFO MemoryStore: Block broadcast_348 stored as values in memory (estimated size 232.9 KB, free 238.4 MB)
15/05/13 11:08:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=28368716, maxMem=278302556
15/05/13 11:08:00 INFO MemoryStore: Block broadcast_348_piece0 stored as bytes in memory (estimated size 34.9 KB, free 238.3 MB)
15/05/13 11:08:00 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:08:00 INFO BlockManagerMaster: Updated info of block broadcast_348_piece0
15/05/13 11:08:00 INFO SparkContext: Created broadcast 348 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:08:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:08:00 INFO JobScheduler: Starting job streaming job 1431529680000 ms.0 from job set of time 1431529680000 ms
15/05/13 11:08:00 INFO JobScheduler: Added jobs for time 1431529680000 ms
15/05/13 11:08:00 INFO JobGenerator: Checkpointing graph for time 1431529680000 ms
15/05/13 11:08:00 INFO DStreamGraph: Updating checkpoint data for time 1431529680000 ms
15/05/13 11:08:00 INFO DStreamGraph: Updated checkpoint data for time 1431529680000 ms
15/05/13 11:08:00 INFO CheckpointWriter: Saving checkpoint for time 1431529680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000'
15/05/13 11:08:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83840): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:08:00 INFO CheckpointWriter: Saving checkpoint for time 1431529680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000'
15/05/13 11:08:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:08:00 INFO DAGScheduler: Got job 237 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:08:00 INFO DAGScheduler: Final stage: Stage 232(reduce at JsonRDD.scala:51)
15/05/13 11:08:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:08:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:08:00 INFO DAGScheduler: Submitting Stage 232 (MapPartitionsRDD[1660] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:08:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=28404424, maxMem=278302556
15/05/13 11:08:00 INFO MemoryStore: Block broadcast_349 stored as values in memory (estimated size 5.9 KB, free 238.3 MB)
15/05/13 11:08:00 INFO MemoryStore: ensureFreeSpace(4223) called with curMem=28410464, maxMem=278302556
15/05/13 11:08:00 INFO MemoryStore: Block broadcast_349_piece0 stored as bytes in memory (estimated size 4.1 KB, free 238.3 MB)
15/05/13 11:08:00 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:08:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83842): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:08:00 INFO CheckpointWriter: Saving checkpoint for time 1431529680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000'
15/05/13 11:08:00 INFO BlockManagerMaster: Updated info of block broadcast_349_piece0
15/05/13 11:08:00 INFO SparkContext: Created broadcast 349 from broadcast at DAGScheduler.scala:839
15/05/13 11:08:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 232 (MapPartitionsRDD[1660] at map at JsonRDD.scala:51)
15/05/13 11:08:00 INFO TaskSchedulerImpl: Adding task set 232.0 with 1 tasks
15/05/13 11:08:00 INFO TaskSetManager: Starting task 0.0 in stage 232.0 (TID 232, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:08:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83844): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:08:00 WARN CheckpointWriter: Could not write checkpoint for time 1431529680000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000'
15/05/13 11:08:00 INFO BlockManagerInfo: Added broadcast_349_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:08:00 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:08:00 INFO TaskSetManager: Finished task 0.0 in stage 232.0 (TID 232) in 467 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:08:00 INFO TaskSchedulerImpl: Removed TaskSet 232.0, whose tasks have all completed, from pool 
15/05/13 11:08:00 INFO DAGScheduler: Stage 232 (reduce at JsonRDD.scala:51) finished in 0.481 s
15/05/13 11:08:00 INFO DAGScheduler: Job 237 finished: reduce at JsonRDD.scala:51, took 0.528919 s
15/05/13 11:08:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:08:00 INFO DAGScheduler: Got job 238 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:08:00 INFO DAGScheduler: Final stage: Stage 233(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:08:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:08:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:08:01 INFO DAGScheduler: Submitting Stage 233 (MapPartitionsRDD[1667] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:08:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=28414687, maxMem=278302556
15/05/13 11:08:01 INFO MemoryStore: Block broadcast_350 stored as values in memory (estimated size 20.8 KB, free 238.3 MB)
15/05/13 11:08:01 INFO MemoryStore: ensureFreeSpace(11105) called with curMem=28435991, maxMem=278302556
15/05/13 11:08:01 INFO MemoryStore: Block broadcast_350_piece0 stored as bytes in memory (estimated size 10.8 KB, free 238.3 MB)
15/05/13 11:08:01 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 261.8 MB)
15/05/13 11:08:01 INFO BlockManagerMaster: Updated info of block broadcast_350_piece0
15/05/13 11:08:01 INFO SparkContext: Created broadcast 350 from broadcast at DAGScheduler.scala:839
15/05/13 11:08:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 233 (MapPartitionsRDD[1667] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:08:01 INFO TaskSchedulerImpl: Adding task set 233.0 with 1 tasks
15/05/13 11:08:01 INFO TaskSetManager: Starting task 0.0 in stage 233.0 (TID 233, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:08:01 INFO BlockManagerInfo: Added broadcast_350_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 263.0 MB)
15/05/13 11:08:01 INFO BlockManagerInfo: Added broadcast_348_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.0 MB)
15/05/13 11:08:01 INFO TaskSetManager: Finished task 0.0 in stage 233.0 (TID 233) in 355 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:08:01 INFO DAGScheduler: Stage 233 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.362 s
15/05/13 11:08:01 INFO TaskSchedulerImpl: Removed TaskSet 233.0, whose tasks have all completed, from pool 
15/05/13 11:08:01 INFO DAGScheduler: Job 238 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.406022 s
15/05/13 11:08:01 INFO JobScheduler: Finished job streaming job 1431529680000 ms.0 from job set of time 1431529680000 ms
15/05/13 11:08:01 INFO JobScheduler: Total delay: 1.444 s for time 1431529680000 ms (execution: 1.283 s)
15/05/13 11:08:01 INFO MapPartitionsRDD: Removing RDD 1642 from persistence list
15/05/13 11:08:01 INFO BlockManager: Removing RDD 1642
15/05/13 11:08:01 INFO UnionRDD: Removing RDD 1641 from persistence list
15/05/13 11:08:01 INFO BlockManager: Removing RDD 1641
15/05/13 11:08:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529620000 ms: 1431529560000 ms
15/05/13 11:08:01 INFO JobGenerator: Checkpointing graph for time 1431529680000 ms
15/05/13 11:08:01 INFO DStreamGraph: Updating checkpoint data for time 1431529680000 ms
15/05/13 11:08:01 INFO DStreamGraph: Updated checkpoint data for time 1431529680000 ms
15/05/13 11:08:01 INFO CheckpointWriter: Saving checkpoint for time 1431529680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000'
15/05/13 11:08:01 INFO CheckpointWriter: Checkpoint for time 1431529680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000', took 7654 bytes and 56 ms
15/05/13 11:08:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529680000 ms
15/05/13 11:08:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529680000 ms
15/05/13 11:08:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:09:00 INFO FileInputDStream: Finding new files took 56 ms
15/05/13 11:09:00 INFO FileInputDStream: New files at time 1431529740000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529639996.json
15/05/13 11:09:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28447096, maxMem=278302556
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_351 stored as values in memory (estimated size 232.9 KB, free 238.1 MB)
15/05/13 11:09:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=28685628, maxMem=278302556
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_351_piece0 stored as bytes in memory (estimated size 34.9 KB, free 238.0 MB)
15/05/13 11:09:00 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_351_piece0
15/05/13 11:09:00 INFO SparkContext: Created broadcast 351 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:09:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:09:00 INFO JobScheduler: Added jobs for time 1431529740000 ms
15/05/13 11:09:00 INFO JobGenerator: Checkpointing graph for time 1431529740000 ms
15/05/13 11:09:00 INFO DStreamGraph: Updating checkpoint data for time 1431529740000 ms
15/05/13 11:09:00 INFO DStreamGraph: Updated checkpoint data for time 1431529740000 ms
15/05/13 11:09:00 INFO JobScheduler: Starting job streaming job 1431529740000 ms.0 from job set of time 1431529740000 ms
15/05/13 11:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431529740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000'
15/05/13 11:09:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83851): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:09:00 INFO CheckpointWriter: Saving checkpoint for time 1431529740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000'
15/05/13 11:09:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:09:00 INFO DAGScheduler: Got job 239 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:09:00 INFO DAGScheduler: Final stage: Stage 234(reduce at JsonRDD.scala:51)
15/05/13 11:09:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:09:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:09:00 INFO DAGScheduler: Submitting Stage 234 (MapPartitionsRDD[1674] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:09:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=28721336, maxMem=278302556
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_352 stored as values in memory (estimated size 5.9 KB, free 238.0 MB)
15/05/13 11:09:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=28727376, maxMem=278302556
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_352_piece0 stored as bytes in memory (estimated size 4.1 KB, free 238.0 MB)
15/05/13 11:09:00 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_352_piece0
15/05/13 11:09:00 INFO SparkContext: Created broadcast 352 from broadcast at DAGScheduler.scala:839
15/05/13 11:09:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 234 (MapPartitionsRDD[1674] at map at JsonRDD.scala:51)
15/05/13 11:09:00 INFO TaskSchedulerImpl: Adding task set 234.0 with 1 tasks
15/05/13 11:09:00 INFO TaskSetManager: Starting task 0.0 in stage 234.0 (TID 234, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:09:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529440000
15/05/13 11:09:00 INFO BlockManagerInfo: Added broadcast_352_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:09:00 INFO CheckpointWriter: Checkpoint for time 1431529740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000', took 7663 bytes and 279 ms
15/05/13 11:09:00 INFO BlockManagerInfo: Added broadcast_351_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:09:00 INFO TaskSetManager: Finished task 0.0 in stage 234.0 (TID 234) in 269 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:09:00 INFO TaskSchedulerImpl: Removed TaskSet 234.0, whose tasks have all completed, from pool 
15/05/13 11:09:00 INFO DAGScheduler: Stage 234 (reduce at JsonRDD.scala:51) finished in 0.277 s
15/05/13 11:09:00 INFO DAGScheduler: Job 239 finished: reduce at JsonRDD.scala:51, took 0.347871 s
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 343
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_343
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_343 of size 6040 dropped from memory (free 249576995)
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_343_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_343_piece0 of size 4225 dropped from memory (free 249581220)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_343_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_343_piece0
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_343_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 343
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 342
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_342_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_342_piece0 of size 35708 dropped from memory (free 249616928)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_342_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_342_piece0
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_342
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_342 of size 238532 dropped from memory (free 249855460)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_342_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_342_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 263.0 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 342
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 341
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_341
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_341 of size 20920 dropped from memory (free 249876380)
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_341_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_341_piece0 of size 10846 dropped from memory (free 249887226)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_341_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_341_piece0
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_341_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 264.1 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 341
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 352
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_352_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_352_piece0 of size 4225 dropped from memory (free 249891451)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_352_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_352_piece0
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_352
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_352 of size 6040 dropped from memory (free 249897491)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_352_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 352
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 350
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_350
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_350 of size 21304 dropped from memory (free 249918795)
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_350_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_350_piece0 of size 11105 dropped from memory (free 249929900)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_350_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 261.9 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_350_piece0
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_350_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 263.0 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 350
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 349
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_349_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_349_piece0 of size 4223 dropped from memory (free 249934123)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_349_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_349_piece0
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_349
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_349 of size 6040 dropped from memory (free 249940163)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_349_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 349
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 347
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_347_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_347_piece0 of size 11144 dropped from memory (free 249951307)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_347_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_347_piece0
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_347
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_347 of size 21480 dropped from memory (free 249972787)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_347_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.1 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 347
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 346
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_346_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_346_piece0 of size 4227 dropped from memory (free 249977014)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_346_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:09:00 INFO BlockManagerMaster: Updated info of block broadcast_346_piece0
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_346
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_346 of size 6040 dropped from memory (free 249983054)
15/05/13 11:09:00 INFO BlockManagerInfo: Removed broadcast_346_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:09:00 INFO ContextCleaner: Cleaned broadcast 346
15/05/13 11:09:00 INFO BlockManager: Removing broadcast 344
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_344
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_344 of size 20488 dropped from memory (free 250003542)
15/05/13 11:09:00 INFO BlockManager: Removing block broadcast_344_piece0
15/05/13 11:09:00 INFO MemoryStore: Block broadcast_344_piece0 of size 10771 dropped from memory (free 250014313)
15/05/13 11:09:01 INFO BlockManagerInfo: Removed broadcast_344_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 261.9 MB)
15/05/13 11:09:01 INFO BlockManagerMaster: Updated info of block broadcast_344_piece0
15/05/13 11:09:01 INFO BlockManagerInfo: Removed broadcast_344_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.5 KB, free: 263.0 MB)
15/05/13 11:09:01 INFO ContextCleaner: Cleaned broadcast 344
15/05/13 11:09:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:09:01 INFO DAGScheduler: Got job 240 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:09:01 INFO DAGScheduler: Final stage: Stage 235(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:09:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:09:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:09:01 INFO DAGScheduler: Submitting Stage 235 (MapPartitionsRDD[1681] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:09:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=28288243, maxMem=278302556
15/05/13 11:09:01 INFO MemoryStore: Block broadcast_353 stored as values in memory (estimated size 20.0 KB, free 238.4 MB)
15/05/13 11:09:01 INFO MemoryStore: ensureFreeSpace(10764) called with curMem=28308731, maxMem=278302556
15/05/13 11:09:01 INFO MemoryStore: Block broadcast_353_piece0 stored as bytes in memory (estimated size 10.5 KB, free 238.4 MB)
15/05/13 11:09:01 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 261.9 MB)
15/05/13 11:09:01 INFO BlockManagerMaster: Updated info of block broadcast_353_piece0
15/05/13 11:09:01 INFO SparkContext: Created broadcast 353 from broadcast at DAGScheduler.scala:839
15/05/13 11:09:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 235 (MapPartitionsRDD[1681] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:09:01 INFO TaskSchedulerImpl: Adding task set 235.0 with 1 tasks
15/05/13 11:09:01 INFO TaskSetManager: Starting task 0.0 in stage 235.0 (TID 235, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:09:01 INFO BlockManagerInfo: Added broadcast_353_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 264.1 MB)
15/05/13 11:09:01 INFO TaskSetManager: Finished task 0.0 in stage 235.0 (TID 235) in 97 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:09:01 INFO TaskSchedulerImpl: Removed TaskSet 235.0, whose tasks have all completed, from pool 
15/05/13 11:09:01 INFO DAGScheduler: Stage 235 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.099 s
15/05/13 11:09:01 INFO DAGScheduler: Job 240 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.133097 s
15/05/13 11:09:01 INFO JobScheduler: Finished job streaming job 1431529740000 ms.0 from job set of time 1431529740000 ms
15/05/13 11:09:01 INFO JobScheduler: Total delay: 1.219 s for time 1431529740000 ms (execution: 0.998 s)
15/05/13 11:09:01 INFO MapPartitionsRDD: Removing RDD 1656 from persistence list
15/05/13 11:09:01 INFO BlockManager: Removing RDD 1656
15/05/13 11:09:01 INFO UnionRDD: Removing RDD 1655 from persistence list
15/05/13 11:09:01 INFO BlockManager: Removing RDD 1655
15/05/13 11:09:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529680000 ms: 1431529620000 ms
15/05/13 11:09:01 INFO JobGenerator: Checkpointing graph for time 1431529740000 ms
15/05/13 11:09:01 INFO DStreamGraph: Updating checkpoint data for time 1431529740000 ms
15/05/13 11:09:01 INFO DStreamGraph: Updated checkpoint data for time 1431529740000 ms
15/05/13 11:09:01 INFO CheckpointWriter: Saving checkpoint for time 1431529740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000'
15/05/13 11:09:01 INFO CheckpointWriter: Checkpoint for time 1431529740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000', took 7655 bytes and 55 ms
15/05/13 11:09:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529740000 ms
15/05/13 11:09:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529740000 ms
15/05/13 11:09:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:10:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 11:10:00 INFO FileInputDStream: New files at time 1431529800000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529700182.json
15/05/13 11:10:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28319495, maxMem=278302556
15/05/13 11:10:00 INFO MemoryStore: Block broadcast_354 stored as values in memory (estimated size 232.9 KB, free 238.2 MB)
15/05/13 11:10:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=28558027, maxMem=278302556
15/05/13 11:10:00 INFO MemoryStore: Block broadcast_354_piece0 stored as bytes in memory (estimated size 34.9 KB, free 238.1 MB)
15/05/13 11:10:00 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:10:00 INFO BlockManagerMaster: Updated info of block broadcast_354_piece0
15/05/13 11:10:00 INFO SparkContext: Created broadcast 354 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:10:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:10:00 INFO JobScheduler: Added jobs for time 1431529800000 ms
15/05/13 11:10:00 INFO JobGenerator: Checkpointing graph for time 1431529800000 ms
15/05/13 11:10:00 INFO DStreamGraph: Updating checkpoint data for time 1431529800000 ms
15/05/13 11:10:00 INFO JobScheduler: Starting job streaming job 1431529800000 ms.0 from job set of time 1431529800000 ms
15/05/13 11:10:00 INFO DStreamGraph: Updated checkpoint data for time 1431529800000 ms
15/05/13 11:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431529800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000'
15/05/13 11:10:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83858): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431529800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000'
15/05/13 11:10:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:10:00 INFO DAGScheduler: Got job 241 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:10:00 INFO DAGScheduler: Final stage: Stage 236(reduce at JsonRDD.scala:51)
15/05/13 11:10:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:10:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:10:00 INFO DAGScheduler: Submitting Stage 236 (MapPartitionsRDD[1688] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:10:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=28593735, maxMem=278302556
15/05/13 11:10:00 INFO MemoryStore: Block broadcast_355 stored as values in memory (estimated size 5.9 KB, free 238.1 MB)
15/05/13 11:10:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=28599775, maxMem=278302556
15/05/13 11:10:00 INFO MemoryStore: Block broadcast_355_piece0 stored as bytes in memory (estimated size 4.1 KB, free 238.1 MB)
15/05/13 11:10:00 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:10:00 INFO BlockManagerMaster: Updated info of block broadcast_355_piece0
15/05/13 11:10:00 INFO SparkContext: Created broadcast 355 from broadcast at DAGScheduler.scala:839
15/05/13 11:10:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 236 (MapPartitionsRDD[1688] at map at JsonRDD.scala:51)
15/05/13 11:10:00 INFO TaskSchedulerImpl: Adding task set 236.0 with 1 tasks
15/05/13 11:10:00 INFO TaskSetManager: Starting task 0.0 in stage 236.0 (TID 236, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:10:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83860): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:10:00 INFO CheckpointWriter: Saving checkpoint for time 1431529800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000'
15/05/13 11:10:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83862): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:10:00 WARN CheckpointWriter: Could not write checkpoint for time 1431529800000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000'
15/05/13 11:10:00 INFO BlockManagerInfo: Added broadcast_355_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.0 MB)
15/05/13 11:10:00 INFO BlockManagerInfo: Added broadcast_354_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 263.0 MB)
15/05/13 11:10:00 INFO TaskSetManager: Finished task 0.0 in stage 236.0 (TID 236) in 572 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:10:00 INFO TaskSchedulerImpl: Removed TaskSet 236.0, whose tasks have all completed, from pool 
15/05/13 11:10:00 INFO DAGScheduler: Stage 236 (reduce at JsonRDD.scala:51) finished in 0.589 s
15/05/13 11:10:00 INFO DAGScheduler: Job 241 finished: reduce at JsonRDD.scala:51, took 0.618645 s
15/05/13 11:10:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:10:01 INFO DAGScheduler: Got job 242 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:10:01 INFO DAGScheduler: Final stage: Stage 237(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:10:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:10:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:10:01 INFO DAGScheduler: Submitting Stage 237 (MapPartitionsRDD[1695] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:10:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=28604000, maxMem=278302556
15/05/13 11:10:01 INFO MemoryStore: Block broadcast_356 stored as values in memory (estimated size 20.4 KB, free 238.1 MB)
15/05/13 11:10:01 INFO MemoryStore: ensureFreeSpace(10909) called with curMem=28624920, maxMem=278302556
15/05/13 11:10:01 INFO MemoryStore: Block broadcast_356_piece0 stored as bytes in memory (estimated size 10.7 KB, free 238.1 MB)
15/05/13 11:10:01 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.8 MB)
15/05/13 11:10:01 INFO BlockManagerMaster: Updated info of block broadcast_356_piece0
15/05/13 11:10:01 INFO SparkContext: Created broadcast 356 from broadcast at DAGScheduler.scala:839
15/05/13 11:10:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 237 (MapPartitionsRDD[1695] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:10:01 INFO TaskSchedulerImpl: Adding task set 237.0 with 1 tasks
15/05/13 11:10:01 INFO TaskSetManager: Starting task 0.0 in stage 237.0 (TID 237, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:10:01 INFO BlockManagerInfo: Added broadcast_356_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 263.0 MB)
15/05/13 11:10:01 INFO TaskSetManager: Finished task 0.0 in stage 237.0 (TID 237) in 145 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:10:01 INFO TaskSchedulerImpl: Removed TaskSet 237.0, whose tasks have all completed, from pool 
15/05/13 11:10:01 INFO DAGScheduler: Stage 237 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.145 s
15/05/13 11:10:01 INFO DAGScheduler: Job 242 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.170538 s
15/05/13 11:10:01 INFO JobScheduler: Finished job streaming job 1431529800000 ms.0 from job set of time 1431529800000 ms
15/05/13 11:10:01 INFO JobScheduler: Total delay: 1.375 s for time 1431529800000 ms (execution: 1.197 s)
15/05/13 11:10:01 INFO MapPartitionsRDD: Removing RDD 1670 from persistence list
15/05/13 11:10:01 INFO BlockManager: Removing RDD 1670
15/05/13 11:10:01 INFO UnionRDD: Removing RDD 1669 from persistence list
15/05/13 11:10:01 INFO BlockManager: Removing RDD 1669
15/05/13 11:10:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529740000 ms: 1431529680000 ms
15/05/13 11:10:01 INFO JobGenerator: Checkpointing graph for time 1431529800000 ms
15/05/13 11:10:01 INFO DStreamGraph: Updating checkpoint data for time 1431529800000 ms
15/05/13 11:10:01 INFO DStreamGraph: Updated checkpoint data for time 1431529800000 ms
15/05/13 11:10:01 INFO CheckpointWriter: Saving checkpoint for time 1431529800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000'
15/05/13 11:10:01 INFO CheckpointWriter: Checkpoint for time 1431529800000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000', took 7655 bytes and 54 ms
15/05/13 11:10:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529800000 ms
15/05/13 11:10:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529800000 ms
15/05/13 11:10:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:11:00 INFO FileInputDStream: Finding new files took 72 ms
15/05/13 11:11:00 INFO FileInputDStream: New files at time 1431529860000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529760723.json
15/05/13 11:11:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28635829, maxMem=278302556
15/05/13 11:11:00 INFO MemoryStore: Block broadcast_357 stored as values in memory (estimated size 232.9 KB, free 237.9 MB)
15/05/13 11:11:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=28874361, maxMem=278302556
15/05/13 11:11:00 INFO MemoryStore: Block broadcast_357_piece0 stored as bytes in memory (estimated size 34.9 KB, free 237.8 MB)
15/05/13 11:11:00 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:11:00 INFO BlockManagerMaster: Updated info of block broadcast_357_piece0
15/05/13 11:11:00 INFO SparkContext: Created broadcast 357 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:11:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:11:00 INFO JobScheduler: Added jobs for time 1431529860000 ms
15/05/13 11:11:00 INFO JobScheduler: Starting job streaming job 1431529860000 ms.0 from job set of time 1431529860000 ms
15/05/13 11:11:00 INFO JobGenerator: Checkpointing graph for time 1431529860000 ms
15/05/13 11:11:00 INFO DStreamGraph: Updating checkpoint data for time 1431529860000 ms
15/05/13 11:11:00 INFO DStreamGraph: Updated checkpoint data for time 1431529860000 ms
15/05/13 11:11:00 INFO CheckpointWriter: Saving checkpoint for time 1431529860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529860000'
15/05/13 11:11:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:11:00 INFO DAGScheduler: Got job 243 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:11:00 INFO DAGScheduler: Final stage: Stage 238(reduce at JsonRDD.scala:51)
15/05/13 11:11:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:11:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:11:00 INFO DAGScheduler: Submitting Stage 238 (MapPartitionsRDD[1702] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:11:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=28910069, maxMem=278302556
15/05/13 11:11:00 INFO MemoryStore: Block broadcast_358 stored as values in memory (estimated size 5.9 KB, free 237.8 MB)
15/05/13 11:11:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=28916109, maxMem=278302556
15/05/13 11:11:00 INFO MemoryStore: Block broadcast_358_piece0 stored as bytes in memory (estimated size 4.1 KB, free 237.8 MB)
15/05/13 11:11:00 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:11:00 INFO BlockManagerMaster: Updated info of block broadcast_358_piece0
15/05/13 11:11:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529560000
15/05/13 11:11:00 INFO SparkContext: Created broadcast 358 from broadcast at DAGScheduler.scala:839
15/05/13 11:11:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 238 (MapPartitionsRDD[1702] at map at JsonRDD.scala:51)
15/05/13 11:11:00 INFO TaskSchedulerImpl: Adding task set 238.0 with 1 tasks
15/05/13 11:11:00 INFO CheckpointWriter: Checkpoint for time 1431529860000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529860000', took 7665 bytes and 98 ms
15/05/13 11:11:00 INFO TaskSetManager: Starting task 0.0 in stage 238.0 (TID 238, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:11:00 INFO BlockManagerInfo: Added broadcast_358_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.0 MB)
15/05/13 11:11:00 INFO BlockManagerInfo: Added broadcast_357_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:11:00 INFO TaskSetManager: Finished task 0.0 in stage 238.0 (TID 238) in 464 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:11:00 INFO TaskSchedulerImpl: Removed TaskSet 238.0, whose tasks have all completed, from pool 
15/05/13 11:11:00 INFO DAGScheduler: Stage 238 (reduce at JsonRDD.scala:51) finished in 0.477 s
15/05/13 11:11:00 INFO DAGScheduler: Job 243 finished: reduce at JsonRDD.scala:51, took 0.514578 s
15/05/13 11:11:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:11:01 INFO DAGScheduler: Got job 244 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:11:01 INFO DAGScheduler: Final stage: Stage 239(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:11:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:11:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:11:01 INFO DAGScheduler: Submitting Stage 239 (MapPartitionsRDD[1709] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:11:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=28920335, maxMem=278302556
15/05/13 11:11:01 INFO MemoryStore: Block broadcast_359 stored as values in memory (estimated size 21.0 KB, free 237.8 MB)
15/05/13 11:11:01 INFO MemoryStore: ensureFreeSpace(11137) called with curMem=28941815, maxMem=278302556
15/05/13 11:11:01 INFO MemoryStore: Block broadcast_359_piece0 stored as bytes in memory (estimated size 10.9 KB, free 237.8 MB)
15/05/13 11:11:01 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.8 MB)
15/05/13 11:11:01 INFO BlockManagerMaster: Updated info of block broadcast_359_piece0
15/05/13 11:11:01 INFO SparkContext: Created broadcast 359 from broadcast at DAGScheduler.scala:839
15/05/13 11:11:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 239 (MapPartitionsRDD[1709] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:11:01 INFO TaskSchedulerImpl: Adding task set 239.0 with 1 tasks
15/05/13 11:11:01 INFO TaskSetManager: Starting task 0.0 in stage 239.0 (TID 239, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:11:01 INFO BlockManagerInfo: Added broadcast_359_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:11:01 INFO TaskSetManager: Finished task 0.0 in stage 239.0 (TID 239) in 190 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:11:01 INFO TaskSchedulerImpl: Removed TaskSet 239.0, whose tasks have all completed, from pool 
15/05/13 11:11:01 INFO DAGScheduler: Stage 239 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.192 s
15/05/13 11:11:01 INFO DAGScheduler: Job 244 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.231053 s
15/05/13 11:11:01 INFO JobScheduler: Finished job streaming job 1431529860000 ms.0 from job set of time 1431529860000 ms
15/05/13 11:11:01 INFO JobScheduler: Total delay: 1.342 s for time 1431529860000 ms (execution: 1.118 s)
15/05/13 11:11:01 INFO MapPartitionsRDD: Removing RDD 1684 from persistence list
15/05/13 11:11:01 INFO BlockManager: Removing RDD 1684
15/05/13 11:11:01 INFO UnionRDD: Removing RDD 1683 from persistence list
15/05/13 11:11:01 INFO BlockManager: Removing RDD 1683
15/05/13 11:11:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529800000 ms: 1431529740000 ms
15/05/13 11:11:01 INFO JobGenerator: Checkpointing graph for time 1431529860000 ms
15/05/13 11:11:01 INFO DStreamGraph: Updating checkpoint data for time 1431529860000 ms
15/05/13 11:11:01 INFO DStreamGraph: Updated checkpoint data for time 1431529860000 ms
15/05/13 11:11:01 INFO CheckpointWriter: Saving checkpoint for time 1431529860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529860000'
15/05/13 11:11:01 INFO CheckpointWriter: Checkpoint for time 1431529860000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529860000', took 7656 bytes and 62 ms
15/05/13 11:11:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529860000 ms
15/05/13 11:11:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529860000 ms
15/05/13 11:11:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:12:00 INFO FileInputDStream: Finding new files took 60 ms
15/05/13 11:12:00 INFO FileInputDStream: New files at time 1431529920000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529821924.json
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28952952, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_360 stored as values in memory (estimated size 232.9 KB, free 237.6 MB)
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=29191484, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_360_piece0 stored as bytes in memory (estimated size 34.9 KB, free 237.5 MB)
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.7 MB)
15/05/13 11:12:00 INFO BlockManagerMaster: Updated info of block broadcast_360_piece0
15/05/13 11:12:00 INFO SparkContext: Created broadcast 360 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:12:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:12:00 INFO JobScheduler: Starting job streaming job 1431529920000 ms.0 from job set of time 1431529920000 ms
15/05/13 11:12:00 INFO JobScheduler: Added jobs for time 1431529920000 ms
15/05/13 11:12:00 INFO JobGenerator: Checkpointing graph for time 1431529920000 ms
15/05/13 11:12:00 INFO DStreamGraph: Updating checkpoint data for time 1431529920000 ms
15/05/13 11:12:00 INFO DStreamGraph: Updated checkpoint data for time 1431529920000 ms
15/05/13 11:12:00 INFO CheckpointWriter: Saving checkpoint for time 1431529920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529920000'
15/05/13 11:12:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:12:00 INFO DAGScheduler: Got job 245 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:12:00 INFO DAGScheduler: Final stage: Stage 240(reduce at JsonRDD.scala:51)
15/05/13 11:12:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:12:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:12:00 INFO DAGScheduler: Submitting Stage 240 (MapPartitionsRDD[1716] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=29227192, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_361 stored as values in memory (estimated size 5.9 KB, free 237.5 MB)
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(4233) called with curMem=29233232, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_361_piece0 stored as bytes in memory (estimated size 4.1 KB, free 237.5 MB)
15/05/13 11:12:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529620000
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:12:00 INFO BlockManagerMaster: Updated info of block broadcast_361_piece0
15/05/13 11:12:00 INFO SparkContext: Created broadcast 361 from broadcast at DAGScheduler.scala:839
15/05/13 11:12:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 240 (MapPartitionsRDD[1716] at map at JsonRDD.scala:51)
15/05/13 11:12:00 INFO TaskSchedulerImpl: Adding task set 240.0 with 1 tasks
15/05/13 11:12:00 INFO CheckpointWriter: Checkpoint for time 1431529920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529920000', took 7662 bytes and 108 ms
15/05/13 11:12:00 INFO TaskSetManager: Starting task 0.0 in stage 240.0 (TID 240, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_361_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.1 MB)
15/05/13 11:12:00 INFO TaskSetManager: Finished task 0.0 in stage 240.0 (TID 240) in 305 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:12:00 INFO TaskSchedulerImpl: Removed TaskSet 240.0, whose tasks have all completed, from pool 
15/05/13 11:12:00 INFO DAGScheduler: Stage 240 (reduce at JsonRDD.scala:51) finished in 0.320 s
15/05/13 11:12:00 INFO DAGScheduler: Job 245 finished: reduce at JsonRDD.scala:51, took 0.353216 s
15/05/13 11:12:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:12:00 INFO DAGScheduler: Got job 246 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:12:00 INFO DAGScheduler: Final stage: Stage 241(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:12:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:12:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:12:00 INFO DAGScheduler: Submitting Stage 241 (MapPartitionsRDD[1723] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=29237465, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_362 stored as values in memory (estimated size 20.8 KB, free 237.5 MB)
15/05/13 11:12:00 INFO MemoryStore: ensureFreeSpace(11124) called with curMem=29258769, maxMem=278302556
15/05/13 11:12:00 INFO MemoryStore: Block broadcast_362_piece0 stored as bytes in memory (estimated size 10.9 KB, free 237.5 MB)
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.7 MB)
15/05/13 11:12:00 INFO BlockManagerMaster: Updated info of block broadcast_362_piece0
15/05/13 11:12:00 INFO SparkContext: Created broadcast 362 from broadcast at DAGScheduler.scala:839
15/05/13 11:12:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 241 (MapPartitionsRDD[1723] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:12:00 INFO TaskSchedulerImpl: Adding task set 241.0 with 1 tasks
15/05/13 11:12:00 INFO TaskSetManager: Starting task 0.0 in stage 241.0 (TID 241, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:12:00 INFO BlockManagerInfo: Added broadcast_362_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:12:01 INFO BlockManagerInfo: Added broadcast_360_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:12:01 INFO TaskSetManager: Finished task 0.0 in stage 241.0 (TID 241) in 250 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:12:01 INFO TaskSchedulerImpl: Removed TaskSet 241.0, whose tasks have all completed, from pool 
15/05/13 11:12:01 INFO DAGScheduler: Stage 241 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.252 s
15/05/13 11:12:01 INFO DAGScheduler: Job 246 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.294038 s
15/05/13 11:12:01 INFO JobScheduler: Finished job streaming job 1431529920000 ms.0 from job set of time 1431529920000 ms
15/05/13 11:12:01 INFO JobScheduler: Total delay: 1.195 s for time 1431529920000 ms (execution: 0.991 s)
15/05/13 11:12:01 INFO MapPartitionsRDD: Removing RDD 1698 from persistence list
15/05/13 11:12:01 INFO BlockManager: Removing RDD 1698
15/05/13 11:12:01 INFO UnionRDD: Removing RDD 1697 from persistence list
15/05/13 11:12:01 INFO BlockManager: Removing RDD 1697
15/05/13 11:12:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529860000 ms: 1431529800000 ms
15/05/13 11:12:01 INFO JobGenerator: Checkpointing graph for time 1431529920000 ms
15/05/13 11:12:01 INFO DStreamGraph: Updating checkpoint data for time 1431529920000 ms
15/05/13 11:12:01 INFO DStreamGraph: Updated checkpoint data for time 1431529920000 ms
15/05/13 11:12:01 INFO CheckpointWriter: Saving checkpoint for time 1431529920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529920000'
15/05/13 11:12:01 INFO CheckpointWriter: Checkpoint for time 1431529920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529920000', took 7655 bytes and 60 ms
15/05/13 11:12:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529920000 ms
15/05/13 11:12:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529920000 ms
15/05/13 11:12:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 353
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_353
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_353 of size 20488 dropped from memory (free 249053151)
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_353_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_353_piece0 of size 10764 dropped from memory (free 249063915)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_353_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 261.7 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_353_piece0
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_353_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 264.1 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 353
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 357
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_357_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_357_piece0 of size 35708 dropped from memory (free 249099623)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_357_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_357_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_357
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_357 of size 238532 dropped from memory (free 249338155)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_357_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 357
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 356
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_356_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_356_piece0 of size 10909 dropped from memory (free 249349064)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_356_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_356_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_356
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_356 of size 20920 dropped from memory (free 249369984)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_356_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 262.9 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 356
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 355
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_355
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_355 of size 6040 dropped from memory (free 249376024)
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_355_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_355_piece0 of size 4225 dropped from memory (free 249380249)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_355_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_355_piece0
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_355_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.9 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 355
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 362
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_362_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_362_piece0 of size 11124 dropped from memory (free 249391373)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_362_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_362_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_362
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_362 of size 21304 dropped from memory (free 249412677)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_362_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 362
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 361
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_361_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_361_piece0 of size 4233 dropped from memory (free 249416910)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_361_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_361_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_361
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_361 of size 6040 dropped from memory (free 249422950)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_361_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 361
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 359
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_359_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_359_piece0 of size 11137 dropped from memory (free 249434087)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_359_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_359_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_359
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_359 of size 21480 dropped from memory (free 249455567)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_359_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 263.0 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 359
15/05/13 11:12:46 INFO BlockManager: Removing broadcast 358
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_358_piece0
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_358_piece0 of size 4226 dropped from memory (free 249459793)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_358_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:12:46 INFO BlockManagerMaster: Updated info of block broadcast_358_piece0
15/05/13 11:12:46 INFO BlockManager: Removing block broadcast_358
15/05/13 11:12:46 INFO MemoryStore: Block broadcast_358 of size 6040 dropped from memory (free 249465833)
15/05/13 11:12:46 INFO BlockManagerInfo: Removed broadcast_358_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 263.0 MB)
15/05/13 11:12:46 INFO ContextCleaner: Cleaned broadcast 358
15/05/13 11:13:00 INFO FileInputDStream: Finding new files took 58 ms
15/05/13 11:13:00 INFO FileInputDStream: New files at time 1431529980000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529883146.json
15/05/13 11:13:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=28836723, maxMem=278302556
15/05/13 11:13:00 INFO MemoryStore: Block broadcast_363 stored as values in memory (estimated size 232.9 KB, free 237.7 MB)
15/05/13 11:13:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=29075255, maxMem=278302556
15/05/13 11:13:00 INFO MemoryStore: Block broadcast_363_piece0 stored as bytes in memory (estimated size 34.9 KB, free 237.6 MB)
15/05/13 11:13:00 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.8 MB)
15/05/13 11:13:00 INFO BlockManagerMaster: Updated info of block broadcast_363_piece0
15/05/13 11:13:00 INFO SparkContext: Created broadcast 363 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:13:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:13:00 INFO JobScheduler: Added jobs for time 1431529980000 ms
15/05/13 11:13:00 INFO JobGenerator: Checkpointing graph for time 1431529980000 ms
15/05/13 11:13:00 INFO DStreamGraph: Updating checkpoint data for time 1431529980000 ms
15/05/13 11:13:00 INFO JobScheduler: Starting job streaming job 1431529980000 ms.0 from job set of time 1431529980000 ms
15/05/13 11:13:00 INFO DStreamGraph: Updated checkpoint data for time 1431529980000 ms
15/05/13 11:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431529980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000'
15/05/13 11:13:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83885): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:13:00 INFO CheckpointWriter: Saving checkpoint for time 1431529980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000'
15/05/13 11:13:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:13:00 INFO DAGScheduler: Got job 247 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:13:00 INFO DAGScheduler: Final stage: Stage 242(reduce at JsonRDD.scala:51)
15/05/13 11:13:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:13:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:13:00 INFO DAGScheduler: Submitting Stage 242 (MapPartitionsRDD[1730] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:13:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=29110963, maxMem=278302556
15/05/13 11:13:00 INFO MemoryStore: Block broadcast_364 stored as values in memory (estimated size 5.9 KB, free 237.6 MB)
15/05/13 11:13:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=29117003, maxMem=278302556
15/05/13 11:13:00 INFO MemoryStore: Block broadcast_364_piece0 stored as bytes in memory (estimated size 4.1 KB, free 237.6 MB)
15/05/13 11:13:00 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.8 MB)
15/05/13 11:13:00 INFO BlockManagerMaster: Updated info of block broadcast_364_piece0
15/05/13 11:13:00 INFO SparkContext: Created broadcast 364 from broadcast at DAGScheduler.scala:839
15/05/13 11:13:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 242 (MapPartitionsRDD[1730] at map at JsonRDD.scala:51)
15/05/13 11:13:00 INFO TaskSchedulerImpl: Adding task set 242.0 with 1 tasks
15/05/13 11:13:00 INFO TaskSetManager: Starting task 0.0 in stage 242.0 (TID 242, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:13:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529680000
15/05/13 11:13:00 INFO CheckpointWriter: Checkpoint for time 1431529980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000', took 7666 bytes and 172 ms
15/05/13 11:13:00 INFO BlockManagerInfo: Added broadcast_364_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 263.0 MB)
15/05/13 11:13:00 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:13:00 INFO TaskSetManager: Finished task 0.0 in stage 242.0 (TID 242) in 411 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:13:00 INFO TaskSchedulerImpl: Removed TaskSet 242.0, whose tasks have all completed, from pool 
15/05/13 11:13:00 INFO DAGScheduler: Stage 242 (reduce at JsonRDD.scala:51) finished in 0.426 s
15/05/13 11:13:00 INFO DAGScheduler: Job 247 finished: reduce at JsonRDD.scala:51, took 0.474907 s
15/05/13 11:13:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:13:01 INFO DAGScheduler: Got job 248 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:13:01 INFO DAGScheduler: Final stage: Stage 243(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:13:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:13:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:13:01 INFO DAGScheduler: Submitting Stage 243 (MapPartitionsRDD[1737] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:13:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=29121228, maxMem=278302556
15/05/13 11:13:01 INFO MemoryStore: Block broadcast_365 stored as values in memory (estimated size 20.8 KB, free 237.6 MB)
15/05/13 11:13:01 INFO MemoryStore: ensureFreeSpace(11117) called with curMem=29142532, maxMem=278302556
15/05/13 11:13:01 INFO MemoryStore: Block broadcast_365_piece0 stored as bytes in memory (estimated size 10.9 KB, free 237.6 MB)
15/05/13 11:13:01 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.8 MB)
15/05/13 11:13:01 INFO BlockManagerMaster: Updated info of block broadcast_365_piece0
15/05/13 11:13:01 INFO SparkContext: Created broadcast 365 from broadcast at DAGScheduler.scala:839
15/05/13 11:13:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 243 (MapPartitionsRDD[1737] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:13:01 INFO TaskSchedulerImpl: Adding task set 243.0 with 1 tasks
15/05/13 11:13:01 INFO TaskSetManager: Starting task 0.0 in stage 243.0 (TID 243, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:13:01 INFO BlockManagerInfo: Added broadcast_365_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 264.1 MB)
15/05/13 11:13:01 INFO BlockManagerInfo: Added broadcast_363_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:13:01 INFO TaskSetManager: Finished task 0.0 in stage 243.0 (TID 243) in 176 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:13:01 INFO TaskSchedulerImpl: Removed TaskSet 243.0, whose tasks have all completed, from pool 
15/05/13 11:13:01 INFO DAGScheduler: Stage 243 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.178 s
15/05/13 11:13:01 INFO DAGScheduler: Job 248 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.211056 s
15/05/13 11:13:01 INFO JobScheduler: Finished job streaming job 1431529980000 ms.0 from job set of time 1431529980000 ms
15/05/13 11:13:01 INFO JobScheduler: Total delay: 1.281 s for time 1431529980000 ms (execution: 1.092 s)
15/05/13 11:13:01 INFO MapPartitionsRDD: Removing RDD 1712 from persistence list
15/05/13 11:13:01 INFO BlockManager: Removing RDD 1712
15/05/13 11:13:01 INFO UnionRDD: Removing RDD 1711 from persistence list
15/05/13 11:13:01 INFO BlockManager: Removing RDD 1711
15/05/13 11:13:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529920000 ms: 1431529860000 ms
15/05/13 11:13:01 INFO JobGenerator: Checkpointing graph for time 1431529980000 ms
15/05/13 11:13:01 INFO DStreamGraph: Updating checkpoint data for time 1431529980000 ms
15/05/13 11:13:01 INFO DStreamGraph: Updated checkpoint data for time 1431529980000 ms
15/05/13 11:13:01 INFO CheckpointWriter: Saving checkpoint for time 1431529980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000'
15/05/13 11:13:01 INFO CheckpointWriter: Checkpoint for time 1431529980000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000', took 7656 bytes and 59 ms
15/05/13 11:13:01 INFO DStreamGraph: Clearing checkpoint data for time 1431529980000 ms
15/05/13 11:13:01 INFO DStreamGraph: Cleared checkpoint data for time 1431529980000 ms
15/05/13 11:13:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:14:00 INFO FileInputDStream: Finding new files took 35 ms
15/05/13 11:14:00 INFO FileInputDStream: New files at time 1431530040000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431529945181.json
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=29153649, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_366 stored as values in memory (estimated size 232.9 KB, free 237.4 MB)
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=29392181, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_366_piece0 stored as bytes in memory (estimated size 34.9 KB, free 237.3 MB)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.7 MB)
15/05/13 11:14:00 INFO BlockManagerMaster: Updated info of block broadcast_366_piece0
15/05/13 11:14:00 INFO SparkContext: Created broadcast 366 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:14:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:14:00 INFO JobScheduler: Added jobs for time 1431530040000 ms
15/05/13 11:14:00 INFO JobScheduler: Starting job streaming job 1431530040000 ms.0 from job set of time 1431530040000 ms
15/05/13 11:14:00 INFO JobGenerator: Checkpointing graph for time 1431530040000 ms
15/05/13 11:14:00 INFO DStreamGraph: Updating checkpoint data for time 1431530040000 ms
15/05/13 11:14:00 INFO DStreamGraph: Updated checkpoint data for time 1431530040000 ms
15/05/13 11:14:00 INFO CheckpointWriter: Saving checkpoint for time 1431530040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000'
15/05/13 11:14:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83891): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:14:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83891): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:14:00 INFO CheckpointWriter: Saving checkpoint for time 1431530040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000'
15/05/13 11:14:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:14:00 INFO DAGScheduler: Got job 249 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:14:00 INFO DAGScheduler: Final stage: Stage 244(reduce at JsonRDD.scala:51)
15/05/13 11:14:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:14:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:14:00 INFO DAGScheduler: Submitting Stage 244 (MapPartitionsRDD[1744] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=29427889, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_367 stored as values in memory (estimated size 5.9 KB, free 237.3 MB)
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=29433929, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_367_piece0 stored as bytes in memory (estimated size 4.1 KB, free 237.3 MB)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:14:00 INFO BlockManagerMaster: Updated info of block broadcast_367_piece0
15/05/13 11:14:00 INFO SparkContext: Created broadcast 367 from broadcast at DAGScheduler.scala:839
15/05/13 11:14:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 244 (MapPartitionsRDD[1744] at map at JsonRDD.scala:51)
15/05/13 11:14:00 INFO TaskSchedulerImpl: Adding task set 244.0 with 1 tasks
15/05/13 11:14:00 INFO TaskSetManager: Starting task 0.0 in stage 244.0 (TID 244, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_367_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:14:00 INFO TaskSetManager: Finished task 0.0 in stage 244.0 (TID 244) in 248 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:14:00 INFO TaskSchedulerImpl: Removed TaskSet 244.0, whose tasks have all completed, from pool 
15/05/13 11:14:00 INFO DAGScheduler: Stage 244 (reduce at JsonRDD.scala:51) finished in 0.263 s
15/05/13 11:14:00 INFO DAGScheduler: Job 249 finished: reduce at JsonRDD.scala:51, took 0.296866 s
15/05/13 11:14:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83893): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:14:00 INFO CheckpointWriter: Saving checkpoint for time 1431530040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000'
15/05/13 11:14:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:14:00 INFO DAGScheduler: Got job 250 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:14:00 INFO DAGScheduler: Final stage: Stage 245(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:14:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:14:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:14:00 INFO DAGScheduler: Submitting Stage 245 (MapPartitionsRDD[1751] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=29438154, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_368 stored as values in memory (estimated size 21.0 KB, free 237.3 MB)
15/05/13 11:14:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529740000
15/05/13 11:14:00 INFO CheckpointWriter: Checkpoint for time 1431530040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000', took 7666 bytes and 576 ms
15/05/13 11:14:00 INFO MemoryStore: ensureFreeSpace(11138) called with curMem=29459634, maxMem=278302556
15/05/13 11:14:00 INFO MemoryStore: Block broadcast_368_piece0 stored as bytes in memory (estimated size 10.9 KB, free 237.3 MB)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.7 MB)
15/05/13 11:14:00 INFO BlockManagerMaster: Updated info of block broadcast_368_piece0
15/05/13 11:14:00 INFO SparkContext: Created broadcast 368 from broadcast at DAGScheduler.scala:839
15/05/13 11:14:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 245 (MapPartitionsRDD[1751] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:14:00 INFO TaskSchedulerImpl: Adding task set 245.0 with 1 tasks
15/05/13 11:14:00 INFO TaskSetManager: Starting task 0.0 in stage 245.0 (TID 245, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_368_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:14:00 INFO BlockManagerInfo: Added broadcast_366_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:14:01 INFO TaskSetManager: Finished task 0.0 in stage 245.0 (TID 245) in 353 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:14:01 INFO TaskSchedulerImpl: Removed TaskSet 245.0, whose tasks have all completed, from pool 
15/05/13 11:14:01 INFO DAGScheduler: Stage 245 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.354 s
15/05/13 11:14:01 INFO DAGScheduler: Job 250 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.406052 s
15/05/13 11:14:01 INFO JobScheduler: Finished job streaming job 1431530040000 ms.0 from job set of time 1431530040000 ms
15/05/13 11:14:01 INFO JobScheduler: Total delay: 1.132 s for time 1431530040000 ms (execution: 1.018 s)
15/05/13 11:14:01 INFO MapPartitionsRDD: Removing RDD 1726 from persistence list
15/05/13 11:14:01 INFO BlockManager: Removing RDD 1726
15/05/13 11:14:01 INFO UnionRDD: Removing RDD 1725 from persistence list
15/05/13 11:14:01 INFO BlockManager: Removing RDD 1725
15/05/13 11:14:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431529980000 ms: 1431529920000 ms
15/05/13 11:14:01 INFO JobGenerator: Checkpointing graph for time 1431530040000 ms
15/05/13 11:14:01 INFO DStreamGraph: Updating checkpoint data for time 1431530040000 ms
15/05/13 11:14:01 INFO DStreamGraph: Updated checkpoint data for time 1431530040000 ms
15/05/13 11:14:01 INFO CheckpointWriter: Saving checkpoint for time 1431530040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000'
15/05/13 11:14:01 INFO CheckpointWriter: Checkpoint for time 1431530040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000', took 7655 bytes and 62 ms
15/05/13 11:14:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530040000 ms
15/05/13 11:14:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530040000 ms
15/05/13 11:14:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:15:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 11:15:00 INFO FileInputDStream: New files at time 1431530100000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530005676.json
15/05/13 11:15:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=29470772, maxMem=278302556
15/05/13 11:15:00 INFO MemoryStore: Block broadcast_369 stored as values in memory (estimated size 232.9 KB, free 237.1 MB)
15/05/13 11:15:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=29709304, maxMem=278302556
15/05/13 11:15:00 INFO MemoryStore: Block broadcast_369_piece0 stored as bytes in memory (estimated size 34.9 KB, free 237.0 MB)
15/05/13 11:15:00 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.7 MB)
15/05/13 11:15:00 INFO BlockManagerMaster: Updated info of block broadcast_369_piece0
15/05/13 11:15:00 INFO SparkContext: Created broadcast 369 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:15:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:15:00 INFO JobScheduler: Starting job streaming job 1431530100000 ms.0 from job set of time 1431530100000 ms
15/05/13 11:15:00 INFO JobScheduler: Added jobs for time 1431530100000 ms
15/05/13 11:15:00 INFO JobGenerator: Checkpointing graph for time 1431530100000 ms
15/05/13 11:15:00 INFO DStreamGraph: Updating checkpoint data for time 1431530100000 ms
15/05/13 11:15:00 INFO DStreamGraph: Updated checkpoint data for time 1431530100000 ms
15/05/13 11:15:00 INFO CheckpointWriter: Saving checkpoint for time 1431530100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530100000'
15/05/13 11:15:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529800000
15/05/13 11:15:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:15:00 INFO DAGScheduler: Got job 251 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:15:00 INFO DAGScheduler: Final stage: Stage 246(reduce at JsonRDD.scala:51)
15/05/13 11:15:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:15:00 INFO CheckpointWriter: Checkpoint for time 1431530100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530100000', took 7663 bytes and 88 ms
15/05/13 11:15:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:15:00 INFO DAGScheduler: Submitting Stage 246 (MapPartitionsRDD[1758] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:15:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=29745013, maxMem=278302556
15/05/13 11:15:00 INFO MemoryStore: Block broadcast_370 stored as values in memory (estimated size 5.9 KB, free 237.0 MB)
15/05/13 11:15:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=29751053, maxMem=278302556
15/05/13 11:15:00 INFO MemoryStore: Block broadcast_370_piece0 stored as bytes in memory (estimated size 4.1 KB, free 237.0 MB)
15/05/13 11:15:00 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:15:00 INFO BlockManagerMaster: Updated info of block broadcast_370_piece0
15/05/13 11:15:00 INFO SparkContext: Created broadcast 370 from broadcast at DAGScheduler.scala:839
15/05/13 11:15:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 246 (MapPartitionsRDD[1758] at map at JsonRDD.scala:51)
15/05/13 11:15:00 INFO TaskSchedulerImpl: Adding task set 246.0 with 1 tasks
15/05/13 11:15:00 INFO TaskSetManager: Starting task 0.0 in stage 246.0 (TID 246, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:15:00 INFO BlockManagerInfo: Added broadcast_370_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.1 MB)
15/05/13 11:15:00 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:15:00 INFO TaskSetManager: Finished task 0.0 in stage 246.0 (TID 246) in 491 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:15:00 INFO TaskSchedulerImpl: Removed TaskSet 246.0, whose tasks have all completed, from pool 
15/05/13 11:15:00 INFO DAGScheduler: Stage 246 (reduce at JsonRDD.scala:51) finished in 0.502 s
15/05/13 11:15:00 INFO DAGScheduler: Job 251 finished: reduce at JsonRDD.scala:51, took 0.555122 s
15/05/13 11:15:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:15:01 INFO DAGScheduler: Got job 252 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:15:01 INFO DAGScheduler: Final stage: Stage 247(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:15:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:15:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:15:01 INFO DAGScheduler: Submitting Stage 247 (MapPartitionsRDD[1765] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:15:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=29755278, maxMem=278302556
15/05/13 11:15:01 INFO MemoryStore: Block broadcast_371 stored as values in memory (estimated size 20.8 KB, free 237.0 MB)
15/05/13 11:15:01 INFO MemoryStore: ensureFreeSpace(11109) called with curMem=29776582, maxMem=278302556
15/05/13 11:15:01 INFO MemoryStore: Block broadcast_371_piece0 stored as bytes in memory (estimated size 10.8 KB, free 237.0 MB)
15/05/13 11:15:01 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 261.7 MB)
15/05/13 11:15:01 INFO BlockManagerMaster: Updated info of block broadcast_371_piece0
15/05/13 11:15:01 INFO SparkContext: Created broadcast 371 from broadcast at DAGScheduler.scala:839
15/05/13 11:15:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 247 (MapPartitionsRDD[1765] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:15:01 INFO TaskSchedulerImpl: Adding task set 247.0 with 1 tasks
15/05/13 11:15:01 INFO TaskSetManager: Starting task 0.0 in stage 247.0 (TID 247, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:15:01 INFO BlockManagerInfo: Added broadcast_371_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 262.9 MB)
15/05/13 11:15:01 INFO BlockManagerInfo: Added broadcast_369_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:15:01 INFO TaskSetManager: Finished task 0.0 in stage 247.0 (TID 247) in 266 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:15:01 INFO TaskSchedulerImpl: Removed TaskSet 247.0, whose tasks have all completed, from pool 
15/05/13 11:15:01 INFO DAGScheduler: Stage 247 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.267 s
15/05/13 11:15:01 INFO DAGScheduler: Job 252 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.299990 s
15/05/13 11:15:01 INFO JobScheduler: Finished job streaming job 1431530100000 ms.0 from job set of time 1431530100000 ms
15/05/13 11:15:01 INFO JobScheduler: Total delay: 1.441 s for time 1431530100000 ms (execution: 1.226 s)
15/05/13 11:15:01 INFO MapPartitionsRDD: Removing RDD 1740 from persistence list
15/05/13 11:15:01 INFO BlockManager: Removing RDD 1740
15/05/13 11:15:01 INFO UnionRDD: Removing RDD 1739 from persistence list
15/05/13 11:15:01 INFO BlockManager: Removing RDD 1739
15/05/13 11:15:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530040000 ms: 1431529980000 ms
15/05/13 11:15:01 INFO JobGenerator: Checkpointing graph for time 1431530100000 ms
15/05/13 11:15:01 INFO DStreamGraph: Updating checkpoint data for time 1431530100000 ms
15/05/13 11:15:01 INFO DStreamGraph: Updated checkpoint data for time 1431530100000 ms
15/05/13 11:15:01 INFO CheckpointWriter: Saving checkpoint for time 1431530100000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530100000'
15/05/13 11:15:01 INFO CheckpointWriter: Checkpoint for time 1431530100000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530100000', took 7657 bytes and 58 ms
15/05/13 11:15:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530100000 ms
15/05/13 11:15:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530100000 ms
15/05/13 11:15:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:16:00 INFO FileInputDStream: Finding new files took 65 ms
15/05/13 11:16:00 INFO FileInputDStream: New files at time 1431530160000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530067451.json
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=29787691, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_372 stored as values in memory (estimated size 232.9 KB, free 236.8 MB)
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=30026223, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_372_piece0 stored as bytes in memory (estimated size 34.9 KB, free 236.7 MB)
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.6 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_372_piece0
15/05/13 11:16:00 INFO SparkContext: Created broadcast 372 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:16:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:16:00 INFO JobScheduler: Starting job streaming job 1431530160000 ms.0 from job set of time 1431530160000 ms
15/05/13 11:16:00 INFO JobScheduler: Added jobs for time 1431530160000 ms
15/05/13 11:16:00 INFO JobGenerator: Checkpointing graph for time 1431530160000 ms
15/05/13 11:16:00 INFO DStreamGraph: Updating checkpoint data for time 1431530160000 ms
15/05/13 11:16:00 INFO DStreamGraph: Updated checkpoint data for time 1431530160000 ms
15/05/13 11:16:00 INFO CheckpointWriter: Saving checkpoint for time 1431530160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000'
15/05/13 11:16:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83907): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:16:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83907): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:16:00 INFO CheckpointWriter: Saving checkpoint for time 1431530160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000'
15/05/13 11:16:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83909): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:16:00 INFO CheckpointWriter: Saving checkpoint for time 1431530160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000'
15/05/13 11:16:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:16:00 INFO DAGScheduler: Got job 253 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:16:00 INFO DAGScheduler: Final stage: Stage 248(reduce at JsonRDD.scala:51)
15/05/13 11:16:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:16:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:16:00 INFO DAGScheduler: Submitting Stage 248 (MapPartitionsRDD[1772] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=30061931, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_373 stored as values in memory (estimated size 5.9 KB, free 236.7 MB)
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=30067971, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_373_piece0 stored as bytes in memory (estimated size 4.1 KB, free 236.7 MB)
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_373_piece0
15/05/13 11:16:00 INFO SparkContext: Created broadcast 373 from broadcast at DAGScheduler.scala:839
15/05/13 11:16:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 248 (MapPartitionsRDD[1772] at map at JsonRDD.scala:51)
15/05/13 11:16:00 INFO TaskSchedulerImpl: Adding task set 248.0 with 1 tasks
15/05/13 11:16:00 INFO TaskSetManager: Starting task 0.0 in stage 248.0 (TID 248, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:16:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529860000.bk
15/05/13 11:16:00 INFO CheckpointWriter: Checkpoint for time 1431530160000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000', took 7663 bytes and 166 ms
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_373_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_372_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO TaskSetManager: Finished task 0.0 in stage 248.0 (TID 248) in 251 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:16:00 INFO TaskSchedulerImpl: Removed TaskSet 248.0, whose tasks have all completed, from pool 
15/05/13 11:16:00 INFO DAGScheduler: Stage 248 (reduce at JsonRDD.scala:51) finished in 0.262 s
15/05/13 11:16:00 INFO DAGScheduler: Job 253 finished: reduce at JsonRDD.scala:51, took 0.296438 s
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 365
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_365
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_365 of size 21304 dropped from memory (free 248251664)
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_365_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_365_piece0 of size 11117 dropped from memory (free 248262781)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_365_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.6 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_365_piece0
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_365_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 365
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 364
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_364
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_364 of size 6040 dropped from memory (free 248268821)
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_364_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_364_piece0 of size 4225 dropped from memory (free 248273046)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_364_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_364_piece0
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_364_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.8 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 364
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 363
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_363_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_363_piece0 of size 35708 dropped from memory (free 248308754)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_363_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_363_piece0
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_363
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_363 of size 238532 dropped from memory (free 248547286)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_363_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_363_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.9 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 363
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 368
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_368
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_368 of size 21480 dropped from memory (free 248568766)
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_368_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_368_piece0 of size 11138 dropped from memory (free 248579904)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_368_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_368_piece0
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_368_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 368
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 367
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_367
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_367 of size 6040 dropped from memory (free 248585944)
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_367_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_367_piece0 of size 4225 dropped from memory (free 248590169)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_367_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_367_piece0
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_367_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 367
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 373
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_373_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_373_piece0 of size 4225 dropped from memory (free 248594394)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_373_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_373_piece0
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_373
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_373 of size 6040 dropped from memory (free 248600434)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_373_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 373
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 371
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_371
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_371 of size 21304 dropped from memory (free 248621738)
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_371_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_371_piece0 of size 11109 dropped from memory (free 248632847)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_371_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_371_piece0
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_371_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 262.9 MB)
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 371
15/05/13 11:16:00 INFO BlockManager: Removing broadcast 370
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_370_piece0
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_370_piece0 of size 4225 dropped from memory (free 248637072)
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_370_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_370_piece0
15/05/13 11:16:00 INFO BlockManager: Removing block broadcast_370
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_370 of size 6040 dropped from memory (free 248643112)
15/05/13 11:16:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:16:00 INFO BlockManagerInfo: Removed broadcast_370_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:16:00 INFO DAGScheduler: Got job 254 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:16:00 INFO DAGScheduler: Final stage: Stage 249(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:16:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:16:00 INFO ContextCleaner: Cleaned broadcast 370
15/05/13 11:16:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:16:00 INFO DAGScheduler: Submitting Stage 249 (MapPartitionsRDD[1779] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=29659444, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_374 stored as values in memory (estimated size 21.0 KB, free 237.1 MB)
15/05/13 11:16:00 INFO MemoryStore: ensureFreeSpace(11296) called with curMem=29680948, maxMem=278302556
15/05/13 11:16:00 INFO MemoryStore: Block broadcast_374_piece0 stored as bytes in memory (estimated size 11.0 KB, free 237.1 MB)
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 261.7 MB)
15/05/13 11:16:00 INFO BlockManagerMaster: Updated info of block broadcast_374_piece0
15/05/13 11:16:00 INFO SparkContext: Created broadcast 374 from broadcast at DAGScheduler.scala:839
15/05/13 11:16:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 249 (MapPartitionsRDD[1779] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:16:00 INFO TaskSchedulerImpl: Adding task set 249.0 with 1 tasks
15/05/13 11:16:00 INFO TaskSetManager: Starting task 0.0 in stage 249.0 (TID 249, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:16:00 INFO BlockManagerInfo: Added broadcast_374_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 11.0 KB, free: 264.0 MB)
15/05/13 11:16:01 INFO TaskSetManager: Finished task 0.0 in stage 249.0 (TID 249) in 77 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:16:01 INFO TaskSchedulerImpl: Removed TaskSet 249.0, whose tasks have all completed, from pool 
15/05/13 11:16:01 INFO DAGScheduler: Stage 249 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.080 s
15/05/13 11:16:01 INFO DAGScheduler: Job 254 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.112558 s
15/05/13 11:16:01 INFO JobScheduler: Finished job streaming job 1431530160000 ms.0 from job set of time 1431530160000 ms
15/05/13 11:16:01 INFO JobScheduler: Total delay: 1.074 s for time 1431530160000 ms (execution: 0.849 s)
15/05/13 11:16:01 INFO MapPartitionsRDD: Removing RDD 1754 from persistence list
15/05/13 11:16:01 INFO BlockManager: Removing RDD 1754
15/05/13 11:16:01 INFO UnionRDD: Removing RDD 1753 from persistence list
15/05/13 11:16:01 INFO BlockManager: Removing RDD 1753
15/05/13 11:16:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530100000 ms: 1431530040000 ms
15/05/13 11:16:01 INFO JobGenerator: Checkpointing graph for time 1431530160000 ms
15/05/13 11:16:01 INFO DStreamGraph: Updating checkpoint data for time 1431530160000 ms
15/05/13 11:16:01 INFO DStreamGraph: Updated checkpoint data for time 1431530160000 ms
15/05/13 11:16:01 INFO CheckpointWriter: Saving checkpoint for time 1431530160000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000'
15/05/13 11:16:01 INFO CheckpointWriter: Checkpoint for time 1431530160000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000', took 7654 bytes and 68 ms
15/05/13 11:16:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530160000 ms
15/05/13 11:16:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530160000 ms
15/05/13 11:16:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:17:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 11:17:00 INFO FileInputDStream: New files at time 1431530220000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530136673.json
15/05/13 11:17:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=29692244, maxMem=278302556
15/05/13 11:17:00 INFO MemoryStore: Block broadcast_375 stored as values in memory (estimated size 232.9 KB, free 236.9 MB)
15/05/13 11:17:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=29930776, maxMem=278302556
15/05/13 11:17:00 INFO MemoryStore: Block broadcast_375_piece0 stored as bytes in memory (estimated size 34.9 KB, free 236.8 MB)
15/05/13 11:17:00 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.7 MB)
15/05/13 11:17:00 INFO BlockManagerMaster: Updated info of block broadcast_375_piece0
15/05/13 11:17:00 INFO SparkContext: Created broadcast 375 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:17:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:17:00 INFO JobScheduler: Added jobs for time 1431530220000 ms
15/05/13 11:17:00 INFO JobGenerator: Checkpointing graph for time 1431530220000 ms
15/05/13 11:17:00 INFO DStreamGraph: Updating checkpoint data for time 1431530220000 ms
15/05/13 11:17:00 INFO JobScheduler: Starting job streaming job 1431530220000 ms.0 from job set of time 1431530220000 ms
15/05/13 11:17:00 INFO DStreamGraph: Updated checkpoint data for time 1431530220000 ms
15/05/13 11:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431530220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000'
15/05/13 11:17:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83915): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431530220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000'
15/05/13 11:17:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83917): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:17:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83917): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:17:00 INFO CheckpointWriter: Saving checkpoint for time 1431530220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000'
15/05/13 11:17:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:17:00 INFO DAGScheduler: Got job 255 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:17:00 INFO DAGScheduler: Final stage: Stage 250(reduce at JsonRDD.scala:51)
15/05/13 11:17:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:17:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:17:00 INFO DAGScheduler: Submitting Stage 250 (MapPartitionsRDD[1786] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:17:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=29966484, maxMem=278302556
15/05/13 11:17:00 INFO MemoryStore: Block broadcast_376 stored as values in memory (estimated size 5.9 KB, free 236.8 MB)
15/05/13 11:17:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=29972524, maxMem=278302556
15/05/13 11:17:00 INFO MemoryStore: Block broadcast_376_piece0 stored as bytes in memory (estimated size 4.1 KB, free 236.8 MB)
15/05/13 11:17:00 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.7 MB)
15/05/13 11:17:00 INFO BlockManagerMaster: Updated info of block broadcast_376_piece0
15/05/13 11:17:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83919): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:17:00 WARN CheckpointWriter: Could not write checkpoint for time 1431530220000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000'
15/05/13 11:17:00 INFO SparkContext: Created broadcast 376 from broadcast at DAGScheduler.scala:839
15/05/13 11:17:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 250 (MapPartitionsRDD[1786] at map at JsonRDD.scala:51)
15/05/13 11:17:00 INFO TaskSchedulerImpl: Adding task set 250.0 with 1 tasks
15/05/13 11:17:00 INFO TaskSetManager: Starting task 0.0 in stage 250.0 (TID 250, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:17:00 INFO BlockManagerInfo: Added broadcast_376_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:17:00 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:17:00 INFO TaskSetManager: Finished task 0.0 in stage 250.0 (TID 250) in 440 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:17:00 INFO TaskSchedulerImpl: Removed TaskSet 250.0, whose tasks have all completed, from pool 
15/05/13 11:17:00 INFO DAGScheduler: Stage 250 (reduce at JsonRDD.scala:51) finished in 0.456 s
15/05/13 11:17:00 INFO DAGScheduler: Job 255 finished: reduce at JsonRDD.scala:51, took 0.495716 s
15/05/13 11:17:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:17:01 INFO DAGScheduler: Got job 256 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:17:01 INFO DAGScheduler: Final stage: Stage 251(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:17:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:17:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:17:01 INFO DAGScheduler: Submitting Stage 251 (MapPartitionsRDD[1793] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:17:01 INFO MemoryStore: ensureFreeSpace(21656) called with curMem=29976749, maxMem=278302556
15/05/13 11:17:01 INFO MemoryStore: Block broadcast_377 stored as values in memory (estimated size 21.1 KB, free 236.8 MB)
15/05/13 11:17:01 INFO MemoryStore: ensureFreeSpace(11157) called with curMem=29998405, maxMem=278302556
15/05/13 11:17:01 INFO MemoryStore: Block broadcast_377_piece0 stored as bytes in memory (estimated size 10.9 KB, free 236.8 MB)
15/05/13 11:17:01 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.7 MB)
15/05/13 11:17:01 INFO BlockManagerMaster: Updated info of block broadcast_377_piece0
15/05/13 11:17:01 INFO SparkContext: Created broadcast 377 from broadcast at DAGScheduler.scala:839
15/05/13 11:17:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 251 (MapPartitionsRDD[1793] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:17:01 INFO TaskSchedulerImpl: Adding task set 251.0 with 1 tasks
15/05/13 11:17:01 INFO TaskSetManager: Starting task 0.0 in stage 251.0 (TID 251, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:17:01 INFO BlockManagerInfo: Added broadcast_377_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.9 MB)
15/05/13 11:17:01 INFO BlockManagerInfo: Added broadcast_375_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:17:01 INFO TaskSetManager: Finished task 0.0 in stage 251.0 (TID 251) in 325 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:17:01 INFO TaskSchedulerImpl: Removed TaskSet 251.0, whose tasks have all completed, from pool 
15/05/13 11:17:01 INFO DAGScheduler: Stage 251 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.326 s
15/05/13 11:17:01 INFO DAGScheduler: Job 256 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.367990 s
15/05/13 11:17:01 INFO JobScheduler: Finished job streaming job 1431530220000 ms.0 from job set of time 1431530220000 ms
15/05/13 11:17:01 INFO JobScheduler: Total delay: 1.516 s for time 1431530220000 ms (execution: 1.308 s)
15/05/13 11:17:01 INFO MapPartitionsRDD: Removing RDD 1768 from persistence list
15/05/13 11:17:01 INFO BlockManager: Removing RDD 1768
15/05/13 11:17:01 INFO UnionRDD: Removing RDD 1767 from persistence list
15/05/13 11:17:01 INFO BlockManager: Removing RDD 1767
15/05/13 11:17:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530160000 ms: 1431530100000 ms
15/05/13 11:17:01 INFO JobGenerator: Checkpointing graph for time 1431530220000 ms
15/05/13 11:17:01 INFO DStreamGraph: Updating checkpoint data for time 1431530220000 ms
15/05/13 11:17:01 INFO DStreamGraph: Updated checkpoint data for time 1431530220000 ms
15/05/13 11:17:01 INFO CheckpointWriter: Saving checkpoint for time 1431530220000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000'
15/05/13 11:17:01 INFO CheckpointWriter: Checkpoint for time 1431530220000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000', took 7658 bytes and 59 ms
15/05/13 11:17:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530220000 ms
15/05/13 11:17:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530220000 ms
15/05/13 11:17:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:18:00 INFO FileInputDStream: Finding new files took 62 ms
15/05/13 11:18:00 INFO FileInputDStream: New files at time 1431530280000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530196903.json
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=30009562, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_378 stored as values in memory (estimated size 232.9 KB, free 236.6 MB)
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=30248094, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_378_piece0 stored as bytes in memory (estimated size 34.9 KB, free 236.5 MB)
15/05/13 11:18:00 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.6 MB)
15/05/13 11:18:00 INFO BlockManagerMaster: Updated info of block broadcast_378_piece0
15/05/13 11:18:00 INFO SparkContext: Created broadcast 378 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:18:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:18:00 INFO JobScheduler: Added jobs for time 1431530280000 ms
15/05/13 11:18:00 INFO JobGenerator: Checkpointing graph for time 1431530280000 ms
15/05/13 11:18:00 INFO DStreamGraph: Updating checkpoint data for time 1431530280000 ms
15/05/13 11:18:00 INFO DStreamGraph: Updated checkpoint data for time 1431530280000 ms
15/05/13 11:18:00 INFO JobScheduler: Starting job streaming job 1431530280000 ms.0 from job set of time 1431530280000 ms
15/05/13 11:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431530280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000'
15/05/13 11:18:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83926): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431530280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000'
15/05/13 11:18:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:18:00 INFO DAGScheduler: Got job 257 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:18:00 INFO DAGScheduler: Final stage: Stage 252(reduce at JsonRDD.scala:51)
15/05/13 11:18:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:18:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:18:00 INFO DAGScheduler: Submitting Stage 252 (MapPartitionsRDD[1800] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=30283802, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_379 stored as values in memory (estimated size 5.9 KB, free 236.5 MB)
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=30289842, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_379_piece0 stored as bytes in memory (estimated size 4.1 KB, free 236.5 MB)
15/05/13 11:18:00 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:18:00 INFO BlockManagerMaster: Updated info of block broadcast_379_piece0
15/05/13 11:18:00 INFO SparkContext: Created broadcast 379 from broadcast at DAGScheduler.scala:839
15/05/13 11:18:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 252 (MapPartitionsRDD[1800] at map at JsonRDD.scala:51)
15/05/13 11:18:00 INFO TaskSchedulerImpl: Adding task set 252.0 with 1 tasks
15/05/13 11:18:00 INFO TaskSetManager: Starting task 0.0 in stage 252.0 (TID 252, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:18:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83928): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:18:00 INFO CheckpointWriter: Saving checkpoint for time 1431530280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000'
15/05/13 11:18:00 INFO BlockManagerInfo: Added broadcast_379_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.8 MB)
15/05/13 11:18:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431529980000.bk
15/05/13 11:18:00 INFO CheckpointWriter: Checkpoint for time 1431530280000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000', took 7665 bytes and 259 ms
15/05/13 11:18:00 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:18:00 INFO TaskSetManager: Finished task 0.0 in stage 252.0 (TID 252) in 370 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:18:00 INFO TaskSchedulerImpl: Removed TaskSet 252.0, whose tasks have all completed, from pool 
15/05/13 11:18:00 INFO DAGScheduler: Stage 252 (reduce at JsonRDD.scala:51) finished in 0.378 s
15/05/13 11:18:00 INFO DAGScheduler: Job 257 finished: reduce at JsonRDD.scala:51, took 0.419603 s
15/05/13 11:18:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:18:00 INFO DAGScheduler: Got job 258 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:18:00 INFO DAGScheduler: Final stage: Stage 253(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:18:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:18:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:18:00 INFO DAGScheduler: Submitting Stage 253 (MapPartitionsRDD[1807] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=30294070, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_380 stored as values in memory (estimated size 20.8 KB, free 236.5 MB)
15/05/13 11:18:00 INFO MemoryStore: ensureFreeSpace(11128) called with curMem=30315374, maxMem=278302556
15/05/13 11:18:00 INFO MemoryStore: Block broadcast_380_piece0 stored as bytes in memory (estimated size 10.9 KB, free 236.5 MB)
15/05/13 11:18:00 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.6 MB)
15/05/13 11:18:00 INFO BlockManagerMaster: Updated info of block broadcast_380_piece0
15/05/13 11:18:00 INFO SparkContext: Created broadcast 380 from broadcast at DAGScheduler.scala:839
15/05/13 11:18:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 253 (MapPartitionsRDD[1807] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:18:00 INFO TaskSchedulerImpl: Adding task set 253.0 with 1 tasks
15/05/13 11:18:00 INFO TaskSetManager: Starting task 0.0 in stage 253.0 (TID 253, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:18:01 INFO BlockManagerInfo: Added broadcast_380_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 264.0 MB)
15/05/13 11:18:01 INFO BlockManagerInfo: Added broadcast_378_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:18:01 INFO TaskSetManager: Finished task 0.0 in stage 253.0 (TID 253) in 145 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:18:01 INFO TaskSchedulerImpl: Removed TaskSet 253.0, whose tasks have all completed, from pool 
15/05/13 11:18:01 INFO DAGScheduler: Stage 253 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.147 s
15/05/13 11:18:01 INFO DAGScheduler: Job 258 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.171863 s
15/05/13 11:18:01 INFO JobScheduler: Finished job streaming job 1431530280000 ms.0 from job set of time 1431530280000 ms
15/05/13 11:18:01 INFO JobScheduler: Total delay: 1.150 s for time 1431530280000 ms (execution: 0.923 s)
15/05/13 11:18:01 INFO MapPartitionsRDD: Removing RDD 1782 from persistence list
15/05/13 11:18:01 INFO BlockManager: Removing RDD 1782
15/05/13 11:18:01 INFO UnionRDD: Removing RDD 1781 from persistence list
15/05/13 11:18:01 INFO BlockManager: Removing RDD 1781
15/05/13 11:18:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530220000 ms: 1431530160000 ms
15/05/13 11:18:01 INFO JobGenerator: Checkpointing graph for time 1431530280000 ms
15/05/13 11:18:01 INFO DStreamGraph: Updating checkpoint data for time 1431530280000 ms
15/05/13 11:18:01 INFO DStreamGraph: Updated checkpoint data for time 1431530280000 ms
15/05/13 11:18:01 INFO CheckpointWriter: Saving checkpoint for time 1431530280000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000'
15/05/13 11:18:01 INFO CheckpointWriter: Checkpoint for time 1431530280000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000', took 7656 bytes and 32 ms
15/05/13 11:18:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530280000 ms
15/05/13 11:18:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530280000 ms
15/05/13 11:18:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:19:00 INFO FileInputDStream: Finding new files took 31 ms
15/05/13 11:19:00 INFO FileInputDStream: New files at time 1431530340000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530257666.json
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=30326502, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_381 stored as values in memory (estimated size 232.9 KB, free 236.3 MB)
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=30565034, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_381_piece0 stored as bytes in memory (estimated size 34.9 KB, free 236.2 MB)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.6 MB)
15/05/13 11:19:00 INFO BlockManagerMaster: Updated info of block broadcast_381_piece0
15/05/13 11:19:00 INFO SparkContext: Created broadcast 381 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:19:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:19:00 INFO JobScheduler: Added jobs for time 1431530340000 ms
15/05/13 11:19:00 INFO JobGenerator: Checkpointing graph for time 1431530340000 ms
15/05/13 11:19:00 INFO DStreamGraph: Updating checkpoint data for time 1431530340000 ms
15/05/13 11:19:00 INFO DStreamGraph: Updated checkpoint data for time 1431530340000 ms
15/05/13 11:19:00 INFO JobScheduler: Starting job streaming job 1431530340000 ms.0 from job set of time 1431530340000 ms
15/05/13 11:19:00 INFO CheckpointWriter: Saving checkpoint for time 1431530340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530340000'
15/05/13 11:19:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530040000.bk
15/05/13 11:19:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:19:00 INFO DAGScheduler: Got job 259 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:19:00 INFO DAGScheduler: Final stage: Stage 254(reduce at JsonRDD.scala:51)
15/05/13 11:19:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:19:00 INFO CheckpointWriter: Checkpoint for time 1431530340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530340000', took 7664 bytes and 84 ms
15/05/13 11:19:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:19:00 INFO DAGScheduler: Submitting Stage 254 (MapPartitionsRDD[1814] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=30600742, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_382 stored as values in memory (estimated size 5.9 KB, free 236.2 MB)
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=30606782, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_382_piece0 stored as bytes in memory (estimated size 4.1 KB, free 236.2 MB)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_382_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:19:00 INFO BlockManagerMaster: Updated info of block broadcast_382_piece0
15/05/13 11:19:00 INFO SparkContext: Created broadcast 382 from broadcast at DAGScheduler.scala:839
15/05/13 11:19:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 254 (MapPartitionsRDD[1814] at map at JsonRDD.scala:51)
15/05/13 11:19:00 INFO TaskSchedulerImpl: Adding task set 254.0 with 1 tasks
15/05/13 11:19:00 INFO TaskSetManager: Starting task 0.0 in stage 254.0 (TID 254, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_382_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:19:00 INFO TaskSetManager: Finished task 0.0 in stage 254.0 (TID 254) in 408 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:19:00 INFO TaskSchedulerImpl: Removed TaskSet 254.0, whose tasks have all completed, from pool 
15/05/13 11:19:00 INFO DAGScheduler: Stage 254 (reduce at JsonRDD.scala:51) finished in 0.413 s
15/05/13 11:19:00 INFO DAGScheduler: Job 259 finished: reduce at JsonRDD.scala:51, took 0.457755 s
15/05/13 11:19:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:19:00 INFO DAGScheduler: Got job 260 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:19:00 INFO DAGScheduler: Final stage: Stage 255(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:19:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:19:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:19:00 INFO DAGScheduler: Submitting Stage 255 (MapPartitionsRDD[1821] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=30611007, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_383 stored as values in memory (estimated size 20.4 KB, free 236.2 MB)
15/05/13 11:19:00 INFO MemoryStore: ensureFreeSpace(10911) called with curMem=30631927, maxMem=278302556
15/05/13 11:19:00 INFO MemoryStore: Block broadcast_383_piece0 stored as bytes in memory (estimated size 10.7 KB, free 236.2 MB)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_383_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.6 MB)
15/05/13 11:19:00 INFO BlockManagerMaster: Updated info of block broadcast_383_piece0
15/05/13 11:19:00 INFO SparkContext: Created broadcast 383 from broadcast at DAGScheduler.scala:839
15/05/13 11:19:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 255 (MapPartitionsRDD[1821] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:19:00 INFO TaskSchedulerImpl: Adding task set 255.0 with 1 tasks
15/05/13 11:19:00 INFO TaskSetManager: Starting task 0.0 in stage 255.0 (TID 255, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:19:00 INFO BlockManagerInfo: Added broadcast_383_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 262.8 MB)
15/05/13 11:19:01 INFO BlockManagerInfo: Added broadcast_381_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:19:01 INFO TaskSetManager: Finished task 0.0 in stage 255.0 (TID 255) in 289 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:19:01 INFO TaskSchedulerImpl: Removed TaskSet 255.0, whose tasks have all completed, from pool 
15/05/13 11:19:01 INFO DAGScheduler: Stage 255 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.291 s
15/05/13 11:19:01 INFO DAGScheduler: Job 260 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.320111 s
15/05/13 11:19:01 INFO JobScheduler: Finished job streaming job 1431530340000 ms.0 from job set of time 1431530340000 ms
15/05/13 11:19:01 INFO JobScheduler: Total delay: 1.284 s for time 1431530340000 ms (execution: 1.173 s)
15/05/13 11:19:01 INFO MapPartitionsRDD: Removing RDD 1796 from persistence list
15/05/13 11:19:01 INFO BlockManager: Removing RDD 1796
15/05/13 11:19:01 INFO UnionRDD: Removing RDD 1795 from persistence list
15/05/13 11:19:01 INFO BlockManager: Removing RDD 1795
15/05/13 11:19:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530280000 ms: 1431530220000 ms
15/05/13 11:19:01 INFO JobGenerator: Checkpointing graph for time 1431530340000 ms
15/05/13 11:19:01 INFO DStreamGraph: Updating checkpoint data for time 1431530340000 ms
15/05/13 11:19:01 INFO DStreamGraph: Updated checkpoint data for time 1431530340000 ms
15/05/13 11:19:01 INFO CheckpointWriter: Saving checkpoint for time 1431530340000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530340000'
15/05/13 11:19:01 INFO CheckpointWriter: Checkpoint for time 1431530340000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530340000', took 7657 bytes and 58 ms
15/05/13 11:19:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530340000 ms
15/05/13 11:19:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530340000 ms
15/05/13 11:19:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:20:00 INFO FileInputDStream: Finding new files took 23 ms
15/05/13 11:20:00 INFO FileInputDStream: New files at time 1431530400000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530318422.json
15/05/13 11:20:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=30642838, maxMem=278302556
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_384 stored as values in memory (estimated size 232.9 KB, free 236.0 MB)
15/05/13 11:20:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=30881370, maxMem=278302556
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_384_piece0 stored as bytes in memory (estimated size 34.9 KB, free 235.9 MB)
15/05/13 11:20:00 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.5 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_384_piece0
15/05/13 11:20:00 INFO SparkContext: Created broadcast 384 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:20:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:20:00 INFO JobScheduler: Added jobs for time 1431530400000 ms
15/05/13 11:20:00 INFO JobGenerator: Checkpointing graph for time 1431530400000 ms
15/05/13 11:20:00 INFO DStreamGraph: Updating checkpoint data for time 1431530400000 ms
15/05/13 11:20:00 INFO DStreamGraph: Updated checkpoint data for time 1431530400000 ms
15/05/13 11:20:00 INFO JobScheduler: Starting job streaming job 1431530400000 ms.0 from job set of time 1431530400000 ms
15/05/13 11:20:00 INFO CheckpointWriter: Saving checkpoint for time 1431530400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530400000'
15/05/13 11:20:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:20:00 INFO DAGScheduler: Got job 261 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:20:00 INFO DAGScheduler: Final stage: Stage 256(reduce at JsonRDD.scala:51)
15/05/13 11:20:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:20:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:20:00 INFO DAGScheduler: Submitting Stage 256 (MapPartitionsRDD[1828] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:20:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=30917078, maxMem=278302556
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_385 stored as values in memory (estimated size 5.9 KB, free 235.9 MB)
15/05/13 11:20:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=30923118, maxMem=278302556
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_385_piece0 stored as bytes in memory (estimated size 4.1 KB, free 235.9 MB)
15/05/13 11:20:00 INFO BlockManagerInfo: Added broadcast_385_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_385_piece0
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 377
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_377_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_377_piece0 of size 11157 dropped from memory (free 247386370)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_377_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.5 MB)
15/05/13 11:20:00 INFO SparkContext: Created broadcast 385 from broadcast at DAGScheduler.scala:839
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_377_piece0
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_377
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_377 of size 21656 dropped from memory (free 247408026)
15/05/13 11:20:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 256 (MapPartitionsRDD[1828] at map at JsonRDD.scala:51)
15/05/13 11:20:00 INFO TaskSchedulerImpl: Adding task set 256.0 with 1 tasks
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_377_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 262.8 MB)
15/05/13 11:20:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530100000
15/05/13 11:20:00 INFO TaskSetManager: Starting task 0.0 in stage 256.0 (TID 256, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:20:00 INFO CheckpointWriter: Checkpoint for time 1431530400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530400000', took 7666 bytes and 149 ms
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 377
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 376
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_376
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_376 of size 6040 dropped from memory (free 247414066)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_376_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_376_piece0 of size 4225 dropped from memory (free 247418291)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_376_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_376_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_376_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 376
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 375
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_375
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_375 of size 238532 dropped from memory (free 247656823)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_375_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_375_piece0 of size 35708 dropped from memory (free 247692531)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_375_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_375_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_375_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:20:00 INFO BlockManagerInfo: Added broadcast_385_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_375_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 375
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 374
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_374_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_374_piece0 of size 11296 dropped from memory (free 247703827)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_374_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_374_piece0
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_374
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_374 of size 21504 dropped from memory (free 247725331)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_374_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 11.0 KB, free: 264.0 MB)
15/05/13 11:20:00 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 374
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 383
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_383
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_383 of size 20920 dropped from memory (free 247746251)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_383_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_383_piece0 of size 10911 dropped from memory (free 247757162)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_383_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_383_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_383_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 262.8 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 383
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 382
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_382
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_382 of size 6040 dropped from memory (free 247763202)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_382_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_382_piece0 of size 4225 dropped from memory (free 247767427)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_382_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_382_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_382_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 382
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 380
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_380
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_380 of size 21304 dropped from memory (free 247788731)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_380_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_380_piece0 of size 11128 dropped from memory (free 247799859)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_380_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_380_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_380_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 263.9 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 380
15/05/13 11:20:00 INFO BlockManager: Removing broadcast 379
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_379
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_379 of size 6040 dropped from memory (free 247805899)
15/05/13 11:20:00 INFO BlockManager: Removing block broadcast_379_piece0
15/05/13 11:20:00 INFO MemoryStore: Block broadcast_379_piece0 of size 4228 dropped from memory (free 247810127)
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_379_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:20:00 INFO BlockManagerMaster: Updated info of block broadcast_379_piece0
15/05/13 11:20:00 INFO BlockManagerInfo: Removed broadcast_379_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.8 MB)
15/05/13 11:20:00 INFO ContextCleaner: Cleaned broadcast 379
15/05/13 11:20:00 INFO TaskSetManager: Finished task 0.0 in stage 256.0 (TID 256) in 453 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:20:00 INFO TaskSchedulerImpl: Removed TaskSet 256.0, whose tasks have all completed, from pool 
15/05/13 11:20:00 INFO DAGScheduler: Stage 256 (reduce at JsonRDD.scala:51) finished in 0.466 s
15/05/13 11:20:00 INFO DAGScheduler: Job 261 finished: reduce at JsonRDD.scala:51, took 0.563217 s
15/05/13 11:20:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:20:01 INFO DAGScheduler: Got job 262 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:20:01 INFO DAGScheduler: Final stage: Stage 257(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:20:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:20:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:20:01 INFO DAGScheduler: Submitting Stage 257 (MapPartitionsRDD[1835] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:20:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=30492429, maxMem=278302556
15/05/13 11:20:01 INFO MemoryStore: Block broadcast_386 stored as values in memory (estimated size 20.6 KB, free 236.3 MB)
15/05/13 11:20:01 INFO MemoryStore: ensureFreeSpace(10946) called with curMem=30513525, maxMem=278302556
15/05/13 11:20:01 INFO MemoryStore: Block broadcast_386_piece0 stored as bytes in memory (estimated size 10.7 KB, free 236.3 MB)
15/05/13 11:20:01 INFO BlockManagerInfo: Added broadcast_386_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.6 MB)
15/05/13 11:20:01 INFO BlockManagerMaster: Updated info of block broadcast_386_piece0
15/05/13 11:20:01 INFO SparkContext: Created broadcast 386 from broadcast at DAGScheduler.scala:839
15/05/13 11:20:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 257 (MapPartitionsRDD[1835] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:20:01 INFO TaskSchedulerImpl: Adding task set 257.0 with 1 tasks
15/05/13 11:20:01 INFO TaskSetManager: Starting task 0.0 in stage 257.0 (TID 257, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:20:01 INFO BlockManagerInfo: Added broadcast_386_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 262.8 MB)
15/05/13 11:20:01 INFO BlockManagerInfo: Added broadcast_384_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.8 MB)
15/05/13 11:20:01 INFO TaskSetManager: Finished task 0.0 in stage 257.0 (TID 257) in 279 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:20:01 INFO TaskSchedulerImpl: Removed TaskSet 257.0, whose tasks have all completed, from pool 
15/05/13 11:20:01 INFO DAGScheduler: Stage 257 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.281 s
15/05/13 11:20:01 INFO DAGScheduler: Job 262 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.310235 s
15/05/13 11:20:01 INFO JobScheduler: Finished job streaming job 1431530400000 ms.0 from job set of time 1431530400000 ms
15/05/13 11:20:01 INFO JobScheduler: Total delay: 1.359 s for time 1431530400000 ms (execution: 1.177 s)
15/05/13 11:20:01 INFO MapPartitionsRDD: Removing RDD 1810 from persistence list
15/05/13 11:20:01 INFO BlockManager: Removing RDD 1810
15/05/13 11:20:01 INFO UnionRDD: Removing RDD 1809 from persistence list
15/05/13 11:20:01 INFO BlockManager: Removing RDD 1809
15/05/13 11:20:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530340000 ms: 1431530280000 ms
15/05/13 11:20:01 INFO JobGenerator: Checkpointing graph for time 1431530400000 ms
15/05/13 11:20:01 INFO DStreamGraph: Updating checkpoint data for time 1431530400000 ms
15/05/13 11:20:01 INFO DStreamGraph: Updated checkpoint data for time 1431530400000 ms
15/05/13 11:20:01 INFO CheckpointWriter: Saving checkpoint for time 1431530400000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530400000'
15/05/13 11:20:01 INFO CheckpointWriter: Checkpoint for time 1431530400000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530400000', took 7657 bytes and 90 ms
15/05/13 11:20:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530400000 ms
15/05/13 11:20:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530400000 ms
15/05/13 11:20:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:21:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 11:21:00 INFO FileInputDStream: New files at time 1431530460000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530378902.json
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=30524471, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_387 stored as values in memory (estimated size 232.9 KB, free 236.1 MB)
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=30763003, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_387_piece0 stored as bytes in memory (estimated size 34.9 KB, free 236.0 MB)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.6 MB)
15/05/13 11:21:00 INFO BlockManagerMaster: Updated info of block broadcast_387_piece0
15/05/13 11:21:00 INFO SparkContext: Created broadcast 387 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:21:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:21:00 INFO JobScheduler: Added jobs for time 1431530460000 ms
15/05/13 11:21:00 INFO JobGenerator: Checkpointing graph for time 1431530460000 ms
15/05/13 11:21:00 INFO DStreamGraph: Updating checkpoint data for time 1431530460000 ms
15/05/13 11:21:00 INFO JobScheduler: Starting job streaming job 1431530460000 ms.0 from job set of time 1431530460000 ms
15/05/13 11:21:00 INFO DStreamGraph: Updated checkpoint data for time 1431530460000 ms
15/05/13 11:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431530460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000'
15/05/13 11:21:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83953): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431530460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000'
15/05/13 11:21:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:21:00 INFO DAGScheduler: Got job 263 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:21:00 INFO DAGScheduler: Final stage: Stage 258(reduce at JsonRDD.scala:51)
15/05/13 11:21:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:21:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:21:00 INFO DAGScheduler: Submitting Stage 258 (MapPartitionsRDD[1842] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=30798711, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_388 stored as values in memory (estimated size 5.9 KB, free 236.0 MB)
15/05/13 11:21:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83955): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:21:00 INFO CheckpointWriter: Saving checkpoint for time 1431530460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000'
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=30804751, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_388_piece0 stored as bytes in memory (estimated size 4.1 KB, free 236.0 MB)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_388_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.6 MB)
15/05/13 11:21:00 INFO BlockManagerMaster: Updated info of block broadcast_388_piece0
15/05/13 11:21:00 INFO SparkContext: Created broadcast 388 from broadcast at DAGScheduler.scala:839
15/05/13 11:21:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 258 (MapPartitionsRDD[1842] at map at JsonRDD.scala:51)
15/05/13 11:21:00 INFO TaskSchedulerImpl: Adding task set 258.0 with 1 tasks
15/05/13 11:21:00 INFO TaskSetManager: Starting task 0.0 in stage 258.0 (TID 258, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_388_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:21:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530160000.bk
15/05/13 11:21:00 INFO CheckpointWriter: Checkpoint for time 1431530460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000', took 7664 bytes and 198 ms
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 264.0 MB)
15/05/13 11:21:00 INFO TaskSetManager: Finished task 0.0 in stage 258.0 (TID 258) in 239 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:21:00 INFO TaskSchedulerImpl: Removed TaskSet 258.0, whose tasks have all completed, from pool 
15/05/13 11:21:00 INFO DAGScheduler: Stage 258 (reduce at JsonRDD.scala:51) finished in 0.254 s
15/05/13 11:21:00 INFO DAGScheduler: Job 263 finished: reduce at JsonRDD.scala:51, took 0.289967 s
15/05/13 11:21:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:21:00 INFO DAGScheduler: Got job 264 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:21:00 INFO DAGScheduler: Final stage: Stage 259(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:21:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:21:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:21:00 INFO DAGScheduler: Submitting Stage 259 (MapPartitionsRDD[1849] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=30808976, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_389 stored as values in memory (estimated size 20.8 KB, free 236.0 MB)
15/05/13 11:21:00 INFO MemoryStore: ensureFreeSpace(11098) called with curMem=30830280, maxMem=278302556
15/05/13 11:21:00 INFO MemoryStore: Block broadcast_389_piece0 stored as bytes in memory (estimated size 10.8 KB, free 236.0 MB)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_389_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 261.6 MB)
15/05/13 11:21:00 INFO BlockManagerMaster: Updated info of block broadcast_389_piece0
15/05/13 11:21:00 INFO SparkContext: Created broadcast 389 from broadcast at DAGScheduler.scala:839
15/05/13 11:21:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 259 (MapPartitionsRDD[1849] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:21:00 INFO TaskSchedulerImpl: Adding task set 259.0 with 1 tasks
15/05/13 11:21:00 INFO TaskSetManager: Starting task 0.0 in stage 259.0 (TID 259, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_389_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 262.8 MB)
15/05/13 11:21:00 INFO BlockManagerInfo: Added broadcast_387_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.7 MB)
15/05/13 11:21:01 INFO TaskSetManager: Finished task 0.0 in stage 259.0 (TID 259) in 240 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:21:01 INFO TaskSchedulerImpl: Removed TaskSet 259.0, whose tasks have all completed, from pool 
15/05/13 11:21:01 INFO DAGScheduler: Stage 259 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.243 s
15/05/13 11:21:01 INFO DAGScheduler: Job 264 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.285548 s
15/05/13 11:21:01 INFO JobScheduler: Finished job streaming job 1431530460000 ms.0 from job set of time 1431530460000 ms
15/05/13 11:21:01 INFO JobScheduler: Total delay: 1.148 s for time 1431530460000 ms (execution: 0.940 s)
15/05/13 11:21:01 INFO MapPartitionsRDD: Removing RDD 1824 from persistence list
15/05/13 11:21:01 INFO BlockManager: Removing RDD 1824
15/05/13 11:21:01 INFO UnionRDD: Removing RDD 1823 from persistence list
15/05/13 11:21:01 INFO BlockManager: Removing RDD 1823
15/05/13 11:21:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530400000 ms: 1431530340000 ms
15/05/13 11:21:01 INFO JobGenerator: Checkpointing graph for time 1431530460000 ms
15/05/13 11:21:01 INFO DStreamGraph: Updating checkpoint data for time 1431530460000 ms
15/05/13 11:21:01 INFO DStreamGraph: Updated checkpoint data for time 1431530460000 ms
15/05/13 11:21:01 INFO CheckpointWriter: Saving checkpoint for time 1431530460000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000'
15/05/13 11:21:01 INFO CheckpointWriter: Checkpoint for time 1431530460000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530460000', took 7653 bytes and 58 ms
15/05/13 11:21:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530460000 ms
15/05/13 11:21:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530460000 ms
15/05/13 11:21:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:22:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 11:22:00 INFO FileInputDStream: New files at time 1431530520000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530439439.json
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=30841378, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_390 stored as values in memory (estimated size 232.9 KB, free 235.8 MB)
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=31079910, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_390_piece0 stored as bytes in memory (estimated size 34.9 KB, free 235.7 MB)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.5 MB)
15/05/13 11:22:00 INFO BlockManagerMaster: Updated info of block broadcast_390_piece0
15/05/13 11:22:00 INFO SparkContext: Created broadcast 390 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:22:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:22:00 INFO JobScheduler: Starting job streaming job 1431530520000 ms.0 from job set of time 1431530520000 ms
15/05/13 11:22:00 INFO JobScheduler: Added jobs for time 1431530520000 ms
15/05/13 11:22:00 INFO JobGenerator: Checkpointing graph for time 1431530520000 ms
15/05/13 11:22:00 INFO DStreamGraph: Updating checkpoint data for time 1431530520000 ms
15/05/13 11:22:00 INFO DStreamGraph: Updated checkpoint data for time 1431530520000 ms
15/05/13 11:22:00 INFO CheckpointWriter: Saving checkpoint for time 1431530520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530520000'
15/05/13 11:22:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530220000
15/05/13 11:22:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:22:00 INFO CheckpointWriter: Checkpoint for time 1431530520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530520000', took 7663 bytes and 76 ms
15/05/13 11:22:00 INFO DAGScheduler: Got job 265 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:22:00 INFO DAGScheduler: Final stage: Stage 260(reduce at JsonRDD.scala:51)
15/05/13 11:22:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:22:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:22:00 INFO DAGScheduler: Submitting Stage 260 (MapPartitionsRDD[1856] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=31115618, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_391 stored as values in memory (estimated size 5.9 KB, free 235.7 MB)
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(4223) called with curMem=31121658, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_391_piece0 stored as bytes in memory (estimated size 4.1 KB, free 235.7 MB)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_391_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:22:00 INFO BlockManagerMaster: Updated info of block broadcast_391_piece0
15/05/13 11:22:00 INFO SparkContext: Created broadcast 391 from broadcast at DAGScheduler.scala:839
15/05/13 11:22:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 260 (MapPartitionsRDD[1856] at map at JsonRDD.scala:51)
15/05/13 11:22:00 INFO TaskSchedulerImpl: Adding task set 260.0 with 1 tasks
15/05/13 11:22:00 INFO TaskSetManager: Starting task 0.0 in stage 260.0 (TID 260, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_391_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 264.0 MB)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:22:00 INFO TaskSetManager: Finished task 0.0 in stage 260.0 (TID 260) in 195 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:22:00 INFO TaskSchedulerImpl: Removed TaskSet 260.0, whose tasks have all completed, from pool 
15/05/13 11:22:00 INFO DAGScheduler: Stage 260 (reduce at JsonRDD.scala:51) finished in 0.207 s
15/05/13 11:22:00 INFO DAGScheduler: Job 265 finished: reduce at JsonRDD.scala:51, took 0.236242 s
15/05/13 11:22:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:22:00 INFO DAGScheduler: Got job 266 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:22:00 INFO DAGScheduler: Final stage: Stage 261(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:22:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:22:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:22:00 INFO DAGScheduler: Submitting Stage 261 (MapPartitionsRDD[1863] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=31125881, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_392 stored as values in memory (estimated size 20.4 KB, free 235.7 MB)
15/05/13 11:22:00 INFO MemoryStore: ensureFreeSpace(10841) called with curMem=31146801, maxMem=278302556
15/05/13 11:22:00 INFO MemoryStore: Block broadcast_392_piece0 stored as bytes in memory (estimated size 10.6 KB, free 235.7 MB)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_392_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.5 MB)
15/05/13 11:22:00 INFO BlockManagerMaster: Updated info of block broadcast_392_piece0
15/05/13 11:22:00 INFO SparkContext: Created broadcast 392 from broadcast at DAGScheduler.scala:839
15/05/13 11:22:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 261 (MapPartitionsRDD[1863] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:22:00 INFO TaskSchedulerImpl: Adding task set 261.0 with 1 tasks
15/05/13 11:22:00 INFO TaskSetManager: Starting task 0.0 in stage 261.0 (TID 261, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_392_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.7 MB)
15/05/13 11:22:00 INFO BlockManagerInfo: Added broadcast_390_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.7 MB)
15/05/13 11:22:01 INFO TaskSetManager: Finished task 0.0 in stage 261.0 (TID 261) in 270 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:22:01 INFO TaskSchedulerImpl: Removed TaskSet 261.0, whose tasks have all completed, from pool 
15/05/13 11:22:01 INFO DAGScheduler: Stage 261 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.272 s
15/05/13 11:22:01 INFO DAGScheduler: Job 266 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.297832 s
15/05/13 11:22:01 INFO JobScheduler: Finished job streaming job 1431530520000 ms.0 from job set of time 1431530520000 ms
15/05/13 11:22:01 INFO JobScheduler: Total delay: 1.084 s for time 1431530520000 ms (execution: 0.883 s)
15/05/13 11:22:01 INFO MapPartitionsRDD: Removing RDD 1838 from persistence list
15/05/13 11:22:01 INFO BlockManager: Removing RDD 1838
15/05/13 11:22:01 INFO UnionRDD: Removing RDD 1837 from persistence list
15/05/13 11:22:01 INFO BlockManager: Removing RDD 1837
15/05/13 11:22:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530460000 ms: 1431530400000 ms
15/05/13 11:22:01 INFO JobGenerator: Checkpointing graph for time 1431530520000 ms
15/05/13 11:22:01 INFO DStreamGraph: Updating checkpoint data for time 1431530520000 ms
15/05/13 11:22:01 INFO DStreamGraph: Updated checkpoint data for time 1431530520000 ms
15/05/13 11:22:01 INFO CheckpointWriter: Saving checkpoint for time 1431530520000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530520000'
15/05/13 11:22:01 INFO CheckpointWriter: Checkpoint for time 1431530520000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530520000', took 7657 bytes and 53 ms
15/05/13 11:22:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530520000 ms
15/05/13 11:22:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530520000 ms
15/05/13 11:22:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:23:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 11:23:00 INFO FileInputDStream: New files at time 1431530580000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530500462.json
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=31157642, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_393 stored as values in memory (estimated size 232.9 KB, free 235.5 MB)
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=31396174, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_393_piece0 stored as bytes in memory (estimated size 34.9 KB, free 235.4 MB)
15/05/13 11:23:00 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.5 MB)
15/05/13 11:23:00 INFO BlockManagerMaster: Updated info of block broadcast_393_piece0
15/05/13 11:23:00 INFO SparkContext: Created broadcast 393 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:23:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:23:00 INFO JobScheduler: Added jobs for time 1431530580000 ms
15/05/13 11:23:00 INFO JobGenerator: Checkpointing graph for time 1431530580000 ms
15/05/13 11:23:00 INFO DStreamGraph: Updating checkpoint data for time 1431530580000 ms
15/05/13 11:23:00 INFO JobScheduler: Starting job streaming job 1431530580000 ms.0 from job set of time 1431530580000 ms
15/05/13 11:23:00 INFO DStreamGraph: Updated checkpoint data for time 1431530580000 ms
15/05/13 11:23:00 INFO CheckpointWriter: Saving checkpoint for time 1431530580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530580000'
15/05/13 11:23:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:23:00 INFO DAGScheduler: Got job 267 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:23:00 INFO DAGScheduler: Final stage: Stage 262(reduce at JsonRDD.scala:51)
15/05/13 11:23:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:23:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530280000.bk
15/05/13 11:23:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:23:00 INFO DAGScheduler: Submitting Stage 262 (MapPartitionsRDD[1870] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:23:00 INFO CheckpointWriter: Checkpoint for time 1431530580000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530580000', took 7662 bytes and 93 ms
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=31431882, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_394 stored as values in memory (estimated size 5.9 KB, free 235.4 MB)
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=31437922, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_394_piece0 stored as bytes in memory (estimated size 4.1 KB, free 235.4 MB)
15/05/13 11:23:00 INFO BlockManagerInfo: Added broadcast_394_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:23:00 INFO BlockManagerMaster: Updated info of block broadcast_394_piece0
15/05/13 11:23:00 INFO SparkContext: Created broadcast 394 from broadcast at DAGScheduler.scala:839
15/05/13 11:23:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 262 (MapPartitionsRDD[1870] at map at JsonRDD.scala:51)
15/05/13 11:23:00 INFO TaskSchedulerImpl: Adding task set 262.0 with 1 tasks
15/05/13 11:23:00 INFO TaskSetManager: Starting task 0.0 in stage 262.0 (TID 262, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:23:00 INFO BlockManagerInfo: Added broadcast_394_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:23:00 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:23:00 INFO TaskSetManager: Finished task 0.0 in stage 262.0 (TID 262) in 375 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:23:00 INFO TaskSchedulerImpl: Removed TaskSet 262.0, whose tasks have all completed, from pool 
15/05/13 11:23:00 INFO DAGScheduler: Stage 262 (reduce at JsonRDD.scala:51) finished in 0.387 s
15/05/13 11:23:00 INFO DAGScheduler: Job 267 finished: reduce at JsonRDD.scala:51, took 0.413155 s
15/05/13 11:23:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:23:00 INFO DAGScheduler: Got job 268 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:23:00 INFO DAGScheduler: Final stage: Stage 263(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:23:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:23:00 INFO BlockManager: Removing broadcast 386
15/05/13 11:23:00 INFO BlockManager: Removing block broadcast_386_piece0
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_386_piece0 of size 10946 dropped from memory (free 246871355)
15/05/13 11:23:00 INFO BlockManagerInfo: Removed broadcast_386_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.5 MB)
15/05/13 11:23:00 INFO BlockManagerMaster: Updated info of block broadcast_386_piece0
15/05/13 11:23:00 INFO BlockManager: Removing block broadcast_386
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_386 of size 21096 dropped from memory (free 246892451)
15/05/13 11:23:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:23:00 INFO DAGScheduler: Submitting Stage 263 (MapPartitionsRDD[1877] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:23:00 INFO BlockManagerInfo: Removed broadcast_386_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 262.7 MB)
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=31410105, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_395 stored as values in memory (estimated size 20.4 KB, free 235.4 MB)
15/05/13 11:23:00 INFO ContextCleaner: Cleaned broadcast 386
15/05/13 11:23:00 INFO BlockManager: Removing broadcast 385
15/05/13 11:23:00 INFO BlockManager: Removing block broadcast_385_piece0
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_385_piece0 of size 4225 dropped from memory (free 246875756)
15/05/13 11:23:00 INFO BlockManagerInfo: Removed broadcast_385_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:23:00 INFO BlockManagerMaster: Updated info of block broadcast_385_piece0
15/05/13 11:23:00 INFO BlockManager: Removing block broadcast_385
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_385 of size 6040 dropped from memory (free 246881796)
15/05/13 11:23:00 INFO MemoryStore: ensureFreeSpace(10888) called with curMem=31420760, maxMem=278302556
15/05/13 11:23:00 INFO MemoryStore: Block broadcast_395_piece0 stored as bytes in memory (estimated size 10.6 KB, free 235.4 MB)
15/05/13 11:23:00 INFO BlockManagerInfo: Added broadcast_395_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.5 MB)
15/05/13 11:23:00 INFO BlockManagerMaster: Updated info of block broadcast_395_piece0
15/05/13 11:23:00 INFO SparkContext: Created broadcast 395 from broadcast at DAGScheduler.scala:839
15/05/13 11:23:00 INFO BlockManagerInfo: Removed broadcast_385_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:23:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 263 (MapPartitionsRDD[1877] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:23:00 INFO TaskSchedulerImpl: Adding task set 263.0 with 1 tasks
15/05/13 11:23:01 INFO TaskSetManager: Starting task 0.0 in stage 263.0 (TID 263, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 385
15/05/13 11:23:01 INFO BlockManager: Removing broadcast 388
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_388
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_388 of size 6040 dropped from memory (free 246876948)
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_388_piece0
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_388_piece0 of size 4225 dropped from memory (free 246881173)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_388_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:23:01 INFO BlockManagerMaster: Updated info of block broadcast_388_piece0
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_388_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 388
15/05/13 11:23:01 INFO BlockManager: Removing broadcast 389
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_389_piece0
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_389_piece0 of size 11098 dropped from memory (free 246892271)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_389_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 261.5 MB)
15/05/13 11:23:01 INFO BlockManagerMaster: Updated info of block broadcast_389_piece0
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_389
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_389 of size 21304 dropped from memory (free 246913575)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_389_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 262.7 MB)
15/05/13 11:23:01 INFO BlockManagerInfo: Added broadcast_395_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.7 MB)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 389
15/05/13 11:23:01 INFO BlockManager: Removing broadcast 394
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_394_piece0
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_394_piece0 of size 4225 dropped from memory (free 246917800)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_394_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:23:01 INFO BlockManagerMaster: Updated info of block broadcast_394_piece0
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_394
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_394 of size 6040 dropped from memory (free 246923840)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_394_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 394
15/05/13 11:23:01 INFO BlockManager: Removing broadcast 391
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_391_piece0
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_391_piece0 of size 4223 dropped from memory (free 246928063)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_391_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:23:01 INFO BlockManagerMaster: Updated info of block broadcast_391_piece0
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_391
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_391 of size 6040 dropped from memory (free 246934103)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_391_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:23:01 INFO BlockManagerInfo: Added broadcast_393_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.7 MB)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 391
15/05/13 11:23:01 INFO BlockManager: Removing broadcast 392
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_392
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_392 of size 20920 dropped from memory (free 246955023)
15/05/13 11:23:01 INFO BlockManager: Removing block broadcast_392_piece0
15/05/13 11:23:01 INFO MemoryStore: Block broadcast_392_piece0 of size 10841 dropped from memory (free 246965864)
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_392_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.5 MB)
15/05/13 11:23:01 INFO BlockManagerMaster: Updated info of block broadcast_392_piece0
15/05/13 11:23:01 INFO BlockManagerInfo: Removed broadcast_392_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.7 MB)
15/05/13 11:23:01 INFO ContextCleaner: Cleaned broadcast 392
15/05/13 11:23:01 INFO TaskSetManager: Finished task 0.0 in stage 263.0 (TID 263) in 285 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:23:01 INFO TaskSchedulerImpl: Removed TaskSet 263.0, whose tasks have all completed, from pool 
15/05/13 11:23:01 INFO DAGScheduler: Stage 263 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.286 s
15/05/13 11:23:01 INFO DAGScheduler: Job 268 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.371627 s
15/05/13 11:23:01 INFO JobScheduler: Finished job streaming job 1431530580000 ms.0 from job set of time 1431530580000 ms
15/05/13 11:23:01 INFO JobScheduler: Total delay: 1.353 s for time 1431530580000 ms (execution: 1.138 s)
15/05/13 11:23:01 INFO MapPartitionsRDD: Removing RDD 1852 from persistence list
15/05/13 11:23:01 INFO BlockManager: Removing RDD 1852
15/05/13 11:23:01 INFO UnionRDD: Removing RDD 1851 from persistence list
15/05/13 11:23:01 INFO BlockManager: Removing RDD 1851
15/05/13 11:23:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530520000 ms: 1431530460000 ms
15/05/13 11:23:01 INFO JobGenerator: Checkpointing graph for time 1431530580000 ms
15/05/13 11:23:01 INFO DStreamGraph: Updating checkpoint data for time 1431530580000 ms
15/05/13 11:23:01 INFO DStreamGraph: Updated checkpoint data for time 1431530580000 ms
15/05/13 11:23:01 INFO CheckpointWriter: Saving checkpoint for time 1431530580000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530580000'
15/05/13 11:23:01 INFO CheckpointWriter: Checkpoint for time 1431530580000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530580000', took 7655 bytes and 67 ms
15/05/13 11:23:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530580000 ms
15/05/13 11:23:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530580000 ms
15/05/13 11:23:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:24:00 INFO FileInputDStream: Finding new files took 50 ms
15/05/13 11:24:00 INFO FileInputDStream: New files at time 1431530640000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530560939.json
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=31336692, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_396 stored as values in memory (estimated size 232.9 KB, free 235.3 MB)
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=31575224, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_396_piece0 stored as bytes in memory (estimated size 34.9 KB, free 235.3 MB)
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.5 MB)
15/05/13 11:24:00 INFO BlockManagerMaster: Updated info of block broadcast_396_piece0
15/05/13 11:24:00 INFO SparkContext: Created broadcast 396 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:24:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:24:00 INFO JobScheduler: Starting job streaming job 1431530640000 ms.0 from job set of time 1431530640000 ms
15/05/13 11:24:00 INFO JobScheduler: Added jobs for time 1431530640000 ms
15/05/13 11:24:00 INFO JobGenerator: Checkpointing graph for time 1431530640000 ms
15/05/13 11:24:00 INFO DStreamGraph: Updating checkpoint data for time 1431530640000 ms
15/05/13 11:24:00 INFO DStreamGraph: Updated checkpoint data for time 1431530640000 ms
15/05/13 11:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431530640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000'
15/05/13 11:24:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83980): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431530640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000'
15/05/13 11:24:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:24:00 INFO DAGScheduler: Got job 269 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:24:00 INFO DAGScheduler: Final stage: Stage 264(reduce at JsonRDD.scala:51)
15/05/13 11:24:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:24:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:24:00 INFO DAGScheduler: Submitting Stage 264 (MapPartitionsRDD[1884] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=31610932, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_397 stored as values in memory (estimated size 5.9 KB, free 235.3 MB)
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=31616972, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_397_piece0 stored as bytes in memory (estimated size 4.1 KB, free 235.3 MB)
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_397_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.5 MB)
15/05/13 11:24:00 INFO BlockManagerMaster: Updated info of block broadcast_397_piece0
15/05/13 11:24:00 INFO SparkContext: Created broadcast 397 from broadcast at DAGScheduler.scala:839
15/05/13 11:24:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 264 (MapPartitionsRDD[1884] at map at JsonRDD.scala:51)
15/05/13 11:24:00 INFO TaskSchedulerImpl: Adding task set 264.0 with 1 tasks
15/05/13 11:24:00 INFO TaskSetManager: Starting task 0.0 in stage 264.0 (TID 264, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:24:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83982): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431530640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000'
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_397_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:24:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83984): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:24:00 WARN CheckpointWriter: Could not write checkpoint for time 1431530640000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000'
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_396_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:24:00 INFO DAGScheduler: Stage 264 (reduce at JsonRDD.scala:51) finished in 0.232 s
15/05/13 11:24:00 INFO TaskSetManager: Finished task 0.0 in stage 264.0 (TID 264) in 233 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:24:00 INFO TaskSchedulerImpl: Removed TaskSet 264.0, whose tasks have all completed, from pool 
15/05/13 11:24:00 INFO DAGScheduler: Job 269 finished: reduce at JsonRDD.scala:51, took 0.276642 s
15/05/13 11:24:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:24:00 INFO DAGScheduler: Got job 270 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:24:00 INFO DAGScheduler: Final stage: Stage 265(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:24:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:24:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:24:00 INFO DAGScheduler: Submitting Stage 265 (MapPartitionsRDD[1891] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=31621197, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_398 stored as values in memory (estimated size 20.0 KB, free 235.2 MB)
15/05/13 11:24:00 INFO MemoryStore: ensureFreeSpace(10757) called with curMem=31641685, maxMem=278302556
15/05/13 11:24:00 INFO MemoryStore: Block broadcast_398_piece0 stored as bytes in memory (estimated size 10.5 KB, free 235.2 MB)
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_398_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 261.5 MB)
15/05/13 11:24:00 INFO BlockManagerMaster: Updated info of block broadcast_398_piece0
15/05/13 11:24:00 INFO SparkContext: Created broadcast 398 from broadcast at DAGScheduler.scala:839
15/05/13 11:24:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 265 (MapPartitionsRDD[1891] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:24:00 INFO TaskSchedulerImpl: Adding task set 265.0 with 1 tasks
15/05/13 11:24:00 INFO TaskSetManager: Starting task 0.0 in stage 265.0 (TID 265, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:24:00 INFO BlockManagerInfo: Added broadcast_398_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 263.8 MB)
15/05/13 11:24:00 INFO TaskSetManager: Finished task 0.0 in stage 265.0 (TID 265) in 77 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:24:00 INFO TaskSchedulerImpl: Removed TaskSet 265.0, whose tasks have all completed, from pool 
15/05/13 11:24:00 INFO DAGScheduler: Stage 265 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.078 s
15/05/13 11:24:00 INFO DAGScheduler: Job 270 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.105135 s
15/05/13 11:24:00 INFO JobScheduler: Finished job streaming job 1431530640000 ms.0 from job set of time 1431530640000 ms
15/05/13 11:24:00 INFO JobScheduler: Total delay: 0.934 s for time 1431530640000 ms (execution: 0.736 s)
15/05/13 11:24:00 INFO MapPartitionsRDD: Removing RDD 1866 from persistence list
15/05/13 11:24:00 INFO BlockManager: Removing RDD 1866
15/05/13 11:24:00 INFO UnionRDD: Removing RDD 1865 from persistence list
15/05/13 11:24:00 INFO BlockManager: Removing RDD 1865
15/05/13 11:24:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431530580000 ms: 1431530520000 ms
15/05/13 11:24:00 INFO JobGenerator: Checkpointing graph for time 1431530640000 ms
15/05/13 11:24:00 INFO DStreamGraph: Updating checkpoint data for time 1431530640000 ms
15/05/13 11:24:00 INFO DStreamGraph: Updated checkpoint data for time 1431530640000 ms
15/05/13 11:24:00 INFO CheckpointWriter: Saving checkpoint for time 1431530640000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000'
15/05/13 11:24:01 INFO CheckpointWriter: Checkpoint for time 1431530640000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530640000', took 7656 bytes and 47 ms
15/05/13 11:24:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530640000 ms
15/05/13 11:24:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530640000 ms
15/05/13 11:24:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:25:00 INFO FileInputDStream: Finding new files took 29 ms
15/05/13 11:25:00 INFO FileInputDStream: New files at time 1431530700000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530624768.json
15/05/13 11:25:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=31652442, maxMem=278302556
15/05/13 11:25:00 INFO MemoryStore: Block broadcast_399 stored as values in memory (estimated size 232.9 KB, free 235.0 MB)
15/05/13 11:25:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=31890974, maxMem=278302556
15/05/13 11:25:00 INFO MemoryStore: Block broadcast_399_piece0 stored as bytes in memory (estimated size 34.9 KB, free 235.0 MB)
15/05/13 11:25:00 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.4 MB)
15/05/13 11:25:00 INFO BlockManagerMaster: Updated info of block broadcast_399_piece0
15/05/13 11:25:00 INFO SparkContext: Created broadcast 399 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:25:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:25:00 INFO JobScheduler: Starting job streaming job 1431530700000 ms.0 from job set of time 1431530700000 ms
15/05/13 11:25:00 INFO JobScheduler: Added jobs for time 1431530700000 ms
15/05/13 11:25:00 INFO JobGenerator: Checkpointing graph for time 1431530700000 ms
15/05/13 11:25:00 INFO DStreamGraph: Updating checkpoint data for time 1431530700000 ms
15/05/13 11:25:00 INFO DStreamGraph: Updated checkpoint data for time 1431530700000 ms
15/05/13 11:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431530700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000'
15/05/13 11:25:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83991): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431530700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000'
15/05/13 11:25:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:25:00 INFO DAGScheduler: Got job 271 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:25:00 INFO DAGScheduler: Final stage: Stage 266(reduce at JsonRDD.scala:51)
15/05/13 11:25:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:25:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:25:00 INFO DAGScheduler: Submitting Stage 266 (MapPartitionsRDD[1898] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:25:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=31926682, maxMem=278302556
15/05/13 11:25:00 INFO MemoryStore: Block broadcast_400 stored as values in memory (estimated size 5.9 KB, free 235.0 MB)
15/05/13 11:25:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83993): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:25:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=31932722, maxMem=278302556
15/05/13 11:25:00 INFO CheckpointWriter: Saving checkpoint for time 1431530700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000'
15/05/13 11:25:00 INFO MemoryStore: Block broadcast_400_piece0 stored as bytes in memory (estimated size 4.1 KB, free 235.0 MB)
15/05/13 11:25:00 INFO BlockManagerInfo: Added broadcast_400_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:25:00 INFO BlockManagerMaster: Updated info of block broadcast_400_piece0
15/05/13 11:25:00 INFO SparkContext: Created broadcast 400 from broadcast at DAGScheduler.scala:839
15/05/13 11:25:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 266 (MapPartitionsRDD[1898] at map at JsonRDD.scala:51)
15/05/13 11:25:00 INFO TaskSchedulerImpl: Adding task set 266.0 with 1 tasks
15/05/13 11:25:00 INFO TaskSetManager: Starting task 0.0 in stage 266.0 (TID 266, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:25:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530400000.bk
15/05/13 11:25:00 INFO CheckpointWriter: Checkpoint for time 1431530700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000', took 7666 bytes and 165 ms
15/05/13 11:25:00 INFO BlockManagerInfo: Added broadcast_400_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.7 MB)
15/05/13 11:25:00 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.6 MB)
15/05/13 11:25:00 INFO TaskSetManager: Finished task 0.0 in stage 266.0 (TID 266) in 614 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:25:00 INFO TaskSchedulerImpl: Removed TaskSet 266.0, whose tasks have all completed, from pool 
15/05/13 11:25:00 INFO DAGScheduler: Stage 266 (reduce at JsonRDD.scala:51) finished in 0.632 s
15/05/13 11:25:00 INFO DAGScheduler: Job 271 finished: reduce at JsonRDD.scala:51, took 0.665834 s
15/05/13 11:25:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:25:01 INFO DAGScheduler: Got job 272 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:25:01 INFO DAGScheduler: Final stage: Stage 267(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:25:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:25:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:25:01 INFO DAGScheduler: Submitting Stage 267 (MapPartitionsRDD[1905] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:25:01 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=31936950, maxMem=278302556
15/05/13 11:25:01 INFO MemoryStore: Block broadcast_401 stored as values in memory (estimated size 20.6 KB, free 234.9 MB)
15/05/13 11:25:01 INFO MemoryStore: ensureFreeSpace(10958) called with curMem=31958046, maxMem=278302556
15/05/13 11:25:01 INFO MemoryStore: Block broadcast_401_piece0 stored as bytes in memory (estimated size 10.7 KB, free 234.9 MB)
15/05/13 11:25:01 INFO BlockManagerInfo: Added broadcast_401_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.4 MB)
15/05/13 11:25:01 INFO BlockManagerMaster: Updated info of block broadcast_401_piece0
15/05/13 11:25:01 INFO SparkContext: Created broadcast 401 from broadcast at DAGScheduler.scala:839
15/05/13 11:25:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 267 (MapPartitionsRDD[1905] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:25:01 INFO TaskSchedulerImpl: Adding task set 267.0 with 1 tasks
15/05/13 11:25:01 INFO TaskSetManager: Starting task 0.0 in stage 267.0 (TID 267, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:25:01 INFO BlockManagerInfo: Added broadcast_401_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 263.9 MB)
15/05/13 11:25:01 INFO BlockManagerInfo: Added broadcast_399_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:25:01 INFO TaskSetManager: Finished task 0.0 in stage 267.0 (TID 267) in 240 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:25:01 INFO TaskSchedulerImpl: Removed TaskSet 267.0, whose tasks have all completed, from pool 
15/05/13 11:25:01 INFO DAGScheduler: Stage 267 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.243 s
15/05/13 11:25:01 INFO DAGScheduler: Job 272 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.278641 s
15/05/13 11:25:01 INFO JobScheduler: Finished job streaming job 1431530700000 ms.0 from job set of time 1431530700000 ms
15/05/13 11:25:01 INFO JobScheduler: Total delay: 1.411 s for time 1431530700000 ms (execution: 1.275 s)
15/05/13 11:25:01 INFO MapPartitionsRDD: Removing RDD 1880 from persistence list
15/05/13 11:25:01 INFO BlockManager: Removing RDD 1880
15/05/13 11:25:01 INFO UnionRDD: Removing RDD 1879 from persistence list
15/05/13 11:25:01 INFO BlockManager: Removing RDD 1879
15/05/13 11:25:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530640000 ms: 1431530580000 ms
15/05/13 11:25:01 INFO JobGenerator: Checkpointing graph for time 1431530700000 ms
15/05/13 11:25:01 INFO DStreamGraph: Updating checkpoint data for time 1431530700000 ms
15/05/13 11:25:01 INFO DStreamGraph: Updated checkpoint data for time 1431530700000 ms
15/05/13 11:25:01 INFO CheckpointWriter: Saving checkpoint for time 1431530700000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000'
15/05/13 11:25:01 INFO CheckpointWriter: Checkpoint for time 1431530700000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000', took 7655 bytes and 61 ms
15/05/13 11:25:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530700000 ms
15/05/13 11:25:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530700000 ms
15/05/13 11:25:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:26:00 INFO FileInputDStream: Finding new files took 45 ms
15/05/13 11:26:00 INFO FileInputDStream: New files at time 1431530760000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530686259.json
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=31969004, maxMem=278302556
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_402 stored as values in memory (estimated size 232.9 KB, free 234.7 MB)
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=32207536, maxMem=278302556
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_402_piece0 stored as bytes in memory (estimated size 34.9 KB, free 234.7 MB)
15/05/13 11:26:00 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.4 MB)
15/05/13 11:26:00 INFO BlockManagerMaster: Updated info of block broadcast_402_piece0
15/05/13 11:26:00 INFO SparkContext: Created broadcast 402 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:26:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:26:00 INFO JobScheduler: Added jobs for time 1431530760000 ms
15/05/13 11:26:00 INFO JobScheduler: Starting job streaming job 1431530760000 ms.0 from job set of time 1431530760000 ms
15/05/13 11:26:00 INFO JobGenerator: Checkpointing graph for time 1431530760000 ms
15/05/13 11:26:00 INFO DStreamGraph: Updating checkpoint data for time 1431530760000 ms
15/05/13 11:26:00 INFO DStreamGraph: Updated checkpoint data for time 1431530760000 ms
15/05/13 11:26:00 INFO CheckpointWriter: Saving checkpoint for time 1431530760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000'
15/05/13 11:26:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83999): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:26:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83999): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:26:00 INFO CheckpointWriter: Saving checkpoint for time 1431530760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000'
15/05/13 11:26:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:26:00 INFO DAGScheduler: Got job 273 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:26:00 INFO DAGScheduler: Final stage: Stage 268(reduce at JsonRDD.scala:51)
15/05/13 11:26:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:26:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:26:00 INFO DAGScheduler: Submitting Stage 268 (MapPartitionsRDD[1912] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=32243244, maxMem=278302556
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_403 stored as values in memory (estimated size 5.9 KB, free 234.7 MB)
15/05/13 11:26:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84001): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(4223) called with curMem=32249284, maxMem=278302556
15/05/13 11:26:00 INFO CheckpointWriter: Saving checkpoint for time 1431530760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000'
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_403_piece0 stored as bytes in memory (estimated size 4.1 KB, free 234.7 MB)
15/05/13 11:26:00 INFO BlockManagerInfo: Added broadcast_403_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:26:00 INFO BlockManagerMaster: Updated info of block broadcast_403_piece0
15/05/13 11:26:00 INFO SparkContext: Created broadcast 403 from broadcast at DAGScheduler.scala:839
15/05/13 11:26:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 268 (MapPartitionsRDD[1912] at map at JsonRDD.scala:51)
15/05/13 11:26:00 INFO TaskSchedulerImpl: Adding task set 268.0 with 1 tasks
15/05/13 11:26:00 INFO TaskSetManager: Starting task 0.0 in stage 268.0 (TID 268, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:26:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84003): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:26:00 WARN CheckpointWriter: Could not write checkpoint for time 1431530760000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000'
15/05/13 11:26:00 INFO BlockManagerInfo: Added broadcast_403_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.6 MB)
15/05/13 11:26:00 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.6 MB)
15/05/13 11:26:00 INFO TaskSetManager: Finished task 0.0 in stage 268.0 (TID 268) in 405 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:26:00 INFO TaskSchedulerImpl: Removed TaskSet 268.0, whose tasks have all completed, from pool 
15/05/13 11:26:00 INFO DAGScheduler: Stage 268 (reduce at JsonRDD.scala:51) finished in 0.418 s
15/05/13 11:26:00 INFO DAGScheduler: Job 273 finished: reduce at JsonRDD.scala:51, took 0.459755 s
15/05/13 11:26:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:26:00 INFO DAGScheduler: Got job 274 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:26:00 INFO DAGScheduler: Final stage: Stage 269(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:26:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:26:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:26:00 INFO DAGScheduler: Submitting Stage 269 (MapPartitionsRDD[1919] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=32253507, maxMem=278302556
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_404 stored as values in memory (estimated size 20.8 KB, free 234.6 MB)
15/05/13 11:26:00 INFO MemoryStore: ensureFreeSpace(11105) called with curMem=32274811, maxMem=278302556
15/05/13 11:26:00 INFO MemoryStore: Block broadcast_404_piece0 stored as bytes in memory (estimated size 10.8 KB, free 234.6 MB)
15/05/13 11:26:00 INFO BlockManagerInfo: Added broadcast_404_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 261.4 MB)
15/05/13 11:26:00 INFO BlockManagerMaster: Updated info of block broadcast_404_piece0
15/05/13 11:26:00 INFO SparkContext: Created broadcast 404 from broadcast at DAGScheduler.scala:839
15/05/13 11:26:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 269 (MapPartitionsRDD[1919] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:26:00 INFO TaskSchedulerImpl: Adding task set 269.0 with 1 tasks
15/05/13 11:26:00 INFO TaskSetManager: Starting task 0.0 in stage 269.0 (TID 269, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:26:01 INFO BlockManagerInfo: Added broadcast_404_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.8 KB, free: 263.8 MB)
15/05/13 11:26:01 INFO BlockManagerInfo: Added broadcast_402_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:26:01 INFO TaskSetManager: Finished task 0.0 in stage 269.0 (TID 269) in 287 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:26:01 INFO TaskSchedulerImpl: Removed TaskSet 269.0, whose tasks have all completed, from pool 
15/05/13 11:26:01 INFO DAGScheduler: Stage 269 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.288 s
15/05/13 11:26:01 INFO DAGScheduler: Job 274 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.307027 s
15/05/13 11:26:01 INFO JobScheduler: Finished job streaming job 1431530760000 ms.0 from job set of time 1431530760000 ms
15/05/13 11:26:01 INFO JobScheduler: Total delay: 1.314 s for time 1431530760000 ms (execution: 1.111 s)
15/05/13 11:26:01 INFO MapPartitionsRDD: Removing RDD 1894 from persistence list
15/05/13 11:26:01 INFO BlockManager: Removing RDD 1894
15/05/13 11:26:01 INFO UnionRDD: Removing RDD 1893 from persistence list
15/05/13 11:26:01 INFO BlockManager: Removing RDD 1893
15/05/13 11:26:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530700000 ms: 1431530640000 ms
15/05/13 11:26:01 INFO JobGenerator: Checkpointing graph for time 1431530760000 ms
15/05/13 11:26:01 INFO DStreamGraph: Updating checkpoint data for time 1431530760000 ms
15/05/13 11:26:01 INFO DStreamGraph: Updated checkpoint data for time 1431530760000 ms
15/05/13 11:26:01 INFO CheckpointWriter: Saving checkpoint for time 1431530760000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000'
15/05/13 11:26:01 INFO CheckpointWriter: Checkpoint for time 1431530760000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530760000', took 7656 bytes and 46 ms
15/05/13 11:26:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530760000 ms
15/05/13 11:26:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530760000 ms
15/05/13 11:26:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:27:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 11:27:00 INFO FileInputDStream: New files at time 1431530820000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530746524.json
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=32285916, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_405 stored as values in memory (estimated size 232.9 KB, free 234.4 MB)
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=32524448, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_405_piece0 stored as bytes in memory (estimated size 34.9 KB, free 234.4 MB)
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.3 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_405_piece0
15/05/13 11:27:00 INFO SparkContext: Created broadcast 405 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:27:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:27:00 INFO JobScheduler: Starting job streaming job 1431530820000 ms.0 from job set of time 1431530820000 ms
15/05/13 11:27:00 INFO JobScheduler: Added jobs for time 1431530820000 ms
15/05/13 11:27:00 INFO JobGenerator: Checkpointing graph for time 1431530820000 ms
15/05/13 11:27:00 INFO DStreamGraph: Updating checkpoint data for time 1431530820000 ms
15/05/13 11:27:00 INFO DStreamGraph: Updated checkpoint data for time 1431530820000 ms
15/05/13 11:27:00 INFO CheckpointWriter: Saving checkpoint for time 1431530820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530820000'
15/05/13 11:27:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530520000.bk
15/05/13 11:27:00 INFO CheckpointWriter: Checkpoint for time 1431530820000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530820000', took 7665 bytes and 75 ms
15/05/13 11:27:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:27:00 INFO DAGScheduler: Got job 275 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:27:00 INFO DAGScheduler: Final stage: Stage 270(reduce at JsonRDD.scala:51)
15/05/13 11:27:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:27:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:27:00 INFO DAGScheduler: Submitting Stage 270 (MapPartitionsRDD[1926] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=32560156, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_406 stored as values in memory (estimated size 5.9 KB, free 234.4 MB)
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=32566196, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_406_piece0 stored as bytes in memory (estimated size 4.1 KB, free 234.3 MB)
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_406_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_406_piece0
15/05/13 11:27:00 INFO SparkContext: Created broadcast 406 from broadcast at DAGScheduler.scala:839
15/05/13 11:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 270 (MapPartitionsRDD[1926] at map at JsonRDD.scala:51)
15/05/13 11:27:00 INFO TaskSchedulerImpl: Adding task set 270.0 with 1 tasks
15/05/13 11:27:00 INFO TaskSetManager: Starting task 0.0 in stage 270.0 (TID 270, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_406_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO TaskSetManager: Finished task 0.0 in stage 270.0 (TID 270) in 409 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:27:00 INFO DAGScheduler: Stage 270 (reduce at JsonRDD.scala:51) finished in 0.417 s
15/05/13 11:27:00 INFO TaskSchedulerImpl: Removed TaskSet 270.0, whose tasks have all completed, from pool 
15/05/13 11:27:00 INFO DAGScheduler: Job 275 finished: reduce at JsonRDD.scala:51, took 0.447406 s
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 398
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_398
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_398 of size 20488 dropped from memory (free 245752622)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_398_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_398_piece0 of size 10757 dropped from memory (free 245763379)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_398_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 261.3 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_398_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_398_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 263.8 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 398
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 397
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_397
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_397 of size 6040 dropped from memory (free 245769419)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_397_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_397_piece0 of size 4225 dropped from memory (free 245773644)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_397_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_397_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_397_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 397
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 396
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_396
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_396 of size 238532 dropped from memory (free 246012176)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_396_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_396_piece0 of size 35708 dropped from memory (free 246047884)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_396_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_396_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_396_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 396
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 395
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_395
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_395 of size 20920 dropped from memory (free 246068804)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_395_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_395_piece0 of size 10888 dropped from memory (free 246079692)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_395_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_395_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_395_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 395
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 401
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_401
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_401 of size 21096 dropped from memory (free 246100788)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_401_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_401_piece0 of size 10958 dropped from memory (free 246111746)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_401_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_401_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_401_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 263.9 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 401
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 400
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_400_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_400_piece0 of size 4228 dropped from memory (free 246115974)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_400_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_400_piece0
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_400
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_400 of size 6040 dropped from memory (free 246122014)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_400_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 400
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 406
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_406
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_406 of size 6040 dropped from memory (free 246128054)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_406_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_406_piece0 of size 4226 dropped from memory (free 246132280)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_406_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_406_piece0
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_406_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 406
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 404
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_404
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_404 of size 21304 dropped from memory (free 246153584)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_404_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_404_piece0 of size 11105 dropped from memory (free 246164689)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_404_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_404_piece0
15/05/13 11:27:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_404_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.8 KB, free: 263.9 MB)
15/05/13 11:27:00 INFO DAGScheduler: Got job 276 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:27:00 INFO DAGScheduler: Final stage: Stage 271(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:27:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 404
15/05/13 11:27:00 INFO BlockManager: Removing broadcast 403
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_403
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_403 of size 6040 dropped from memory (free 246170729)
15/05/13 11:27:00 INFO BlockManager: Removing block broadcast_403_piece0
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_403_piece0 of size 4223 dropped from memory (free 246174952)
15/05/13 11:27:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:27:00 INFO DAGScheduler: Submitting Stage 271 (MapPartitionsRDD[1933] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_403_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_403_piece0
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=32127604, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_407 stored as values in memory (estimated size 20.0 KB, free 234.8 MB)
15/05/13 11:27:00 INFO BlockManagerInfo: Removed broadcast_403_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.6 MB)
15/05/13 11:27:00 INFO MemoryStore: ensureFreeSpace(10767) called with curMem=32148092, maxMem=278302556
15/05/13 11:27:00 INFO MemoryStore: Block broadcast_407_piece0 stored as bytes in memory (estimated size 10.5 KB, free 234.7 MB)
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_407_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 261.4 MB)
15/05/13 11:27:00 INFO BlockManagerMaster: Updated info of block broadcast_407_piece0
15/05/13 11:27:00 INFO SparkContext: Created broadcast 407 from broadcast at DAGScheduler.scala:839
15/05/13 11:27:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 271 (MapPartitionsRDD[1933] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:27:00 INFO TaskSchedulerImpl: Adding task set 271.0 with 1 tasks
15/05/13 11:27:00 INFO TaskSetManager: Starting task 0.0 in stage 271.0 (TID 271, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:27:00 INFO ContextCleaner: Cleaned broadcast 403
15/05/13 11:27:00 INFO BlockManagerInfo: Added broadcast_407_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 263.9 MB)
15/05/13 11:27:01 INFO BlockManagerInfo: Added broadcast_405_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:27:01 INFO TaskSetManager: Finished task 0.0 in stage 271.0 (TID 271) in 250 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:27:01 INFO TaskSchedulerImpl: Removed TaskSet 271.0, whose tasks have all completed, from pool 
15/05/13 11:27:01 INFO DAGScheduler: Stage 271 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.251 s
15/05/13 11:27:01 INFO DAGScheduler: Job 276 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.276894 s
15/05/13 11:27:01 INFO JobScheduler: Finished job streaming job 1431530820000 ms.0 from job set of time 1431530820000 ms
15/05/13 11:27:01 INFO JobScheduler: Total delay: 1.259 s for time 1431530820000 ms (execution: 1.138 s)
15/05/13 11:27:01 INFO MapPartitionsRDD: Removing RDD 1908 from persistence list
15/05/13 11:27:01 INFO BlockManager: Removing RDD 1908
15/05/13 11:27:01 INFO UnionRDD: Removing RDD 1907 from persistence list
15/05/13 11:27:01 INFO BlockManager: Removing RDD 1907
15/05/13 11:27:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530760000 ms: 1431530700000 ms
15/05/13 11:27:01 INFO JobGenerator: Checkpointing graph for time 1431530820000 ms
15/05/13 11:27:01 INFO DStreamGraph: Updating checkpoint data for time 1431530820000 ms
15/05/13 11:27:01 INFO DStreamGraph: Updated checkpoint data for time 1431530820000 ms
15/05/13 11:27:01 INFO CheckpointWriter: Saving checkpoint for time 1431530820000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530820000'
15/05/13 11:27:01 INFO CheckpointWriter: Checkpoint for time 1431530820000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530820000', took 7657 bytes and 61 ms
15/05/13 11:27:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530820000 ms
15/05/13 11:27:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530820000 ms
15/05/13 11:27:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:28:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 11:28:00 INFO FileInputDStream: New files at time 1431530880000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530806818.json
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=32158859, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_408 stored as values in memory (estimated size 232.9 KB, free 234.5 MB)
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=32397391, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_408_piece0 stored as bytes in memory (estimated size 34.9 KB, free 234.5 MB)
15/05/13 11:28:00 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.4 MB)
15/05/13 11:28:00 INFO BlockManagerMaster: Updated info of block broadcast_408_piece0
15/05/13 11:28:00 INFO SparkContext: Created broadcast 408 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:28:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:28:00 INFO JobScheduler: Added jobs for time 1431530880000 ms
15/05/13 11:28:00 INFO JobGenerator: Checkpointing graph for time 1431530880000 ms
15/05/13 11:28:00 INFO JobScheduler: Starting job streaming job 1431530880000 ms.0 from job set of time 1431530880000 ms
15/05/13 11:28:00 INFO DStreamGraph: Updating checkpoint data for time 1431530880000 ms
15/05/13 11:28:00 INFO DStreamGraph: Updated checkpoint data for time 1431530880000 ms
15/05/13 11:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431530880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000'
15/05/13 11:28:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84016): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431530880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000'
15/05/13 11:28:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:28:00 INFO DAGScheduler: Got job 277 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:28:00 INFO DAGScheduler: Final stage: Stage 272(reduce at JsonRDD.scala:51)
15/05/13 11:28:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:28:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:28:00 INFO DAGScheduler: Submitting Stage 272 (MapPartitionsRDD[1940] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=32433099, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_409 stored as values in memory (estimated size 5.9 KB, free 234.5 MB)
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(4233) called with curMem=32439139, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_409_piece0 stored as bytes in memory (estimated size 4.1 KB, free 234.5 MB)
15/05/13 11:28:00 INFO BlockManagerInfo: Added broadcast_409_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.4 MB)
15/05/13 11:28:00 INFO BlockManagerMaster: Updated info of block broadcast_409_piece0
15/05/13 11:28:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84018): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:28:00 INFO SparkContext: Created broadcast 409 from broadcast at DAGScheduler.scala:839
15/05/13 11:28:00 INFO CheckpointWriter: Saving checkpoint for time 1431530880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000'
15/05/13 11:28:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 272 (MapPartitionsRDD[1940] at map at JsonRDD.scala:51)
15/05/13 11:28:00 INFO TaskSchedulerImpl: Adding task set 272.0 with 1 tasks
15/05/13 11:28:00 INFO TaskSetManager: Starting task 0.0 in stage 272.0 (TID 272, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:28:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530580000.bk
15/05/13 11:28:00 INFO CheckpointWriter: Checkpoint for time 1431530880000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000', took 7666 bytes and 163 ms
15/05/13 11:28:00 INFO BlockManagerInfo: Added broadcast_409_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:28:00 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:28:00 INFO TaskSetManager: Finished task 0.0 in stage 272.0 (TID 272) in 407 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:28:00 INFO TaskSchedulerImpl: Removed TaskSet 272.0, whose tasks have all completed, from pool 
15/05/13 11:28:00 INFO DAGScheduler: Stage 272 (reduce at JsonRDD.scala:51) finished in 0.424 s
15/05/13 11:28:00 INFO DAGScheduler: Job 277 finished: reduce at JsonRDD.scala:51, took 0.459865 s
15/05/13 11:28:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:28:00 INFO DAGScheduler: Got job 278 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:28:00 INFO DAGScheduler: Final stage: Stage 273(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:28:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:28:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:28:00 INFO DAGScheduler: Submitting Stage 273 (MapPartitionsRDD[1947] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=32443372, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_410 stored as values in memory (estimated size 20.6 KB, free 234.4 MB)
15/05/13 11:28:00 INFO MemoryStore: ensureFreeSpace(10944) called with curMem=32464468, maxMem=278302556
15/05/13 11:28:00 INFO MemoryStore: Block broadcast_410_piece0 stored as bytes in memory (estimated size 10.7 KB, free 234.4 MB)
15/05/13 11:28:00 INFO BlockManagerInfo: Added broadcast_410_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.4 MB)
15/05/13 11:28:00 INFO BlockManagerMaster: Updated info of block broadcast_410_piece0
15/05/13 11:28:00 INFO SparkContext: Created broadcast 410 from broadcast at DAGScheduler.scala:839
15/05/13 11:28:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 273 (MapPartitionsRDD[1947] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:28:00 INFO TaskSchedulerImpl: Adding task set 273.0 with 1 tasks
15/05/13 11:28:00 INFO TaskSetManager: Starting task 0.0 in stage 273.0 (TID 273, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:28:01 INFO BlockManagerInfo: Added broadcast_410_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 262.6 MB)
15/05/13 11:28:01 INFO BlockManagerInfo: Added broadcast_408_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.5 MB)
15/05/13 11:28:01 INFO DAGScheduler: Stage 273 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.297 s
15/05/13 11:28:01 INFO TaskSetManager: Finished task 0.0 in stage 273.0 (TID 273) in 294 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:28:01 INFO TaskSchedulerImpl: Removed TaskSet 273.0, whose tasks have all completed, from pool 
15/05/13 11:28:01 INFO DAGScheduler: Job 278 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.328637 s
15/05/13 11:28:01 INFO JobScheduler: Finished job streaming job 1431530880000 ms.0 from job set of time 1431530880000 ms
15/05/13 11:28:01 INFO MapPartitionsRDD: Removing RDD 1922 from persistence list
15/05/13 11:28:01 INFO JobScheduler: Total delay: 1.347 s for time 1431530880000 ms (execution: 1.119 s)
15/05/13 11:28:01 INFO BlockManager: Removing RDD 1922
15/05/13 11:28:01 INFO UnionRDD: Removing RDD 1921 from persistence list
15/05/13 11:28:01 INFO BlockManager: Removing RDD 1921
15/05/13 11:28:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530820000 ms: 1431530760000 ms
15/05/13 11:28:01 INFO JobGenerator: Checkpointing graph for time 1431530880000 ms
15/05/13 11:28:01 INFO DStreamGraph: Updating checkpoint data for time 1431530880000 ms
15/05/13 11:28:01 INFO DStreamGraph: Updated checkpoint data for time 1431530880000 ms
15/05/13 11:28:01 INFO CheckpointWriter: Saving checkpoint for time 1431530880000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000'
15/05/13 11:28:01 INFO CheckpointWriter: Checkpoint for time 1431530880000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530880000', took 7655 bytes and 59 ms
15/05/13 11:28:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530880000 ms
15/05/13 11:28:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530880000 ms
15/05/13 11:28:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:29:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 11:29:00 INFO FileInputDStream: New files at time 1431530940000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530867173.json
15/05/13 11:29:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=32475412, maxMem=278302556
15/05/13 11:29:00 INFO MemoryStore: Block broadcast_411 stored as values in memory (estimated size 232.9 KB, free 234.2 MB)
15/05/13 11:29:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=32713944, maxMem=278302556
15/05/13 11:29:00 INFO MemoryStore: Block broadcast_411_piece0 stored as bytes in memory (estimated size 34.9 KB, free 234.2 MB)
15/05/13 11:29:00 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.3 MB)
15/05/13 11:29:00 INFO BlockManagerMaster: Updated info of block broadcast_411_piece0
15/05/13 11:29:00 INFO SparkContext: Created broadcast 411 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:29:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:29:00 INFO JobScheduler: Added jobs for time 1431530940000 ms
15/05/13 11:29:00 INFO JobScheduler: Starting job streaming job 1431530940000 ms.0 from job set of time 1431530940000 ms
15/05/13 11:29:00 INFO JobGenerator: Checkpointing graph for time 1431530940000 ms
15/05/13 11:29:00 INFO DStreamGraph: Updating checkpoint data for time 1431530940000 ms
15/05/13 11:29:00 INFO DStreamGraph: Updated checkpoint data for time 1431530940000 ms
15/05/13 11:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431530940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000'
15/05/13 11:29:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84030): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:29:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431530940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000'
15/05/13 11:29:00 INFO DAGScheduler: Got job 279 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:29:00 INFO DAGScheduler: Final stage: Stage 274(reduce at JsonRDD.scala:51)
15/05/13 11:29:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:29:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:29:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84032): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:29:00 INFO DAGScheduler: Submitting Stage 274 (MapPartitionsRDD[1954] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:29:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84032): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:29:00 INFO CheckpointWriter: Saving checkpoint for time 1431530940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000'
15/05/13 11:29:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=32749652, maxMem=278302556
15/05/13 11:29:00 INFO MemoryStore: Block broadcast_412 stored as values in memory (estimated size 5.9 KB, free 234.2 MB)
15/05/13 11:29:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=32755692, maxMem=278302556
15/05/13 11:29:00 INFO MemoryStore: Block broadcast_412_piece0 stored as bytes in memory (estimated size 4.1 KB, free 234.2 MB)
15/05/13 11:29:00 INFO BlockManagerInfo: Added broadcast_412_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:29:00 INFO BlockManagerMaster: Updated info of block broadcast_412_piece0
15/05/13 11:29:00 INFO SparkContext: Created broadcast 412 from broadcast at DAGScheduler.scala:839
15/05/13 11:29:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 274 (MapPartitionsRDD[1954] at map at JsonRDD.scala:51)
15/05/13 11:29:00 INFO TaskSchedulerImpl: Adding task set 274.0 with 1 tasks
15/05/13 11:29:00 INFO TaskSetManager: Starting task 0.0 in stage 274.0 (TID 274, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:29:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84034): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:29:00 WARN CheckpointWriter: Could not write checkpoint for time 1431530940000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000'
15/05/13 11:29:00 INFO BlockManagerInfo: Added broadcast_412_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:29:00 INFO BlockManagerInfo: Added broadcast_411_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:29:00 INFO TaskSetManager: Finished task 0.0 in stage 274.0 (TID 274) in 363 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:29:00 INFO TaskSchedulerImpl: Removed TaskSet 274.0, whose tasks have all completed, from pool 
15/05/13 11:29:00 INFO DAGScheduler: Stage 274 (reduce at JsonRDD.scala:51) finished in 0.376 s
15/05/13 11:29:00 INFO DAGScheduler: Job 279 finished: reduce at JsonRDD.scala:51, took 0.428144 s
15/05/13 11:29:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:29:01 INFO DAGScheduler: Got job 280 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:29:01 INFO DAGScheduler: Final stage: Stage 275(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:29:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:29:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:29:01 INFO DAGScheduler: Submitting Stage 275 (MapPartitionsRDD[1961] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:29:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=32759919, maxMem=278302556
15/05/13 11:29:01 INFO MemoryStore: Block broadcast_413 stored as values in memory (estimated size 20.8 KB, free 234.1 MB)
15/05/13 11:29:01 INFO MemoryStore: ensureFreeSpace(11134) called with curMem=32781223, maxMem=278302556
15/05/13 11:29:01 INFO MemoryStore: Block broadcast_413_piece0 stored as bytes in memory (estimated size 10.9 KB, free 234.1 MB)
15/05/13 11:29:01 INFO BlockManagerInfo: Added broadcast_413_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.3 MB)
15/05/13 11:29:01 INFO BlockManagerMaster: Updated info of block broadcast_413_piece0
15/05/13 11:29:01 INFO SparkContext: Created broadcast 413 from broadcast at DAGScheduler.scala:839
15/05/13 11:29:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 275 (MapPartitionsRDD[1961] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:29:01 INFO TaskSchedulerImpl: Adding task set 275.0 with 1 tasks
15/05/13 11:29:01 INFO TaskSetManager: Starting task 0.0 in stage 275.0 (TID 275, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:29:01 INFO BlockManagerInfo: Added broadcast_413_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:29:01 INFO TaskSetManager: Finished task 0.0 in stage 275.0 (TID 275) in 124 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:29:01 INFO TaskSchedulerImpl: Removed TaskSet 275.0, whose tasks have all completed, from pool 
15/05/13 11:29:01 INFO DAGScheduler: Stage 275 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.127 s
15/05/13 11:29:01 INFO DAGScheduler: Job 280 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.168686 s
15/05/13 11:29:01 INFO JobScheduler: Finished job streaming job 1431530940000 ms.0 from job set of time 1431530940000 ms
15/05/13 11:29:01 INFO JobScheduler: Total delay: 1.223 s for time 1431530940000 ms (execution: 1.035 s)
15/05/13 11:29:01 INFO MapPartitionsRDD: Removing RDD 1936 from persistence list
15/05/13 11:29:01 INFO BlockManager: Removing RDD 1936
15/05/13 11:29:01 INFO UnionRDD: Removing RDD 1935 from persistence list
15/05/13 11:29:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530880000 ms: 1431530820000 ms
15/05/13 11:29:01 INFO BlockManager: Removing RDD 1935
15/05/13 11:29:01 INFO JobGenerator: Checkpointing graph for time 1431530940000 ms
15/05/13 11:29:01 INFO DStreamGraph: Updating checkpoint data for time 1431530940000 ms
15/05/13 11:29:01 INFO DStreamGraph: Updated checkpoint data for time 1431530940000 ms
15/05/13 11:29:01 INFO CheckpointWriter: Saving checkpoint for time 1431530940000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000'
15/05/13 11:29:01 INFO CheckpointWriter: Checkpoint for time 1431530940000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530940000', took 7654 bytes and 54 ms
15/05/13 11:29:01 INFO DStreamGraph: Clearing checkpoint data for time 1431530940000 ms
15/05/13 11:29:01 INFO DStreamGraph: Cleared checkpoint data for time 1431530940000 ms
15/05/13 11:29:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:30:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 11:30:00 INFO FileInputDStream: New files at time 1431531000000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530938074.json
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=32792357, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_414 stored as values in memory (estimated size 232.9 KB, free 233.9 MB)
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=33030889, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_414_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.9 MB)
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.3 MB)
15/05/13 11:30:00 INFO BlockManagerMaster: Updated info of block broadcast_414_piece0
15/05/13 11:30:00 INFO SparkContext: Created broadcast 414 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:30:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:30:00 INFO JobScheduler: Added jobs for time 1431531000000 ms
15/05/13 11:30:00 INFO JobGenerator: Checkpointing graph for time 1431531000000 ms
15/05/13 11:30:00 INFO DStreamGraph: Updating checkpoint data for time 1431531000000 ms
15/05/13 11:30:00 INFO DStreamGraph: Updated checkpoint data for time 1431531000000 ms
15/05/13 11:30:00 INFO JobScheduler: Starting job streaming job 1431531000000 ms.0 from job set of time 1431531000000 ms
15/05/13 11:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431531000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000'
15/05/13 11:30:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84041): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431531000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000'
15/05/13 11:30:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:30:00 INFO DAGScheduler: Got job 281 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:30:00 INFO DAGScheduler: Final stage: Stage 276(reduce at JsonRDD.scala:51)
15/05/13 11:30:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:30:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84043): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:30:00 INFO CheckpointWriter: Saving checkpoint for time 1431531000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000'
15/05/13 11:30:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:30:00 INFO DAGScheduler: Submitting Stage 276 (MapPartitionsRDD[1968] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=33066597, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_415 stored as values in memory (estimated size 5.9 KB, free 233.9 MB)
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=33072637, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_415_piece0 stored as bytes in memory (estimated size 4.1 KB, free 233.9 MB)
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_415_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:30:00 INFO BlockManagerMaster: Updated info of block broadcast_415_piece0
15/05/13 11:30:00 INFO SparkContext: Created broadcast 415 from broadcast at DAGScheduler.scala:839
15/05/13 11:30:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 276 (MapPartitionsRDD[1968] at map at JsonRDD.scala:51)
15/05/13 11:30:00 INFO TaskSchedulerImpl: Adding task set 276.0 with 1 tasks
15/05/13 11:30:00 INFO TaskSetManager: Starting task 0.0 in stage 276.0 (TID 276, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_415_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:30:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431530700000.bk
15/05/13 11:30:00 INFO CheckpointWriter: Checkpoint for time 1431531000000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000', took 7665 bytes and 217 ms
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.9 MB)
15/05/13 11:30:00 INFO TaskSetManager: Finished task 0.0 in stage 276.0 (TID 276) in 281 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:30:00 INFO DAGScheduler: Stage 276 (reduce at JsonRDD.scala:51) finished in 0.293 s
15/05/13 11:30:00 INFO TaskSchedulerImpl: Removed TaskSet 276.0, whose tasks have all completed, from pool 
15/05/13 11:30:00 INFO DAGScheduler: Job 281 finished: reduce at JsonRDD.scala:51, took 0.339843 s
15/05/13 11:30:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:30:00 INFO DAGScheduler: Got job 282 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:30:00 INFO DAGScheduler: Final stage: Stage 277(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:30:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:30:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:30:00 INFO DAGScheduler: Submitting Stage 277 (MapPartitionsRDD[1975] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(21504) called with curMem=33076862, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_416 stored as values in memory (estimated size 21.0 KB, free 233.8 MB)
15/05/13 11:30:00 INFO MemoryStore: ensureFreeSpace(11228) called with curMem=33098366, maxMem=278302556
15/05/13 11:30:00 INFO MemoryStore: Block broadcast_416_piece0 stored as bytes in memory (estimated size 11.0 KB, free 233.8 MB)
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_416_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 261.3 MB)
15/05/13 11:30:00 INFO BlockManagerMaster: Updated info of block broadcast_416_piece0
15/05/13 11:30:00 INFO SparkContext: Created broadcast 416 from broadcast at DAGScheduler.scala:839
15/05/13 11:30:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 277 (MapPartitionsRDD[1975] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:30:00 INFO TaskSchedulerImpl: Adding task set 277.0 with 1 tasks
15/05/13 11:30:00 INFO TaskSetManager: Starting task 0.0 in stage 277.0 (TID 277, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:30:00 INFO BlockManagerInfo: Added broadcast_416_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 262.5 MB)
15/05/13 11:30:01 INFO BlockManagerInfo: Added broadcast_414_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.5 MB)
15/05/13 11:30:01 INFO TaskSetManager: Finished task 0.0 in stage 277.0 (TID 277) in 343 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:30:01 INFO TaskSchedulerImpl: Removed TaskSet 277.0, whose tasks have all completed, from pool 
15/05/13 11:30:01 INFO DAGScheduler: Stage 277 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.344 s
15/05/13 11:30:01 INFO DAGScheduler: Job 282 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.378387 s
15/05/13 11:30:01 INFO JobScheduler: Finished job streaming job 1431531000000 ms.0 from job set of time 1431531000000 ms
15/05/13 11:30:01 INFO JobScheduler: Total delay: 1.349 s for time 1431531000000 ms (execution: 1.155 s)
15/05/13 11:30:01 INFO MapPartitionsRDD: Removing RDD 1950 from persistence list
15/05/13 11:30:01 INFO BlockManager: Removing RDD 1950
15/05/13 11:30:01 INFO UnionRDD: Removing RDD 1949 from persistence list
15/05/13 11:30:01 INFO BlockManager: Removing RDD 1949
15/05/13 11:30:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431530940000 ms: 1431530880000 ms
15/05/13 11:30:01 INFO JobGenerator: Checkpointing graph for time 1431531000000 ms
15/05/13 11:30:01 INFO DStreamGraph: Updating checkpoint data for time 1431531000000 ms
15/05/13 11:30:01 INFO DStreamGraph: Updated checkpoint data for time 1431531000000 ms
15/05/13 11:30:01 INFO CheckpointWriter: Saving checkpoint for time 1431531000000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000'
15/05/13 11:30:01 INFO CheckpointWriter: Checkpoint for time 1431531000000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531000000', took 7657 bytes and 114 ms
15/05/13 11:30:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531000000 ms
15/05/13 11:30:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531000000 ms
15/05/13 11:30:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:31:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 11:31:00 INFO FileInputDStream: New files at time 1431531060000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431530999176.json
15/05/13 11:31:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=33109594, maxMem=278302556
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_417 stored as values in memory (estimated size 232.9 KB, free 233.6 MB)
15/05/13 11:31:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=33348126, maxMem=278302556
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_417_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.6 MB)
15/05/13 11:31:00 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.2 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_417_piece0
15/05/13 11:31:00 INFO SparkContext: Created broadcast 417 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 416
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_416_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_416_piece0 of size 11228 dropped from memory (free 244929950)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_416_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 261.2 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_416_piece0
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_416
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_416 of size 21504 dropped from memory (free 244951454)
15/05/13 11:31:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_416_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 262.5 MB)
15/05/13 11:31:00 INFO JobScheduler: Added jobs for time 1431531060000 ms
15/05/13 11:31:00 INFO JobGenerator: Checkpointing graph for time 1431531060000 ms
15/05/13 11:31:00 INFO DStreamGraph: Updating checkpoint data for time 1431531060000 ms
15/05/13 11:31:00 INFO DStreamGraph: Updated checkpoint data for time 1431531060000 ms
15/05/13 11:31:00 INFO JobScheduler: Starting job streaming job 1431531060000 ms.0 from job set of time 1431531060000 ms
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 416
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 407
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_407_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_407_piece0 of size 10767 dropped from memory (free 244962221)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_407_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 261.2 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_407_piece0
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_407
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_407 of size 20488 dropped from memory (free 244982709)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_407_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 263.7 MB)
15/05/13 11:31:00 INFO CheckpointWriter: Saving checkpoint for time 1431531060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000'
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 407
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 415
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_415_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_415_piece0 of size 4225 dropped from memory (free 244986934)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_415_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_415_piece0
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_415
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_415 of size 6040 dropped from memory (free 244992974)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_415_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 415
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 413
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_413
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_413 of size 21304 dropped from memory (free 245014278)
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_413_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_413_piece0 of size 11134 dropped from memory (free 245025412)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_413_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_413_piece0
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_413_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.9 KB, free: 263.8 MB)
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 413
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 412
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_412_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_412_piece0 of size 4227 dropped from memory (free 245029639)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_412_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_412_piece0
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_412
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_412 of size 6040 dropped from memory (free 245035679)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_412_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 412
15/05/13 11:31:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84049): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 411
15/05/13 11:31:00 INFO CheckpointWriter: Saving checkpoint for time 1431531060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000'
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_411
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_411 of size 238532 dropped from memory (free 245274211)
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_411_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_411_piece0 of size 35708 dropped from memory (free 245309919)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_411_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_411_piece0
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_411_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 411
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 410
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_410_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_410_piece0 of size 10944 dropped from memory (free 245320863)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_410_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_410_piece0
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_410_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 262.5 MB)
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_410
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_410 of size 21096 dropped from memory (free 245341959)
15/05/13 11:31:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 410
15/05/13 11:31:00 INFO DAGScheduler: Got job 283 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:31:00 INFO DAGScheduler: Final stage: Stage 278(reduce at JsonRDD.scala:51)
15/05/13 11:31:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:31:00 INFO BlockManager: Removing broadcast 409
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_409_piece0
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_409_piece0 of size 4233 dropped from memory (free 245346192)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_409_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_409_piece0
15/05/13 11:31:00 INFO BlockManager: Removing block broadcast_409
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_409 of size 6040 dropped from memory (free 245352232)
15/05/13 11:31:00 INFO BlockManagerInfo: Removed broadcast_409_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:31:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84051): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:31:00 INFO CheckpointWriter: Saving checkpoint for time 1431531060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000'
15/05/13 11:31:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:31:00 INFO DAGScheduler: Submitting Stage 278 (MapPartitionsRDD[1982] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:31:00 INFO ContextCleaner: Cleaned broadcast 409
15/05/13 11:31:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=32950324, maxMem=278302556
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_418 stored as values in memory (estimated size 5.9 KB, free 234.0 MB)
15/05/13 11:31:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=32956364, maxMem=278302556
15/05/13 11:31:00 INFO MemoryStore: Block broadcast_418_piece0 stored as bytes in memory (estimated size 4.1 KB, free 234.0 MB)
15/05/13 11:31:00 INFO BlockManagerInfo: Added broadcast_418_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:31:00 INFO BlockManagerMaster: Updated info of block broadcast_418_piece0
15/05/13 11:31:00 INFO SparkContext: Created broadcast 418 from broadcast at DAGScheduler.scala:839
15/05/13 11:31:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 278 (MapPartitionsRDD[1982] at map at JsonRDD.scala:51)
15/05/13 11:31:00 INFO TaskSchedulerImpl: Adding task set 278.0 with 1 tasks
15/05/13 11:31:00 INFO TaskSetManager: Starting task 0.0 in stage 278.0 (TID 278, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:31:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84053): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:31:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531060000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000'
15/05/13 11:31:00 INFO BlockManagerInfo: Added broadcast_418_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:31:00 INFO BlockManagerInfo: Added broadcast_417_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:31:00 INFO TaskSetManager: Finished task 0.0 in stage 278.0 (TID 278) in 335 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:31:00 INFO TaskSchedulerImpl: Removed TaskSet 278.0, whose tasks have all completed, from pool 
15/05/13 11:31:00 INFO DAGScheduler: Stage 278 (reduce at JsonRDD.scala:51) finished in 0.351 s
15/05/13 11:31:00 INFO DAGScheduler: Job 283 finished: reduce at JsonRDD.scala:51, took 0.398787 s
15/05/13 11:31:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:31:01 INFO DAGScheduler: Got job 284 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:31:01 INFO DAGScheduler: Final stage: Stage 279(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:31:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:31:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:31:01 INFO DAGScheduler: Submitting Stage 279 (MapPartitionsRDD[1989] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:31:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=32960589, maxMem=278302556
15/05/13 11:31:01 INFO MemoryStore: Block broadcast_419 stored as values in memory (estimated size 20.4 KB, free 234.0 MB)
15/05/13 11:31:01 INFO MemoryStore: ensureFreeSpace(10830) called with curMem=32981509, maxMem=278302556
15/05/13 11:31:01 INFO MemoryStore: Block broadcast_419_piece0 stored as bytes in memory (estimated size 10.6 KB, free 233.9 MB)
15/05/13 11:31:01 INFO BlockManagerInfo: Added broadcast_419_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.3 MB)
15/05/13 11:31:01 INFO BlockManagerMaster: Updated info of block broadcast_419_piece0
15/05/13 11:31:01 INFO SparkContext: Created broadcast 419 from broadcast at DAGScheduler.scala:839
15/05/13 11:31:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 279 (MapPartitionsRDD[1989] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:31:01 INFO TaskSchedulerImpl: Adding task set 279.0 with 1 tasks
15/05/13 11:31:01 INFO TaskSetManager: Starting task 0.0 in stage 279.0 (TID 279, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:31:01 INFO BlockManagerInfo: Added broadcast_419_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 263.7 MB)
15/05/13 11:31:01 INFO TaskSetManager: Finished task 0.0 in stage 279.0 (TID 279) in 151 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:31:01 INFO TaskSchedulerImpl: Removed TaskSet 279.0, whose tasks have all completed, from pool 
15/05/13 11:31:01 INFO DAGScheduler: Stage 279 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.153 s
15/05/13 11:31:01 INFO DAGScheduler: Job 284 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.178814 s
15/05/13 11:31:01 INFO JobScheduler: Finished job streaming job 1431531060000 ms.0 from job set of time 1431531060000 ms
15/05/13 11:31:01 INFO JobScheduler: Total delay: 1.318 s for time 1431531060000 ms (execution: 1.010 s)
15/05/13 11:31:01 INFO MapPartitionsRDD: Removing RDD 1964 from persistence list
15/05/13 11:31:01 INFO BlockManager: Removing RDD 1964
15/05/13 11:31:01 INFO UnionRDD: Removing RDD 1963 from persistence list
15/05/13 11:31:01 INFO BlockManager: Removing RDD 1963
15/05/13 11:31:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531000000 ms: 1431530940000 ms
15/05/13 11:31:01 INFO JobGenerator: Checkpointing graph for time 1431531060000 ms
15/05/13 11:31:01 INFO DStreamGraph: Updating checkpoint data for time 1431531060000 ms
15/05/13 11:31:01 INFO DStreamGraph: Updated checkpoint data for time 1431531060000 ms
15/05/13 11:31:01 INFO CheckpointWriter: Saving checkpoint for time 1431531060000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000'
15/05/13 11:31:01 INFO CheckpointWriter: Checkpoint for time 1431531060000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000', took 7654 bytes and 46 ms
15/05/13 11:31:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531060000 ms
15/05/13 11:31:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531060000 ms
15/05/13 11:31:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:32:00 INFO FileInputDStream: Finding new files took 55 ms
15/05/13 11:32:00 INFO FileInputDStream: New files at time 1431531120000 ms:

15/05/13 11:32:00 INFO JobScheduler: Added jobs for time 1431531120000 ms
15/05/13 11:32:00 INFO JobGenerator: Checkpointing graph for time 1431531120000 ms
15/05/13 11:32:00 INFO JobScheduler: Starting job streaming job 1431531120000 ms.0 from job set of time 1431531120000 ms
15/05/13 11:32:00 INFO DStreamGraph: Updating checkpoint data for time 1431531120000 ms
15/05/13 11:32:00 INFO DStreamGraph: Updated checkpoint data for time 1431531120000 ms
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84058): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:32:00 INFO DAGScheduler: Job 285 finished: reduce at JsonRDD.scala:51, took 0.000250 s
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 11:32:00 INFO JobScheduler: Finished job streaming job 1431531120000 ms.0 from job set of time 1431531120000 ms
15/05/13 11:32:00 INFO JobScheduler: Total delay: 0.222 s for time 1431531120000 ms (execution: 0.141 s)
15/05/13 11:32:00 INFO MapPartitionsRDD: Removing RDD 1978 from persistence list
15/05/13 11:32:00 INFO BlockManager: Removing RDD 1978
15/05/13 11:32:00 INFO UnionRDD: Removing RDD 1977 from persistence list
15/05/13 11:32:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431531060000 ms: 1431531000000 ms
15/05/13 11:32:00 INFO BlockManager: Removing RDD 1977
15/05/13 11:32:00 INFO JobGenerator: Checkpointing graph for time 1431531120000 ms
15/05/13 11:32:00 INFO DStreamGraph: Updating checkpoint data for time 1431531120000 ms
15/05/13 11:32:00 INFO DStreamGraph: Updated checkpoint data for time 1431531120000 ms
15/05/13 11:32:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84060): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84060): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84062): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:32:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531120000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84063): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84063): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84065): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:32:00 INFO CheckpointWriter: Saving checkpoint for time 1431531120000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:32:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84066): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84066): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:32:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531120000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531120000'
15/05/13 11:33:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 11:33:00 INFO FileInputDStream: New files at time 1431531180000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531063985.json
15/05/13 11:33:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=32992339, maxMem=278302556
15/05/13 11:33:00 INFO MemoryStore: Block broadcast_420 stored as values in memory (estimated size 232.9 KB, free 233.7 MB)
15/05/13 11:33:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=33230871, maxMem=278302556
15/05/13 11:33:00 INFO MemoryStore: Block broadcast_420_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.7 MB)
15/05/13 11:33:00 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.3 MB)
15/05/13 11:33:00 INFO BlockManagerMaster: Updated info of block broadcast_420_piece0
15/05/13 11:33:00 INFO SparkContext: Created broadcast 420 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:33:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:33:00 INFO JobScheduler: Added jobs for time 1431531180000 ms
15/05/13 11:33:00 INFO JobScheduler: Starting job streaming job 1431531180000 ms.0 from job set of time 1431531180000 ms
15/05/13 11:33:00 INFO JobGenerator: Checkpointing graph for time 1431531180000 ms
15/05/13 11:33:00 INFO DStreamGraph: Updating checkpoint data for time 1431531180000 ms
15/05/13 11:33:00 INFO DStreamGraph: Updated checkpoint data for time 1431531180000 ms
15/05/13 11:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431531180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000'
15/05/13 11:33:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84070): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431531180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000'
15/05/13 11:33:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:33:00 INFO DAGScheduler: Got job 286 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:33:00 INFO DAGScheduler: Final stage: Stage 280(reduce at JsonRDD.scala:51)
15/05/13 11:33:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:33:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:33:00 INFO DAGScheduler: Submitting Stage 280 (MapPartitionsRDD[2002] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:33:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=33266579, maxMem=278302556
15/05/13 11:33:00 INFO MemoryStore: Block broadcast_421 stored as values in memory (estimated size 5.9 KB, free 233.7 MB)
15/05/13 11:33:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=33272619, maxMem=278302556
15/05/13 11:33:00 INFO MemoryStore: Block broadcast_421_piece0 stored as bytes in memory (estimated size 4.1 KB, free 233.7 MB)
15/05/13 11:33:00 INFO BlockManagerInfo: Added broadcast_421_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.3 MB)
15/05/13 11:33:00 INFO BlockManagerMaster: Updated info of block broadcast_421_piece0
15/05/13 11:33:00 INFO SparkContext: Created broadcast 421 from broadcast at DAGScheduler.scala:839
15/05/13 11:33:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84072): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:33:00 INFO CheckpointWriter: Saving checkpoint for time 1431531180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000'
15/05/13 11:33:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 280 (MapPartitionsRDD[2002] at map at JsonRDD.scala:51)
15/05/13 11:33:00 INFO TaskSchedulerImpl: Adding task set 280.0 with 1 tasks
15/05/13 11:33:00 INFO TaskSetManager: Starting task 0.0 in stage 280.0 (TID 280, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:33:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84074): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:33:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531180000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000'
15/05/13 11:33:00 INFO BlockManagerInfo: Added broadcast_421_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:33:00 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:33:00 INFO DAGScheduler: Stage 280 (reduce at JsonRDD.scala:51) finished in 0.361 s
15/05/13 11:33:00 INFO TaskSetManager: Finished task 0.0 in stage 280.0 (TID 280) in 351 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:33:00 INFO DAGScheduler: Job 286 finished: reduce at JsonRDD.scala:51, took 0.438333 s
15/05/13 11:33:00 INFO TaskSchedulerImpl: Removed TaskSet 280.0, whose tasks have all completed, from pool 
15/05/13 11:33:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:33:01 INFO DAGScheduler: Got job 287 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:33:01 INFO DAGScheduler: Final stage: Stage 281(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:33:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:33:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:33:01 INFO DAGScheduler: Submitting Stage 281 (MapPartitionsRDD[2009] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:33:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=33276844, maxMem=278302556
15/05/13 11:33:01 INFO MemoryStore: Block broadcast_422 stored as values in memory (estimated size 20.4 KB, free 233.7 MB)
15/05/13 11:33:01 INFO MemoryStore: ensureFreeSpace(10833) called with curMem=33297764, maxMem=278302556
15/05/13 11:33:01 INFO MemoryStore: Block broadcast_422_piece0 stored as bytes in memory (estimated size 10.6 KB, free 233.6 MB)
15/05/13 11:33:01 INFO BlockManagerInfo: Added broadcast_422_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.2 MB)
15/05/13 11:33:01 INFO BlockManagerMaster: Updated info of block broadcast_422_piece0
15/05/13 11:33:01 INFO SparkContext: Created broadcast 422 from broadcast at DAGScheduler.scala:839
15/05/13 11:33:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 281 (MapPartitionsRDD[2009] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:33:01 INFO TaskSchedulerImpl: Adding task set 281.0 with 1 tasks
15/05/13 11:33:01 INFO TaskSetManager: Starting task 0.0 in stage 281.0 (TID 281, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:33:01 INFO BlockManagerInfo: Added broadcast_422_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.5 MB)
15/05/13 11:33:01 INFO BlockManagerInfo: Added broadcast_420_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.5 MB)
15/05/13 11:33:01 INFO TaskSetManager: Finished task 0.0 in stage 281.0 (TID 281) in 371 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:33:01 INFO TaskSchedulerImpl: Removed TaskSet 281.0, whose tasks have all completed, from pool 
15/05/13 11:33:01 INFO DAGScheduler: Stage 281 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.372 s
15/05/13 11:33:01 INFO DAGScheduler: Job 287 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.410637 s
15/05/13 11:33:01 INFO JobScheduler: Finished job streaming job 1431531180000 ms.0 from job set of time 1431531180000 ms
15/05/13 11:33:01 INFO JobScheduler: Total delay: 1.477 s for time 1431531180000 ms (execution: 1.267 s)
15/05/13 11:33:01 INFO MapPartitionsRDD: Removing RDD 1991 from persistence list
15/05/13 11:33:01 INFO BlockManager: Removing RDD 1991
15/05/13 11:33:01 INFO UnionRDD: Removing RDD 1990 from persistence list
15/05/13 11:33:01 INFO BlockManager: Removing RDD 1990
15/05/13 11:33:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531120000 ms: 1431531060000 ms
15/05/13 11:33:01 INFO JobGenerator: Checkpointing graph for time 1431531180000 ms
15/05/13 11:33:01 INFO DStreamGraph: Updating checkpoint data for time 1431531180000 ms
15/05/13 11:33:01 INFO DStreamGraph: Updated checkpoint data for time 1431531180000 ms
15/05/13 11:33:01 INFO CheckpointWriter: Saving checkpoint for time 1431531180000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000'
15/05/13 11:33:01 INFO CheckpointWriter: Checkpoint for time 1431531180000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000', took 7647 bytes and 56 ms
15/05/13 11:33:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531180000 ms
15/05/13 11:33:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531180000 ms
15/05/13 11:33:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:34:00 INFO FileInputDStream: Finding new files took 44 ms
15/05/13 11:34:00 INFO FileInputDStream: New files at time 1431531240000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531127890.json
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=33308597, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_423 stored as values in memory (estimated size 232.9 KB, free 233.4 MB)
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=33547129, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_423_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.4 MB)
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.2 MB)
15/05/13 11:34:00 INFO BlockManagerMaster: Updated info of block broadcast_423_piece0
15/05/13 11:34:00 INFO SparkContext: Created broadcast 423 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:34:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:34:00 INFO JobScheduler: Starting job streaming job 1431531240000 ms.0 from job set of time 1431531240000 ms
15/05/13 11:34:00 INFO JobScheduler: Added jobs for time 1431531240000 ms
15/05/13 11:34:00 INFO JobGenerator: Checkpointing graph for time 1431531240000 ms
15/05/13 11:34:00 INFO DStreamGraph: Updating checkpoint data for time 1431531240000 ms
15/05/13 11:34:00 INFO DStreamGraph: Updated checkpoint data for time 1431531240000 ms
15/05/13 11:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431531240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000'
15/05/13 11:34:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84086): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431531240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000'
15/05/13 11:34:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:34:00 INFO DAGScheduler: Got job 288 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:34:00 INFO DAGScheduler: Final stage: Stage 282(reduce at JsonRDD.scala:51)
15/05/13 11:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:34:00 INFO DAGScheduler: Submitting Stage 282 (MapPartitionsRDD[2016] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=33582838, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_424 stored as values in memory (estimated size 5.9 KB, free 233.4 MB)
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=33588878, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_424_piece0 stored as bytes in memory (estimated size 4.1 KB, free 233.4 MB)
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_424_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:34:00 INFO BlockManagerMaster: Updated info of block broadcast_424_piece0
15/05/13 11:34:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84088): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:34:00 INFO CheckpointWriter: Saving checkpoint for time 1431531240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000'
15/05/13 11:34:00 INFO SparkContext: Created broadcast 424 from broadcast at DAGScheduler.scala:839
15/05/13 11:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 282 (MapPartitionsRDD[2016] at map at JsonRDD.scala:51)
15/05/13 11:34:00 INFO TaskSchedulerImpl: Adding task set 282.0 with 1 tasks
15/05/13 11:34:00 INFO TaskSetManager: Starting task 0.0 in stage 282.0 (TID 282, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:34:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84090): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:34:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531240000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000'
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_424_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:34:00 INFO TaskSetManager: Finished task 0.0 in stage 282.0 (TID 282) in 333 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:34:00 INFO TaskSchedulerImpl: Removed TaskSet 282.0, whose tasks have all completed, from pool 
15/05/13 11:34:00 INFO DAGScheduler: Stage 282 (reduce at JsonRDD.scala:51) finished in 0.340 s
15/05/13 11:34:00 INFO DAGScheduler: Job 288 finished: reduce at JsonRDD.scala:51, took 0.384898 s
15/05/13 11:34:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:34:00 INFO DAGScheduler: Got job 289 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:34:00 INFO DAGScheduler: Final stage: Stage 283(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:34:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:34:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:34:00 INFO DAGScheduler: Submitting Stage 283 (MapPartitionsRDD[2023] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=33593103, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_425 stored as values in memory (estimated size 20.4 KB, free 233.4 MB)
15/05/13 11:34:00 INFO MemoryStore: ensureFreeSpace(10844) called with curMem=33614023, maxMem=278302556
15/05/13 11:34:00 INFO MemoryStore: Block broadcast_425_piece0 stored as bytes in memory (estimated size 10.6 KB, free 233.3 MB)
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_425_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.2 MB)
15/05/13 11:34:00 INFO BlockManagerMaster: Updated info of block broadcast_425_piece0
15/05/13 11:34:00 INFO SparkContext: Created broadcast 425 from broadcast at DAGScheduler.scala:839
15/05/13 11:34:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 283 (MapPartitionsRDD[2023] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:34:00 INFO TaskSchedulerImpl: Adding task set 283.0 with 1 tasks
15/05/13 11:34:00 INFO TaskSetManager: Starting task 0.0 in stage 283.0 (TID 283, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:34:00 INFO BlockManagerInfo: Added broadcast_425_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.5 MB)
15/05/13 11:34:01 INFO BlockManagerInfo: Added broadcast_423_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.4 MB)
15/05/13 11:34:01 INFO TaskSetManager: Finished task 0.0 in stage 283.0 (TID 283) in 284 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:34:01 INFO TaskSchedulerImpl: Removed TaskSet 283.0, whose tasks have all completed, from pool 
15/05/13 11:34:01 INFO DAGScheduler: Stage 283 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.288 s
15/05/13 11:34:01 INFO DAGScheduler: Job 289 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.328966 s
15/05/13 11:34:01 INFO JobScheduler: Finished job streaming job 1431531240000 ms.0 from job set of time 1431531240000 ms
15/05/13 11:34:01 INFO JobScheduler: Total delay: 1.264 s for time 1431531240000 ms (execution: 1.072 s)
15/05/13 11:34:01 INFO MapPartitionsRDD: Removing RDD 1998 from persistence list
15/05/13 11:34:01 INFO BlockManager: Removing RDD 1998
15/05/13 11:34:01 INFO UnionRDD: Removing RDD 1997 from persistence list
15/05/13 11:34:01 INFO BlockManager: Removing RDD 1997
15/05/13 11:34:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531180000 ms: 1431531120000 ms
15/05/13 11:34:01 INFO JobGenerator: Checkpointing graph for time 1431531240000 ms
15/05/13 11:34:01 INFO DStreamGraph: Updating checkpoint data for time 1431531240000 ms
15/05/13 11:34:01 INFO DStreamGraph: Updated checkpoint data for time 1431531240000 ms
15/05/13 11:34:01 INFO CheckpointWriter: Saving checkpoint for time 1431531240000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000'
15/05/13 11:34:01 INFO CheckpointWriter: Checkpoint for time 1431531240000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000', took 7655 bytes and 62 ms
15/05/13 11:34:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531240000 ms
15/05/13 11:34:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531240000 ms
15/05/13 11:34:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:35:00 INFO FileInputDStream: Finding new files took 40 ms
15/05/13 11:35:00 INFO FileInputDStream: New files at time 1431531300000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531196552.json
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=33624867, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_426 stored as values in memory (estimated size 232.9 KB, free 233.1 MB)
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=33863399, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_426_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.1 MB)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_426_piece0
15/05/13 11:35:00 INFO SparkContext: Created broadcast 426 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:35:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:35:00 INFO JobScheduler: Added jobs for time 1431531300000 ms
15/05/13 11:35:00 INFO JobGenerator: Checkpointing graph for time 1431531300000 ms
15/05/13 11:35:00 INFO DStreamGraph: Updating checkpoint data for time 1431531300000 ms
15/05/13 11:35:00 INFO JobScheduler: Starting job streaming job 1431531300000 ms.0 from job set of time 1431531300000 ms
15/05/13 11:35:00 INFO DStreamGraph: Updated checkpoint data for time 1431531300000 ms
15/05/13 11:35:00 INFO CheckpointWriter: Saving checkpoint for time 1431531300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000'
15/05/13 11:35:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000
15/05/13 11:35:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:35:00 INFO DAGScheduler: Got job 290 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:35:00 INFO DAGScheduler: Final stage: Stage 284(reduce at JsonRDD.scala:51)
15/05/13 11:35:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:35:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:35:00 INFO DAGScheduler: Submitting Stage 284 (MapPartitionsRDD[2030] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=33899107, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_427 stored as values in memory (estimated size 5.9 KB, free 233.1 MB)
15/05/13 11:35:00 INFO CheckpointWriter: Checkpoint for time 1431531300000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000', took 7662 bytes and 77 ms
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=33905147, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_427_piece0 stored as bytes in memory (estimated size 4.1 KB, free 233.1 MB)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_427_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_427_piece0
15/05/13 11:35:00 INFO SparkContext: Created broadcast 427 from broadcast at DAGScheduler.scala:839
15/05/13 11:35:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 284 (MapPartitionsRDD[2030] at map at JsonRDD.scala:51)
15/05/13 11:35:00 INFO TaskSchedulerImpl: Adding task set 284.0 with 1 tasks
15/05/13 11:35:00 INFO TaskSetManager: Starting task 0.0 in stage 284.0 (TID 284, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_427_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.4 MB)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.4 MB)
15/05/13 11:35:00 INFO TaskSetManager: Finished task 0.0 in stage 284.0 (TID 284) in 350 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:35:00 INFO TaskSchedulerImpl: Removed TaskSet 284.0, whose tasks have all completed, from pool 
15/05/13 11:35:00 INFO DAGScheduler: Stage 284 (reduce at JsonRDD.scala:51) finished in 0.362 s
15/05/13 11:35:00 INFO DAGScheduler: Job 290 finished: reduce at JsonRDD.scala:51, took 0.397239 s
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 417
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_417_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_417_piece0 of size 35708 dropped from memory (free 244428892)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_417_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_417_piece0
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_417
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_417 of size 238532 dropped from memory (free 244667424)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_417_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 417
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 419
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_419
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_419 of size 20920 dropped from memory (free 244688344)
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_419_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_419_piece0 of size 10830 dropped from memory (free 244699174)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_419_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_419_piece0
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_419_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 263.7 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 419
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 418
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_418
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_418 of size 6040 dropped from memory (free 244705214)
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_418_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_418_piece0 of size 4225 dropped from memory (free 244709439)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_418_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_418_piece0
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_418_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 418
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 422
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_422
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_422 of size 20920 dropped from memory (free 244730359)
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_422_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_422_piece0 of size 10833 dropped from memory (free 244741192)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_422_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_422_piece0
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_422_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.4 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 422
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 421
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_421_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_421_piece0 of size 4225 dropped from memory (free 244745417)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_421_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_421_piece0
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_421
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_421 of size 6040 dropped from memory (free 244751457)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_421_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 421
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 427
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_427_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_427_piece0 of size 4225 dropped from memory (free 244755682)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_427_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_427_piece0
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_427
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_427 of size 6040 dropped from memory (free 244761722)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_427_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.4 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 427
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 425
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_425
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_425 of size 20920 dropped from memory (free 244782642)
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_425_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_425_piece0 of size 10844 dropped from memory (free 244793486)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_425_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_425_piece0
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_425_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.4 MB)
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 425
15/05/13 11:35:00 INFO BlockManager: Removing broadcast 424
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_424_piece0
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_424_piece0 of size 4225 dropped from memory (free 244797711)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_424_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_424_piece0
15/05/13 11:35:00 INFO BlockManager: Removing block broadcast_424
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_424 of size 6040 dropped from memory (free 244803751)
15/05/13 11:35:00 INFO BlockManagerInfo: Removed broadcast_424_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:35:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:35:00 INFO ContextCleaner: Cleaned broadcast 424
15/05/13 11:35:00 INFO DAGScheduler: Got job 291 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:35:00 INFO DAGScheduler: Final stage: Stage 285(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:35:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:35:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:35:00 INFO DAGScheduler: Submitting Stage 285 (MapPartitionsRDD[2037] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=33498805, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_428 stored as values in memory (estimated size 20.0 KB, free 233.4 MB)
15/05/13 11:35:00 INFO MemoryStore: ensureFreeSpace(10768) called with curMem=33519293, maxMem=278302556
15/05/13 11:35:00 INFO MemoryStore: Block broadcast_428_piece0 stored as bytes in memory (estimated size 10.5 KB, free 233.4 MB)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_428_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 261.2 MB)
15/05/13 11:35:00 INFO BlockManagerMaster: Updated info of block broadcast_428_piece0
15/05/13 11:35:00 INFO SparkContext: Created broadcast 428 from broadcast at DAGScheduler.scala:839
15/05/13 11:35:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 285 (MapPartitionsRDD[2037] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:35:00 INFO TaskSchedulerImpl: Adding task set 285.0 with 1 tasks
15/05/13 11:35:00 INFO TaskSetManager: Starting task 0.0 in stage 285.0 (TID 285, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:35:00 INFO BlockManagerInfo: Added broadcast_428_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 263.7 MB)
15/05/13 11:35:01 INFO BlockManagerInfo: Added broadcast_426_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:35:01 INFO TaskSetManager: Finished task 0.0 in stage 285.0 (TID 285) in 239 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:35:01 INFO TaskSchedulerImpl: Removed TaskSet 285.0, whose tasks have all completed, from pool 
15/05/13 11:35:01 INFO DAGScheduler: Stage 285 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.240 s
15/05/13 11:35:01 INFO DAGScheduler: Job 291 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.260784 s
15/05/13 11:35:01 INFO JobScheduler: Finished job streaming job 1431531300000 ms.0 from job set of time 1431531300000 ms
15/05/13 11:35:01 INFO JobScheduler: Total delay: 1.237 s for time 1431531300000 ms (execution: 1.100 s)
15/05/13 11:35:01 INFO MapPartitionsRDD: Removing RDD 2012 from persistence list
15/05/13 11:35:01 INFO BlockManager: Removing RDD 2012
15/05/13 11:35:01 INFO UnionRDD: Removing RDD 2011 from persistence list
15/05/13 11:35:01 INFO BlockManager: Removing RDD 2011
15/05/13 11:35:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531240000 ms: 1431531180000 ms
15/05/13 11:35:01 INFO JobGenerator: Checkpointing graph for time 1431531300000 ms
15/05/13 11:35:01 INFO DStreamGraph: Updating checkpoint data for time 1431531300000 ms
15/05/13 11:35:01 INFO DStreamGraph: Updated checkpoint data for time 1431531300000 ms
15/05/13 11:35:01 INFO CheckpointWriter: Saving checkpoint for time 1431531300000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000'
15/05/13 11:35:01 INFO CheckpointWriter: Checkpoint for time 1431531300000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000', took 7654 bytes and 64 ms
15/05/13 11:35:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531300000 ms
15/05/13 11:35:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531300000 ms
15/05/13 11:35:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:36:00 INFO FileInputDStream: Finding new files took 39 ms
15/05/13 11:36:00 INFO FileInputDStream: New files at time 1431531360000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531258980.json
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=33530061, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_429 stored as values in memory (estimated size 232.9 KB, free 233.2 MB)
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=33768593, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_429_piece0 stored as bytes in memory (estimated size 34.9 KB, free 233.2 MB)
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.2 MB)
15/05/13 11:36:00 INFO BlockManagerMaster: Updated info of block broadcast_429_piece0
15/05/13 11:36:00 INFO SparkContext: Created broadcast 429 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:36:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:36:00 INFO JobScheduler: Added jobs for time 1431531360000 ms
15/05/13 11:36:00 INFO JobGenerator: Checkpointing graph for time 1431531360000 ms
15/05/13 11:36:00 INFO JobScheduler: Starting job streaming job 1431531360000 ms.0 from job set of time 1431531360000 ms
15/05/13 11:36:00 INFO DStreamGraph: Updating checkpoint data for time 1431531360000 ms
15/05/13 11:36:00 INFO DStreamGraph: Updated checkpoint data for time 1431531360000 ms
15/05/13 11:36:00 INFO CheckpointWriter: Saving checkpoint for time 1431531360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531360000'
15/05/13 11:36:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:36:00 INFO DAGScheduler: Got job 292 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:36:00 INFO DAGScheduler: Final stage: Stage 286(reduce at JsonRDD.scala:51)
15/05/13 11:36:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:36:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:36:00 INFO DAGScheduler: Submitting Stage 286 (MapPartitionsRDD[2044] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=33804301, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_430 stored as values in memory (estimated size 5.9 KB, free 233.2 MB)
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=33810341, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_430_piece0 stored as bytes in memory (estimated size 4.1 KB, free 233.2 MB)
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_430_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.2 MB)
15/05/13 11:36:00 INFO BlockManagerMaster: Updated info of block broadcast_430_piece0
15/05/13 11:36:00 INFO SparkContext: Created broadcast 430 from broadcast at DAGScheduler.scala:839
15/05/13 11:36:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 286 (MapPartitionsRDD[2044] at map at JsonRDD.scala:51)
15/05/13 11:36:00 INFO TaskSchedulerImpl: Adding task set 286.0 with 1 tasks
15/05/13 11:36:00 INFO TaskSetManager: Starting task 0.0 in stage 286.0 (TID 286, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:36:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531060000.bk
15/05/13 11:36:00 INFO CheckpointWriter: Checkpoint for time 1431531360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531360000', took 7664 bytes and 90 ms
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_430_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.9 MB)
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_429_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:36:00 INFO TaskSetManager: Finished task 0.0 in stage 286.0 (TID 286) in 238 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:36:00 INFO TaskSchedulerImpl: Removed TaskSet 286.0, whose tasks have all completed, from pool 
15/05/13 11:36:00 INFO DAGScheduler: Stage 286 (reduce at JsonRDD.scala:51) finished in 0.248 s
15/05/13 11:36:00 INFO DAGScheduler: Job 292 finished: reduce at JsonRDD.scala:51, took 0.295003 s
15/05/13 11:36:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:36:00 INFO DAGScheduler: Got job 293 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:36:00 INFO DAGScheduler: Final stage: Stage 287(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:36:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:36:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:36:00 INFO DAGScheduler: Submitting Stage 287 (MapPartitionsRDD[2051] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(21680) called with curMem=33814566, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_431 stored as values in memory (estimated size 21.2 KB, free 233.1 MB)
15/05/13 11:36:00 INFO MemoryStore: ensureFreeSpace(11149) called with curMem=33836246, maxMem=278302556
15/05/13 11:36:00 INFO MemoryStore: Block broadcast_431_piece0 stored as bytes in memory (estimated size 10.9 KB, free 233.1 MB)
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_431_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.2 MB)
15/05/13 11:36:00 INFO BlockManagerMaster: Updated info of block broadcast_431_piece0
15/05/13 11:36:00 INFO SparkContext: Created broadcast 431 from broadcast at DAGScheduler.scala:839
15/05/13 11:36:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 287 (MapPartitionsRDD[2051] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:36:00 INFO TaskSchedulerImpl: Adding task set 287.0 with 1 tasks
15/05/13 11:36:00 INFO TaskSetManager: Starting task 0.0 in stage 287.0 (TID 287, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:36:00 INFO BlockManagerInfo: Added broadcast_431_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 263.8 MB)
15/05/13 11:36:00 INFO TaskSetManager: Finished task 0.0 in stage 287.0 (TID 287) in 66 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:36:00 INFO TaskSchedulerImpl: Removed TaskSet 287.0, whose tasks have all completed, from pool 
15/05/13 11:36:00 INFO DAGScheduler: Stage 287 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.068 s
15/05/13 11:36:00 INFO DAGScheduler: Job 293 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.088950 s
15/05/13 11:36:00 INFO JobScheduler: Finished job streaming job 1431531360000 ms.0 from job set of time 1431531360000 ms
15/05/13 11:36:00 INFO JobScheduler: Total delay: 0.995 s for time 1431531360000 ms (execution: 0.805 s)
15/05/13 11:36:00 INFO MapPartitionsRDD: Removing RDD 2026 from persistence list
15/05/13 11:36:00 INFO BlockManager: Removing RDD 2026
15/05/13 11:36:00 INFO UnionRDD: Removing RDD 2025 from persistence list
15/05/13 11:36:01 INFO BlockManager: Removing RDD 2025
15/05/13 11:36:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531300000 ms: 1431531240000 ms
15/05/13 11:36:01 INFO JobGenerator: Checkpointing graph for time 1431531360000 ms
15/05/13 11:36:01 INFO DStreamGraph: Updating checkpoint data for time 1431531360000 ms
15/05/13 11:36:01 INFO DStreamGraph: Updated checkpoint data for time 1431531360000 ms
15/05/13 11:36:01 INFO CheckpointWriter: Saving checkpoint for time 1431531360000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531360000'
15/05/13 11:36:01 INFO CheckpointWriter: Checkpoint for time 1431531360000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531360000', took 7657 bytes and 56 ms
15/05/13 11:36:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531360000 ms
15/05/13 11:36:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531360000 ms
15/05/13 11:36:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:37:00 INFO FileInputDStream: Finding new files took 52 ms
15/05/13 11:37:00 INFO FileInputDStream: New files at time 1431531420000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531320593.json
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=33847395, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_432 stored as values in memory (estimated size 232.9 KB, free 232.9 MB)
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=34085927, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_432_piece0 stored as bytes in memory (estimated size 34.9 KB, free 232.9 MB)
15/05/13 11:37:00 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.1 MB)
15/05/13 11:37:00 INFO BlockManagerMaster: Updated info of block broadcast_432_piece0
15/05/13 11:37:00 INFO SparkContext: Created broadcast 432 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:37:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:37:00 INFO JobScheduler: Starting job streaming job 1431531420000 ms.0 from job set of time 1431531420000 ms
15/05/13 11:37:00 INFO JobScheduler: Added jobs for time 1431531420000 ms
15/05/13 11:37:00 INFO JobGenerator: Checkpointing graph for time 1431531420000 ms
15/05/13 11:37:00 INFO DStreamGraph: Updating checkpoint data for time 1431531420000 ms
15/05/13 11:37:00 INFO DStreamGraph: Updated checkpoint data for time 1431531420000 ms
15/05/13 11:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431531420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000'
15/05/13 11:37:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84108): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:37:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84108): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431531420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000'
15/05/13 11:37:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:37:00 INFO DAGScheduler: Got job 294 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:37:00 INFO DAGScheduler: Final stage: Stage 288(reduce at JsonRDD.scala:51)
15/05/13 11:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:37:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84110): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:37:00 INFO CheckpointWriter: Saving checkpoint for time 1431531420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000'
15/05/13 11:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:37:00 INFO DAGScheduler: Submitting Stage 288 (MapPartitionsRDD[2058] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34121635, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_433 stored as values in memory (estimated size 5.9 KB, free 232.9 MB)
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=34127675, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_433_piece0 stored as bytes in memory (estimated size 4.1 KB, free 232.9 MB)
15/05/13 11:37:00 INFO BlockManagerInfo: Added broadcast_433_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:37:00 INFO BlockManagerMaster: Updated info of block broadcast_433_piece0
15/05/13 11:37:00 INFO SparkContext: Created broadcast 433 from broadcast at DAGScheduler.scala:839
15/05/13 11:37:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 288 (MapPartitionsRDD[2058] at map at JsonRDD.scala:51)
15/05/13 11:37:00 INFO TaskSchedulerImpl: Adding task set 288.0 with 1 tasks
15/05/13 11:37:00 INFO TaskSetManager: Starting task 0.0 in stage 288.0 (TID 288, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:37:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84112): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:37:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531420000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000'
15/05/13 11:37:00 INFO BlockManagerInfo: Added broadcast_433_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.4 MB)
15/05/13 11:37:00 INFO BlockManagerInfo: Added broadcast_432_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.4 MB)
15/05/13 11:37:00 INFO TaskSetManager: Finished task 0.0 in stage 288.0 (TID 288) in 386 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:37:00 INFO TaskSchedulerImpl: Removed TaskSet 288.0, whose tasks have all completed, from pool 
15/05/13 11:37:00 INFO DAGScheduler: Stage 288 (reduce at JsonRDD.scala:51) finished in 0.394 s
15/05/13 11:37:00 INFO DAGScheduler: Job 294 finished: reduce at JsonRDD.scala:51, took 0.450097 s
15/05/13 11:37:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:37:00 INFO DAGScheduler: Got job 295 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:37:00 INFO DAGScheduler: Final stage: Stage 289(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:37:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:37:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:37:00 INFO DAGScheduler: Submitting Stage 289 (MapPartitionsRDD[2065] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=34131900, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_434 stored as values in memory (estimated size 20.8 KB, free 232.8 MB)
15/05/13 11:37:00 INFO MemoryStore: ensureFreeSpace(11129) called with curMem=34153204, maxMem=278302556
15/05/13 11:37:00 INFO MemoryStore: Block broadcast_434_piece0 stored as bytes in memory (estimated size 10.9 KB, free 232.8 MB)
15/05/13 11:37:00 INFO BlockManagerInfo: Added broadcast_434_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.1 MB)
15/05/13 11:37:00 INFO BlockManagerMaster: Updated info of block broadcast_434_piece0
15/05/13 11:37:00 INFO SparkContext: Created broadcast 434 from broadcast at DAGScheduler.scala:839
15/05/13 11:37:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 289 (MapPartitionsRDD[2065] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:37:00 INFO TaskSchedulerImpl: Adding task set 289.0 with 1 tasks
15/05/13 11:37:00 INFO TaskSetManager: Starting task 0.0 in stage 289.0 (TID 289, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:37:01 INFO BlockManagerInfo: Added broadcast_434_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.4 MB)
15/05/13 11:37:01 INFO TaskSetManager: Finished task 0.0 in stage 289.0 (TID 289) in 123 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:37:01 INFO TaskSchedulerImpl: Removed TaskSet 289.0, whose tasks have all completed, from pool 
15/05/13 11:37:01 INFO DAGScheduler: Stage 289 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.124 s
15/05/13 11:37:01 INFO DAGScheduler: Job 295 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.159646 s
15/05/13 11:37:01 INFO JobScheduler: Finished job streaming job 1431531420000 ms.0 from job set of time 1431531420000 ms
15/05/13 11:37:01 INFO JobScheduler: Total delay: 1.145 s for time 1431531420000 ms (execution: 0.956 s)
15/05/13 11:37:01 INFO MapPartitionsRDD: Removing RDD 2040 from persistence list
15/05/13 11:37:01 INFO BlockManager: Removing RDD 2040
15/05/13 11:37:01 INFO UnionRDD: Removing RDD 2039 from persistence list
15/05/13 11:37:01 INFO BlockManager: Removing RDD 2039
15/05/13 11:37:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531360000 ms: 1431531300000 ms
15/05/13 11:37:01 INFO JobGenerator: Checkpointing graph for time 1431531420000 ms
15/05/13 11:37:01 INFO DStreamGraph: Updating checkpoint data for time 1431531420000 ms
15/05/13 11:37:01 INFO DStreamGraph: Updated checkpoint data for time 1431531420000 ms
15/05/13 11:37:01 INFO CheckpointWriter: Saving checkpoint for time 1431531420000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000'
15/05/13 11:37:01 INFO CheckpointWriter: Checkpoint for time 1431531420000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531420000', took 7655 bytes and 63 ms
15/05/13 11:37:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531420000 ms
15/05/13 11:37:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531420000 ms
15/05/13 11:37:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:38:00 INFO FileInputDStream: Finding new files took 28 ms
15/05/13 11:38:00 INFO FileInputDStream: New files at time 1431531480000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531380839.json
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=34164333, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_435 stored as values in memory (estimated size 232.9 KB, free 232.6 MB)
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=34402865, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_435_piece0 stored as bytes in memory (estimated size 34.9 KB, free 232.6 MB)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.1 MB)
15/05/13 11:38:00 INFO BlockManagerMaster: Updated info of block broadcast_435_piece0
15/05/13 11:38:00 INFO SparkContext: Created broadcast 435 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:38:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:38:00 INFO JobScheduler: Starting job streaming job 1431531480000 ms.0 from job set of time 1431531480000 ms
15/05/13 11:38:00 INFO JobScheduler: Added jobs for time 1431531480000 ms
15/05/13 11:38:00 INFO JobGenerator: Checkpointing graph for time 1431531480000 ms
15/05/13 11:38:00 INFO DStreamGraph: Updating checkpoint data for time 1431531480000 ms
15/05/13 11:38:00 INFO DStreamGraph: Updated checkpoint data for time 1431531480000 ms
15/05/13 11:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431531480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000'
15/05/13 11:38:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:38:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431531480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000'
15/05/13 11:38:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:38:00 INFO DAGScheduler: Got job 296 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:38:00 INFO DAGScheduler: Final stage: Stage 290(reduce at JsonRDD.scala:51)
15/05/13 11:38:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:38:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:38:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84120): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:38:00 INFO CheckpointWriter: Saving checkpoint for time 1431531480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000'
15/05/13 11:38:00 INFO DAGScheduler: Submitting Stage 290 (MapPartitionsRDD[2072] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34438573, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_436 stored as values in memory (estimated size 5.9 KB, free 232.6 MB)
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=34444613, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_436_piece0 stored as bytes in memory (estimated size 4.1 KB, free 232.6 MB)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_436_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:38:00 INFO BlockManagerMaster: Updated info of block broadcast_436_piece0
15/05/13 11:38:00 INFO SparkContext: Created broadcast 436 from broadcast at DAGScheduler.scala:839
15/05/13 11:38:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 290 (MapPartitionsRDD[2072] at map at JsonRDD.scala:51)
15/05/13 11:38:00 INFO TaskSchedulerImpl: Adding task set 290.0 with 1 tasks
15/05/13 11:38:00 INFO TaskSetManager: Starting task 0.0 in stage 290.0 (TID 290, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_436_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:38:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531180000.bk
15/05/13 11:38:00 INFO CheckpointWriter: Checkpoint for time 1431531480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000', took 7661 bytes and 172 ms
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:38:00 INFO TaskSetManager: Finished task 0.0 in stage 290.0 (TID 290) in 223 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:38:00 INFO TaskSchedulerImpl: Removed TaskSet 290.0, whose tasks have all completed, from pool 
15/05/13 11:38:00 INFO DAGScheduler: Stage 290 (reduce at JsonRDD.scala:51) finished in 0.232 s
15/05/13 11:38:00 INFO DAGScheduler: Job 296 finished: reduce at JsonRDD.scala:51, took 0.284286 s
15/05/13 11:38:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:38:00 INFO DAGScheduler: Got job 297 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:38:00 INFO DAGScheduler: Final stage: Stage 291(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:38:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:38:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:38:00 INFO DAGScheduler: Submitting Stage 291 (MapPartitionsRDD[2079] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=34448838, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_437 stored as values in memory (estimated size 20.4 KB, free 232.5 MB)
15/05/13 11:38:00 INFO MemoryStore: ensureFreeSpace(10841) called with curMem=34469758, maxMem=278302556
15/05/13 11:38:00 INFO MemoryStore: Block broadcast_437_piece0 stored as bytes in memory (estimated size 10.6 KB, free 232.5 MB)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_437_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.1 MB)
15/05/13 11:38:00 INFO BlockManagerMaster: Updated info of block broadcast_437_piece0
15/05/13 11:38:00 INFO SparkContext: Created broadcast 437 from broadcast at DAGScheduler.scala:839
15/05/13 11:38:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 291 (MapPartitionsRDD[2079] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:38:00 INFO TaskSchedulerImpl: Adding task set 291.0 with 1 tasks
15/05/13 11:38:00 INFO TaskSetManager: Starting task 0.0 in stage 291.0 (TID 291, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_437_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.4 MB)
15/05/13 11:38:00 INFO BlockManagerInfo: Added broadcast_435_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.3 MB)
15/05/13 11:38:00 INFO TaskSetManager: Finished task 0.0 in stage 291.0 (TID 291) in 274 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:38:00 INFO TaskSchedulerImpl: Removed TaskSet 291.0, whose tasks have all completed, from pool 
15/05/13 11:38:00 INFO DAGScheduler: Stage 291 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.275 s
15/05/13 11:38:00 INFO DAGScheduler: Job 297 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.311585 s
15/05/13 11:38:00 INFO JobScheduler: Finished job streaming job 1431531480000 ms.0 from job set of time 1431531480000 ms
15/05/13 11:38:00 INFO JobScheduler: Total delay: 0.996 s for time 1431531480000 ms (execution: 0.889 s)
15/05/13 11:38:00 INFO MapPartitionsRDD: Removing RDD 2054 from persistence list
15/05/13 11:38:01 INFO BlockManager: Removing RDD 2054
15/05/13 11:38:01 INFO UnionRDD: Removing RDD 2053 from persistence list
15/05/13 11:38:01 INFO BlockManager: Removing RDD 2053
15/05/13 11:38:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531420000 ms: 1431531360000 ms
15/05/13 11:38:01 INFO JobGenerator: Checkpointing graph for time 1431531480000 ms
15/05/13 11:38:01 INFO DStreamGraph: Updating checkpoint data for time 1431531480000 ms
15/05/13 11:38:01 INFO DStreamGraph: Updated checkpoint data for time 1431531480000 ms
15/05/13 11:38:01 INFO CheckpointWriter: Saving checkpoint for time 1431531480000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000'
15/05/13 11:38:01 INFO CheckpointWriter: Checkpoint for time 1431531480000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000', took 7654 bytes and 66 ms
15/05/13 11:38:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531480000 ms
15/05/13 11:38:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531480000 ms
15/05/13 11:38:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:39:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 11:39:00 INFO FileInputDStream: New files at time 1431531540000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531443452.json
15/05/13 11:39:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=34480599, maxMem=278302556
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_438 stored as values in memory (estimated size 232.9 KB, free 232.3 MB)
15/05/13 11:39:00 INFO MemoryStore: ensureFreeSpace(35709) called with curMem=34719131, maxMem=278302556
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_438_piece0 stored as bytes in memory (estimated size 34.9 KB, free 232.3 MB)
15/05/13 11:39:00 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_438_piece0
15/05/13 11:39:00 INFO SparkContext: Created broadcast 438 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:39:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:39:00 INFO JobScheduler: Added jobs for time 1431531540000 ms
15/05/13 11:39:00 INFO JobGenerator: Checkpointing graph for time 1431531540000 ms
15/05/13 11:39:00 INFO JobScheduler: Starting job streaming job 1431531540000 ms.0 from job set of time 1431531540000 ms
15/05/13 11:39:00 INFO DStreamGraph: Updating checkpoint data for time 1431531540000 ms
15/05/13 11:39:00 INFO DStreamGraph: Updated checkpoint data for time 1431531540000 ms
15/05/13 11:39:00 INFO CheckpointWriter: Saving checkpoint for time 1431531540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531540000'
15/05/13 11:39:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531240000
15/05/13 11:39:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:39:00 INFO DAGScheduler: Got job 298 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:39:00 INFO DAGScheduler: Final stage: Stage 292(reduce at JsonRDD.scala:51)
15/05/13 11:39:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:39:00 INFO CheckpointWriter: Checkpoint for time 1431531540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531540000', took 7662 bytes and 82 ms
15/05/13 11:39:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:39:00 INFO DAGScheduler: Submitting Stage 292 (MapPartitionsRDD[2086] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:39:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34754840, maxMem=278302556
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_439 stored as values in memory (estimated size 5.9 KB, free 232.3 MB)
15/05/13 11:39:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=34760880, maxMem=278302556
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_439_piece0 stored as bytes in memory (estimated size 4.1 KB, free 232.3 MB)
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 431
15/05/13 11:39:00 INFO BlockManagerInfo: Added broadcast_439_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_439_piece0
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_431
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_431 of size 21680 dropped from memory (free 243559131)
15/05/13 11:39:00 INFO SparkContext: Created broadcast 439 from broadcast at DAGScheduler.scala:839
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_431_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_431_piece0 of size 11149 dropped from memory (free 243570280)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_431_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_431_piece0
15/05/13 11:39:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 292 (MapPartitionsRDD[2086] at map at JsonRDD.scala:51)
15/05/13 11:39:00 INFO TaskSchedulerImpl: Adding task set 292.0 with 1 tasks
15/05/13 11:39:00 INFO TaskSetManager: Starting task 0.0 in stage 292.0 (TID 292, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_431_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 263.8 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 431
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 430
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_430_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_430_piece0 of size 4225 dropped from memory (free 243574505)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_430_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_430_piece0
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_430
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_430 of size 6040 dropped from memory (free 243580545)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_430_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:39:00 INFO BlockManagerInfo: Added broadcast_439_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 430
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 428
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_428_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_428_piece0 of size 10768 dropped from memory (free 243591313)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_428_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_428_piece0
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_428
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_428 of size 20488 dropped from memory (free 243611801)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_428_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 263.7 MB)
15/05/13 11:39:00 INFO BlockManagerInfo: Added broadcast_438_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 428
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 437
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_437
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_437 of size 20920 dropped from memory (free 243632721)
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_437_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_437_piece0 of size 10841 dropped from memory (free 243643562)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_437_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_437_piece0
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_437_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.3 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 437
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 436
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_436_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_436_piece0 of size 4225 dropped from memory (free 243647787)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_436_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_436_piece0
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_436
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_436 of size 6040 dropped from memory (free 243653827)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_436_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 436
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 434
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_434_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_434_piece0 of size 11129 dropped from memory (free 243664956)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_434_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_434_piece0
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_434
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_434 of size 21304 dropped from memory (free 243686260)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_434_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 262.3 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 434
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 433
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_433
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_433 of size 6040 dropped from memory (free 243692300)
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_433_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_433_piece0 of size 4225 dropped from memory (free 243696525)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_433_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_433_piece0
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_433_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 433
15/05/13 11:39:00 INFO BlockManager: Removing broadcast 432
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_432
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_432 of size 238532 dropped from memory (free 243935057)
15/05/13 11:39:00 INFO BlockManager: Removing block broadcast_432_piece0
15/05/13 11:39:00 INFO MemoryStore: Block broadcast_432_piece0 of size 35708 dropped from memory (free 243970765)
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_432_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 261.1 MB)
15/05/13 11:39:00 INFO BlockManagerMaster: Updated info of block broadcast_432_piece0
15/05/13 11:39:00 INFO BlockManagerInfo: Removed broadcast_432_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.4 MB)
15/05/13 11:39:00 INFO ContextCleaner: Cleaned broadcast 432
15/05/13 11:39:00 INFO TaskSetManager: Finished task 0.0 in stage 292.0 (TID 292) in 368 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:39:00 INFO TaskSchedulerImpl: Removed TaskSet 292.0, whose tasks have all completed, from pool 
15/05/13 11:39:00 INFO DAGScheduler: Stage 292 (reduce at JsonRDD.scala:51) finished in 0.381 s
15/05/13 11:39:00 INFO DAGScheduler: Job 298 finished: reduce at JsonRDD.scala:51, took 0.478357 s
15/05/13 11:39:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:39:00 INFO DAGScheduler: Got job 299 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:39:00 INFO DAGScheduler: Final stage: Stage 293(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:39:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:39:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:39:01 INFO DAGScheduler: Submitting Stage 293 (MapPartitionsRDD[2093] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:39:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=34331791, maxMem=278302556
15/05/13 11:39:01 INFO MemoryStore: Block broadcast_440 stored as values in memory (estimated size 20.4 KB, free 232.6 MB)
15/05/13 11:39:01 INFO MemoryStore: ensureFreeSpace(10907) called with curMem=34352711, maxMem=278302556
15/05/13 11:39:01 INFO MemoryStore: Block broadcast_440_piece0 stored as bytes in memory (estimated size 10.7 KB, free 232.6 MB)
15/05/13 11:39:01 INFO BlockManagerInfo: Added broadcast_440_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 261.1 MB)
15/05/13 11:39:01 INFO BlockManagerMaster: Updated info of block broadcast_440_piece0
15/05/13 11:39:01 INFO SparkContext: Created broadcast 440 from broadcast at DAGScheduler.scala:839
15/05/13 11:39:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 293 (MapPartitionsRDD[2093] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:39:01 INFO TaskSchedulerImpl: Adding task set 293.0 with 1 tasks
15/05/13 11:39:01 INFO TaskSetManager: Starting task 0.0 in stage 293.0 (TID 293, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:39:01 INFO BlockManagerInfo: Added broadcast_440_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.7 KB, free: 263.6 MB)
15/05/13 11:39:01 INFO TaskSetManager: Finished task 0.0 in stage 293.0 (TID 293) in 114 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:39:01 INFO TaskSchedulerImpl: Removed TaskSet 293.0, whose tasks have all completed, from pool 
15/05/13 11:39:01 INFO DAGScheduler: Stage 293 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.118 s
15/05/13 11:39:01 INFO DAGScheduler: Job 299 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.157790 s
15/05/13 11:39:01 INFO JobScheduler: Finished job streaming job 1431531540000 ms.0 from job set of time 1431531540000 ms
15/05/13 11:39:01 INFO JobScheduler: Total delay: 1.194 s for time 1431531540000 ms (execution: 0.956 s)
15/05/13 11:39:01 INFO MapPartitionsRDD: Removing RDD 2068 from persistence list
15/05/13 11:39:01 INFO BlockManager: Removing RDD 2068
15/05/13 11:39:01 INFO UnionRDD: Removing RDD 2067 from persistence list
15/05/13 11:39:01 INFO BlockManager: Removing RDD 2067
15/05/13 11:39:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531480000 ms: 1431531420000 ms
15/05/13 11:39:01 INFO JobGenerator: Checkpointing graph for time 1431531540000 ms
15/05/13 11:39:01 INFO DStreamGraph: Updating checkpoint data for time 1431531540000 ms
15/05/13 11:39:01 INFO DStreamGraph: Updated checkpoint data for time 1431531540000 ms
15/05/13 11:39:01 INFO CheckpointWriter: Saving checkpoint for time 1431531540000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531540000'
15/05/13 11:39:01 INFO CheckpointWriter: Checkpoint for time 1431531540000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531540000', took 7655 bytes and 60 ms
15/05/13 11:39:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531540000 ms
15/05/13 11:39:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531540000 ms
15/05/13 11:39:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:40:00 INFO FileInputDStream: Finding new files took 64 ms
15/05/13 11:40:00 INFO FileInputDStream: New files at time 1431531600000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531507971.json
15/05/13 11:40:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=34363618, maxMem=278302556
15/05/13 11:40:00 INFO MemoryStore: Block broadcast_441 stored as values in memory (estimated size 232.9 KB, free 232.4 MB)
15/05/13 11:40:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=34602150, maxMem=278302556
15/05/13 11:40:00 INFO MemoryStore: Block broadcast_441_piece0 stored as bytes in memory (estimated size 34.9 KB, free 232.4 MB)
15/05/13 11:40:00 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.1 MB)
15/05/13 11:40:00 INFO BlockManagerMaster: Updated info of block broadcast_441_piece0
15/05/13 11:40:00 INFO SparkContext: Created broadcast 441 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:40:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:40:00 INFO JobScheduler: Added jobs for time 1431531600000 ms
15/05/13 11:40:00 INFO JobGenerator: Checkpointing graph for time 1431531600000 ms
15/05/13 11:40:00 INFO DStreamGraph: Updating checkpoint data for time 1431531600000 ms
15/05/13 11:40:00 INFO JobScheduler: Starting job streaming job 1431531600000 ms.0 from job set of time 1431531600000 ms
15/05/13 11:40:00 INFO DStreamGraph: Updated checkpoint data for time 1431531600000 ms
15/05/13 11:40:00 INFO CheckpointWriter: Saving checkpoint for time 1431531600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531600000'
15/05/13 11:40:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:40:00 INFO DAGScheduler: Got job 300 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:40:00 INFO DAGScheduler: Final stage: Stage 294(reduce at JsonRDD.scala:51)
15/05/13 11:40:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:40:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:40:00 INFO DAGScheduler: Submitting Stage 294 (MapPartitionsRDD[2100] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:40:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34637858, maxMem=278302556
15/05/13 11:40:00 INFO MemoryStore: Block broadcast_442 stored as values in memory (estimated size 5.9 KB, free 232.4 MB)
15/05/13 11:40:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=34643898, maxMem=278302556
15/05/13 11:40:00 INFO MemoryStore: Block broadcast_442_piece0 stored as bytes in memory (estimated size 4.1 KB, free 232.4 MB)
15/05/13 11:40:00 INFO BlockManagerInfo: Added broadcast_442_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.1 MB)
15/05/13 11:40:00 INFO BlockManagerMaster: Updated info of block broadcast_442_piece0
15/05/13 11:40:00 INFO SparkContext: Created broadcast 442 from broadcast at DAGScheduler.scala:839
15/05/13 11:40:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 294 (MapPartitionsRDD[2100] at map at JsonRDD.scala:51)
15/05/13 11:40:00 INFO TaskSchedulerImpl: Adding task set 294.0 with 1 tasks
15/05/13 11:40:00 INFO TaskSetManager: Starting task 0.0 in stage 294.0 (TID 294, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:40:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531300000
15/05/13 11:40:00 INFO CheckpointWriter: Checkpoint for time 1431531600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531600000', took 7666 bytes and 94 ms
15/05/13 11:40:00 INFO BlockManagerInfo: Added broadcast_442_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.4 MB)
15/05/13 11:40:00 INFO BlockManagerInfo: Added broadcast_441_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.3 MB)
15/05/13 11:40:00 INFO TaskSetManager: Finished task 0.0 in stage 294.0 (TID 294) in 437 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:40:00 INFO TaskSchedulerImpl: Removed TaskSet 294.0, whose tasks have all completed, from pool 
15/05/13 11:40:00 INFO DAGScheduler: Stage 294 (reduce at JsonRDD.scala:51) finished in 0.451 s
15/05/13 11:40:00 INFO DAGScheduler: Job 300 finished: reduce at JsonRDD.scala:51, took 0.473400 s
15/05/13 11:40:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:40:01 INFO DAGScheduler: Got job 301 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:40:01 INFO DAGScheduler: Final stage: Stage 295(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:40:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:40:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:40:01 INFO DAGScheduler: Submitting Stage 295 (MapPartitionsRDD[2107] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:40:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=34648123, maxMem=278302556
15/05/13 11:40:01 INFO MemoryStore: Block broadcast_443 stored as values in memory (estimated size 20.8 KB, free 232.3 MB)
15/05/13 11:40:01 INFO MemoryStore: ensureFreeSpace(11107) called with curMem=34669427, maxMem=278302556
15/05/13 11:40:01 INFO MemoryStore: Block broadcast_443_piece0 stored as bytes in memory (estimated size 10.8 KB, free 232.3 MB)
15/05/13 11:40:01 INFO BlockManagerInfo: Added broadcast_443_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.8 KB, free: 261.1 MB)
15/05/13 11:40:01 INFO BlockManagerMaster: Updated info of block broadcast_443_piece0
15/05/13 11:40:01 INFO SparkContext: Created broadcast 443 from broadcast at DAGScheduler.scala:839
15/05/13 11:40:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 295 (MapPartitionsRDD[2107] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:40:01 INFO TaskSchedulerImpl: Adding task set 295.0 with 1 tasks
15/05/13 11:40:01 INFO TaskSetManager: Starting task 0.0 in stage 295.0 (TID 295, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:40:01 INFO BlockManagerInfo: Added broadcast_443_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.8 KB, free: 262.3 MB)
15/05/13 11:40:01 INFO TaskSetManager: Finished task 0.0 in stage 295.0 (TID 295) in 126 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:40:01 INFO TaskSchedulerImpl: Removed TaskSet 295.0, whose tasks have all completed, from pool 
15/05/13 11:40:01 INFO DAGScheduler: Stage 295 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.127 s
15/05/13 11:40:01 INFO DAGScheduler: Job 301 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.158408 s
15/05/13 11:40:01 INFO JobScheduler: Finished job streaming job 1431531600000 ms.0 from job set of time 1431531600000 ms
15/05/13 11:40:01 INFO JobScheduler: Total delay: 1.287 s for time 1431531600000 ms (execution: 1.066 s)
15/05/13 11:40:01 INFO MapPartitionsRDD: Removing RDD 2082 from persistence list
15/05/13 11:40:01 INFO BlockManager: Removing RDD 2082
15/05/13 11:40:01 INFO UnionRDD: Removing RDD 2081 from persistence list
15/05/13 11:40:01 INFO BlockManager: Removing RDD 2081
15/05/13 11:40:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531540000 ms: 1431531480000 ms
15/05/13 11:40:01 INFO JobGenerator: Checkpointing graph for time 1431531600000 ms
15/05/13 11:40:01 INFO DStreamGraph: Updating checkpoint data for time 1431531600000 ms
15/05/13 11:40:01 INFO DStreamGraph: Updated checkpoint data for time 1431531600000 ms
15/05/13 11:40:01 INFO CheckpointWriter: Saving checkpoint for time 1431531600000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531600000'
15/05/13 11:40:01 INFO CheckpointWriter: Checkpoint for time 1431531600000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531600000', took 7657 bytes and 59 ms
15/05/13 11:40:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531600000 ms
15/05/13 11:40:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531600000 ms
15/05/13 11:40:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:41:00 INFO FileInputDStream: Finding new files took 67 ms
15/05/13 11:41:00 INFO FileInputDStream: New files at time 1431531660000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531568179.json
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=34680534, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_444 stored as values in memory (estimated size 232.9 KB, free 232.1 MB)
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=34919066, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_444_piece0 stored as bytes in memory (estimated size 34.9 KB, free 232.1 MB)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.0 MB)
15/05/13 11:41:00 INFO BlockManagerMaster: Updated info of block broadcast_444_piece0
15/05/13 11:41:00 INFO SparkContext: Created broadcast 444 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:41:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:41:00 INFO JobScheduler: Added jobs for time 1431531660000 ms
15/05/13 11:41:00 INFO JobGenerator: Checkpointing graph for time 1431531660000 ms
15/05/13 11:41:00 INFO JobScheduler: Starting job streaming job 1431531660000 ms.0 from job set of time 1431531660000 ms
15/05/13 11:41:00 INFO DStreamGraph: Updating checkpoint data for time 1431531660000 ms
15/05/13 11:41:00 INFO DStreamGraph: Updated checkpoint data for time 1431531660000 ms
15/05/13 11:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431531660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000'
15/05/13 11:41:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84145): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:41:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84145): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:41:00 INFO CheckpointWriter: Saving checkpoint for time 1431531660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000'
15/05/13 11:41:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:41:00 INFO DAGScheduler: Got job 302 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:41:00 INFO DAGScheduler: Final stage: Stage 296(reduce at JsonRDD.scala:51)
15/05/13 11:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:41:00 INFO DAGScheduler: Submitting Stage 296 (MapPartitionsRDD[2114] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=34954774, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_445 stored as values in memory (estimated size 5.9 KB, free 232.1 MB)
15/05/13 11:41:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531360000.bk
15/05/13 11:41:00 INFO CheckpointWriter: Checkpoint for time 1431531660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000', took 7665 bytes and 93 ms
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=34960814, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_445_piece0 stored as bytes in memory (estimated size 4.1 KB, free 232.1 MB)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_445_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:41:00 INFO BlockManagerMaster: Updated info of block broadcast_445_piece0
15/05/13 11:41:00 INFO SparkContext: Created broadcast 445 from broadcast at DAGScheduler.scala:839
15/05/13 11:41:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 296 (MapPartitionsRDD[2114] at map at JsonRDD.scala:51)
15/05/13 11:41:00 INFO TaskSchedulerImpl: Adding task set 296.0 with 1 tasks
15/05/13 11:41:00 INFO TaskSetManager: Starting task 0.0 in stage 296.0 (TID 296, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_445_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.3 MB)
15/05/13 11:41:00 INFO TaskSetManager: Finished task 0.0 in stage 296.0 (TID 296) in 340 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:41:00 INFO TaskSchedulerImpl: Removed TaskSet 296.0, whose tasks have all completed, from pool 
15/05/13 11:41:00 INFO DAGScheduler: Stage 296 (reduce at JsonRDD.scala:51) finished in 0.354 s
15/05/13 11:41:00 INFO DAGScheduler: Job 302 finished: reduce at JsonRDD.scala:51, took 0.392450 s
15/05/13 11:41:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:41:00 INFO DAGScheduler: Got job 303 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:41:00 INFO DAGScheduler: Final stage: Stage 297(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:41:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:41:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:41:00 INFO DAGScheduler: Submitting Stage 297 (MapPartitionsRDD[2121] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=34965039, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_446 stored as values in memory (estimated size 21.0 KB, free 232.0 MB)
15/05/13 11:41:00 INFO MemoryStore: ensureFreeSpace(11113) called with curMem=34986519, maxMem=278302556
15/05/13 11:41:00 INFO MemoryStore: Block broadcast_446_piece0 stored as bytes in memory (estimated size 10.9 KB, free 232.0 MB)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_446_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 261.0 MB)
15/05/13 11:41:00 INFO BlockManagerMaster: Updated info of block broadcast_446_piece0
15/05/13 11:41:00 INFO SparkContext: Created broadcast 446 from broadcast at DAGScheduler.scala:839
15/05/13 11:41:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 297 (MapPartitionsRDD[2121] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:41:00 INFO TaskSchedulerImpl: Adding task set 297.0 with 1 tasks
15/05/13 11:41:00 INFO TaskSetManager: Starting task 0.0 in stage 297.0 (TID 297, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:41:00 INFO BlockManagerInfo: Added broadcast_446_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 263.8 MB)
15/05/13 11:41:01 INFO BlockManagerInfo: Added broadcast_444_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.8 MB)
15/05/13 11:41:01 INFO TaskSetManager: Finished task 0.0 in stage 297.0 (TID 297) in 185 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:41:01 INFO TaskSchedulerImpl: Removed TaskSet 297.0, whose tasks have all completed, from pool 
15/05/13 11:41:01 INFO DAGScheduler: Stage 297 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.190 s
15/05/13 11:41:01 INFO DAGScheduler: Job 303 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.236394 s
15/05/13 11:41:01 INFO JobScheduler: Finished job streaming job 1431531660000 ms.0 from job set of time 1431531660000 ms
15/05/13 11:41:01 INFO MapPartitionsRDD: Removing RDD 2096 from persistence list
15/05/13 11:41:01 INFO JobScheduler: Total delay: 1.181 s for time 1431531660000 ms (execution: 0.980 s)
15/05/13 11:41:01 INFO BlockManager: Removing RDD 2096
15/05/13 11:41:01 INFO UnionRDD: Removing RDD 2095 from persistence list
15/05/13 11:41:01 INFO BlockManager: Removing RDD 2095
15/05/13 11:41:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531600000 ms: 1431531540000 ms
15/05/13 11:41:01 INFO JobGenerator: Checkpointing graph for time 1431531660000 ms
15/05/13 11:41:01 INFO DStreamGraph: Updating checkpoint data for time 1431531660000 ms
15/05/13 11:41:01 INFO DStreamGraph: Updated checkpoint data for time 1431531660000 ms
15/05/13 11:41:01 INFO CheckpointWriter: Saving checkpoint for time 1431531660000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000'
15/05/13 11:41:01 INFO CheckpointWriter: Checkpoint for time 1431531660000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000', took 7654 bytes and 38 ms
15/05/13 11:41:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531660000 ms
15/05/13 11:41:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531660000 ms
15/05/13 11:41:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:42:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 11:42:00 INFO FileInputDStream: New files at time 1431531720000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531630964.json
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=34997632, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_447 stored as values in memory (estimated size 232.9 KB, free 231.8 MB)
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=35236164, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_447_piece0 stored as bytes in memory (estimated size 34.9 KB, free 231.8 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_447_piece0
15/05/13 11:42:00 INFO SparkContext: Created broadcast 447 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:42:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:42:00 INFO JobScheduler: Added jobs for time 1431531720000 ms
15/05/13 11:42:00 INFO JobGenerator: Checkpointing graph for time 1431531720000 ms
15/05/13 11:42:00 INFO DStreamGraph: Updating checkpoint data for time 1431531720000 ms
15/05/13 11:42:00 INFO JobScheduler: Starting job streaming job 1431531720000 ms.0 from job set of time 1431531720000 ms
15/05/13 11:42:00 INFO DStreamGraph: Updated checkpoint data for time 1431531720000 ms
15/05/13 11:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431531720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000'
15/05/13 11:42:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84153): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431531720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000'
15/05/13 11:42:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:42:00 INFO DAGScheduler: Got job 304 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:42:00 INFO DAGScheduler: Final stage: Stage 298(reduce at JsonRDD.scala:51)
15/05/13 11:42:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:42:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:42:00 INFO DAGScheduler: Submitting Stage 298 (MapPartitionsRDD[2128] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=35271872, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_448 stored as values in memory (estimated size 5.9 KB, free 231.8 MB)
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=35277912, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_448_piece0 stored as bytes in memory (estimated size 4.1 KB, free 231.8 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_448_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_448_piece0
15/05/13 11:42:00 INFO SparkContext: Created broadcast 448 from broadcast at DAGScheduler.scala:839
15/05/13 11:42:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 298 (MapPartitionsRDD[2128] at map at JsonRDD.scala:51)
15/05/13 11:42:00 INFO TaskSchedulerImpl: Adding task set 298.0 with 1 tasks
15/05/13 11:42:00 INFO TaskSetManager: Starting task 0.0 in stage 298.0 (TID 298, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:42:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84155): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:42:00 INFO CheckpointWriter: Saving checkpoint for time 1431531720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000'
15/05/13 11:42:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84157): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:42:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84157): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:42:00 WARN CheckpointWriter: Could not write checkpoint for time 1431531720000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000'
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_448_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.3 MB)
15/05/13 11:42:00 INFO TaskSetManager: Finished task 0.0 in stage 298.0 (TID 298) in 300 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:42:00 INFO TaskSchedulerImpl: Removed TaskSet 298.0, whose tasks have all completed, from pool 
15/05/13 11:42:00 INFO DAGScheduler: Stage 298 (reduce at JsonRDD.scala:51) finished in 0.310 s
15/05/13 11:42:00 INFO DAGScheduler: Job 304 finished: reduce at JsonRDD.scala:51, took 0.349185 s
15/05/13 11:42:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:42:00 INFO DAGScheduler: Got job 305 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:42:00 INFO DAGScheduler: Final stage: Stage 299(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:42:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:42:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:42:00 INFO DAGScheduler: Submitting Stage 299 (MapPartitionsRDD[2135] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=35282137, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_449 stored as values in memory (estimated size 20.4 KB, free 231.7 MB)
15/05/13 11:42:00 INFO BlockManager: Removing broadcast 448
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_448
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_448 of size 6040 dropped from memory (free 243005539)
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_448_piece0
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_448_piece0 of size 4225 dropped from memory (free 243009764)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_448_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_448_piece0
15/05/13 11:42:00 INFO MemoryStore: ensureFreeSpace(10901) called with curMem=35292792, maxMem=278302556
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_449_piece0 stored as bytes in memory (estimated size 10.6 KB, free 231.7 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_449_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_448_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_449_piece0
15/05/13 11:42:00 INFO SparkContext: Created broadcast 449 from broadcast at DAGScheduler.scala:839
15/05/13 11:42:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 299 (MapPartitionsRDD[2135] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:42:00 INFO TaskSchedulerImpl: Adding task set 299.0 with 1 tasks
15/05/13 11:42:00 INFO TaskSetManager: Starting task 0.0 in stage 299.0 (TID 299, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:42:00 INFO ContextCleaner: Cleaned broadcast 448
15/05/13 11:42:00 INFO BlockManager: Removing broadcast 446
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_446
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_446 of size 21480 dropped from memory (free 243020343)
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_446_piece0
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_446_piece0 of size 11113 dropped from memory (free 243031456)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_446_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_446_piece0
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_446_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 263.8 MB)
15/05/13 11:42:00 INFO ContextCleaner: Cleaned broadcast 446
15/05/13 11:42:00 INFO BlockManager: Removing broadcast 445
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_445_piece0
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_445_piece0 of size 4225 dropped from memory (free 243035681)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_445_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_445_piece0
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_445
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_445 of size 6040 dropped from memory (free 243041721)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_445_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:42:00 INFO ContextCleaner: Cleaned broadcast 445
15/05/13 11:42:00 INFO BlockManager: Removing broadcast 439
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_439
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_439 of size 6040 dropped from memory (free 243047761)
15/05/13 11:42:00 INFO BlockManager: Removing block broadcast_439_piece0
15/05/13 11:42:00 INFO MemoryStore: Block broadcast_439_piece0 of size 4225 dropped from memory (free 243051986)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_439_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:42:00 INFO BlockManagerMaster: Updated info of block broadcast_439_piece0
15/05/13 11:42:00 INFO BlockManagerInfo: Added broadcast_449_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 263.6 MB)
15/05/13 11:42:00 INFO BlockManagerInfo: Removed broadcast_439_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:42:01 INFO ContextCleaner: Cleaned broadcast 439
15/05/13 11:42:01 INFO BlockManager: Removing broadcast 443
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_443_piece0
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_443_piece0 of size 11107 dropped from memory (free 243063093)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_443_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.8 KB, free: 261.0 MB)
15/05/13 11:42:01 INFO BlockManagerMaster: Updated info of block broadcast_443_piece0
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_443
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_443 of size 21304 dropped from memory (free 243084397)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_443_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.8 KB, free: 262.3 MB)
15/05/13 11:42:01 INFO ContextCleaner: Cleaned broadcast 443
15/05/13 11:42:01 INFO BlockManager: Removing broadcast 442
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_442_piece0
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_442_piece0 of size 4225 dropped from memory (free 243088622)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_442_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:42:01 INFO BlockManagerMaster: Updated info of block broadcast_442_piece0
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_442
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_442 of size 6040 dropped from memory (free 243094662)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_442_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:42:01 INFO ContextCleaner: Cleaned broadcast 442
15/05/13 11:42:01 INFO BlockManager: Removing broadcast 440
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_440_piece0
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_440_piece0 of size 10907 dropped from memory (free 243105569)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_440_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 261.0 MB)
15/05/13 11:42:01 INFO BlockManagerMaster: Updated info of block broadcast_440_piece0
15/05/13 11:42:01 INFO BlockManager: Removing block broadcast_440
15/05/13 11:42:01 INFO MemoryStore: Block broadcast_440 of size 20920 dropped from memory (free 243126489)
15/05/13 11:42:01 INFO BlockManagerInfo: Removed broadcast_440_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.7 KB, free: 263.6 MB)
15/05/13 11:42:01 INFO ContextCleaner: Cleaned broadcast 440
15/05/13 11:42:01 INFO BlockManagerInfo: Added broadcast_447_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:42:01 INFO TaskSetManager: Finished task 0.0 in stage 299.0 (TID 299) in 550 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:42:01 INFO TaskSchedulerImpl: Removed TaskSet 299.0, whose tasks have all completed, from pool 
15/05/13 11:42:01 INFO DAGScheduler: Stage 299 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.551 s
15/05/13 11:42:01 INFO DAGScheduler: Job 305 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.636490 s
15/05/13 11:42:01 INFO JobScheduler: Finished job streaming job 1431531720000 ms.0 from job set of time 1431531720000 ms
15/05/13 11:42:01 INFO JobScheduler: Total delay: 1.528 s for time 1431531720000 ms (execution: 1.333 s)
15/05/13 11:42:01 INFO MapPartitionsRDD: Removing RDD 2110 from persistence list
15/05/13 11:42:01 INFO BlockManager: Removing RDD 2110
15/05/13 11:42:01 INFO UnionRDD: Removing RDD 2109 from persistence list
15/05/13 11:42:01 INFO BlockManager: Removing RDD 2109
15/05/13 11:42:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531660000 ms: 1431531600000 ms
15/05/13 11:42:01 INFO JobGenerator: Checkpointing graph for time 1431531720000 ms
15/05/13 11:42:01 INFO DStreamGraph: Updating checkpoint data for time 1431531720000 ms
15/05/13 11:42:01 INFO DStreamGraph: Updated checkpoint data for time 1431531720000 ms
15/05/13 11:42:01 INFO CheckpointWriter: Saving checkpoint for time 1431531720000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000'
15/05/13 11:42:01 INFO CheckpointWriter: Checkpoint for time 1431531720000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000', took 7655 bytes and 63 ms
15/05/13 11:42:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531720000 ms
15/05/13 11:42:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531720000 ms
15/05/13 11:42:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:43:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 11:43:00 INFO FileInputDStream: New files at time 1431531780000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531692465.json
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=35176067, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_450 stored as values in memory (estimated size 232.9 KB, free 231.6 MB)
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=35414599, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_450_piece0 stored as bytes in memory (estimated size 34.9 KB, free 231.6 MB)
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 261.0 MB)
15/05/13 11:43:00 INFO BlockManagerMaster: Updated info of block broadcast_450_piece0
15/05/13 11:43:00 INFO SparkContext: Created broadcast 450 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:43:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:43:00 INFO JobScheduler: Added jobs for time 1431531780000 ms
15/05/13 11:43:00 INFO JobGenerator: Checkpointing graph for time 1431531780000 ms
15/05/13 11:43:00 INFO DStreamGraph: Updating checkpoint data for time 1431531780000 ms
15/05/13 11:43:00 INFO JobScheduler: Starting job streaming job 1431531780000 ms.0 from job set of time 1431531780000 ms
15/05/13 11:43:00 INFO DStreamGraph: Updated checkpoint data for time 1431531780000 ms
15/05/13 11:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431531780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000'
15/05/13 11:43:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84163): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431531780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000'
15/05/13 11:43:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:43:00 INFO DAGScheduler: Got job 306 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:43:00 INFO DAGScheduler: Final stage: Stage 300(reduce at JsonRDD.scala:51)
15/05/13 11:43:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:43:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:43:00 INFO DAGScheduler: Submitting Stage 300 (MapPartitionsRDD[2142] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=35450307, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_451 stored as values in memory (estimated size 5.9 KB, free 231.6 MB)
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=35456347, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_451_piece0 stored as bytes in memory (estimated size 4.1 KB, free 231.6 MB)
15/05/13 11:43:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84165): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_451_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:43:00 INFO CheckpointWriter: Saving checkpoint for time 1431531780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000'
15/05/13 11:43:00 INFO BlockManagerMaster: Updated info of block broadcast_451_piece0
15/05/13 11:43:00 INFO SparkContext: Created broadcast 451 from broadcast at DAGScheduler.scala:839
15/05/13 11:43:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 300 (MapPartitionsRDD[2142] at map at JsonRDD.scala:51)
15/05/13 11:43:00 INFO TaskSchedulerImpl: Adding task set 300.0 with 1 tasks
15/05/13 11:43:00 INFO TaskSetManager: Starting task 0.0 in stage 300.0 (TID 300, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:43:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531480000.bk
15/05/13 11:43:00 INFO CheckpointWriter: Checkpoint for time 1431531780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000', took 7662 bytes and 194 ms
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_451_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.3 MB)
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_450_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:43:00 INFO TaskSetManager: Finished task 0.0 in stage 300.0 (TID 300) in 342 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:43:00 INFO DAGScheduler: Stage 300 (reduce at JsonRDD.scala:51) finished in 0.360 s
15/05/13 11:43:00 INFO TaskSchedulerImpl: Removed TaskSet 300.0, whose tasks have all completed, from pool 
15/05/13 11:43:00 INFO DAGScheduler: Job 306 finished: reduce at JsonRDD.scala:51, took 0.402301 s
15/05/13 11:43:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:43:00 INFO DAGScheduler: Got job 307 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:43:00 INFO DAGScheduler: Final stage: Stage 301(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:43:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:43:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:43:00 INFO DAGScheduler: Submitting Stage 301 (MapPartitionsRDD[2149] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(22240) called with curMem=35460572, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_452 stored as values in memory (estimated size 21.7 KB, free 231.6 MB)
15/05/13 11:43:00 INFO MemoryStore: ensureFreeSpace(11415) called with curMem=35482812, maxMem=278302556
15/05/13 11:43:00 INFO MemoryStore: Block broadcast_452_piece0 stored as bytes in memory (estimated size 11.1 KB, free 231.6 MB)
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_452_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.1 KB, free: 261.0 MB)
15/05/13 11:43:00 INFO BlockManagerMaster: Updated info of block broadcast_452_piece0
15/05/13 11:43:00 INFO SparkContext: Created broadcast 452 from broadcast at DAGScheduler.scala:839
15/05/13 11:43:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 301 (MapPartitionsRDD[2149] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:43:00 INFO TaskSchedulerImpl: Adding task set 301.0 with 1 tasks
15/05/13 11:43:00 INFO TaskSetManager: Starting task 0.0 in stage 301.0 (TID 301, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:43:00 INFO BlockManagerInfo: Added broadcast_452_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.1 KB, free: 262.2 MB)
15/05/13 11:43:01 INFO TaskSetManager: Finished task 0.0 in stage 301.0 (TID 301) in 85 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:43:01 INFO TaskSchedulerImpl: Removed TaskSet 301.0, whose tasks have all completed, from pool 
15/05/13 11:43:01 INFO DAGScheduler: Stage 301 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.087 s
15/05/13 11:43:01 INFO DAGScheduler: Job 307 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.115668 s
15/05/13 11:43:01 INFO JobScheduler: Finished job streaming job 1431531780000 ms.0 from job set of time 1431531780000 ms
15/05/13 11:43:01 INFO JobScheduler: Total delay: 1.076 s for time 1431531780000 ms (execution: 0.897 s)
15/05/13 11:43:01 INFO MapPartitionsRDD: Removing RDD 2124 from persistence list
15/05/13 11:43:01 INFO BlockManager: Removing RDD 2124
15/05/13 11:43:01 INFO UnionRDD: Removing RDD 2123 from persistence list
15/05/13 11:43:01 INFO BlockManager: Removing RDD 2123
15/05/13 11:43:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531720000 ms: 1431531660000 ms
15/05/13 11:43:01 INFO JobGenerator: Checkpointing graph for time 1431531780000 ms
15/05/13 11:43:01 INFO DStreamGraph: Updating checkpoint data for time 1431531780000 ms
15/05/13 11:43:01 INFO DStreamGraph: Updated checkpoint data for time 1431531780000 ms
15/05/13 11:43:01 INFO CheckpointWriter: Saving checkpoint for time 1431531780000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000'
15/05/13 11:43:01 INFO CheckpointWriter: Checkpoint for time 1431531780000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000', took 7654 bytes and 49 ms
15/05/13 11:43:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531780000 ms
15/05/13 11:43:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531780000 ms
15/05/13 11:43:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:44:00 INFO FileInputDStream: Finding new files took 53 ms
15/05/13 11:44:00 INFO FileInputDStream: New files at time 1431531840000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531753762.json
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=35494227, maxMem=278302556
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_453 stored as values in memory (estimated size 232.9 KB, free 231.3 MB)
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=35732759, maxMem=278302556
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_453_piece0 stored as bytes in memory (estimated size 34.9 KB, free 231.3 MB)
15/05/13 11:44:00 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:44:00 INFO BlockManagerMaster: Updated info of block broadcast_453_piece0
15/05/13 11:44:00 INFO SparkContext: Created broadcast 453 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:44:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:44:00 INFO JobScheduler: Added jobs for time 1431531840000 ms
15/05/13 11:44:00 INFO JobGenerator: Checkpointing graph for time 1431531840000 ms
15/05/13 11:44:00 INFO DStreamGraph: Updating checkpoint data for time 1431531840000 ms
15/05/13 11:44:00 INFO JobScheduler: Starting job streaming job 1431531840000 ms.0 from job set of time 1431531840000 ms
15/05/13 11:44:00 INFO DStreamGraph: Updated checkpoint data for time 1431531840000 ms
15/05/13 11:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431531840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000'
15/05/13 11:44:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:44:00 INFO DAGScheduler: Got job 308 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:44:00 INFO DAGScheduler: Final stage: Stage 302(reduce at JsonRDD.scala:51)
15/05/13 11:44:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:44:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:44:00 INFO DAGScheduler: Submitting Stage 302 (MapPartitionsRDD[2156] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=35768467, maxMem=278302556
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_454 stored as values in memory (estimated size 5.9 KB, free 231.3 MB)
15/05/13 11:44:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84178): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=35774507, maxMem=278302556
15/05/13 11:44:00 INFO CheckpointWriter: Saving checkpoint for time 1431531840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000'
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_454_piece0 stored as bytes in memory (estimated size 4.1 KB, free 231.3 MB)
15/05/13 11:44:00 INFO BlockManagerInfo: Added broadcast_454_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:44:00 INFO BlockManagerMaster: Updated info of block broadcast_454_piece0
15/05/13 11:44:00 INFO SparkContext: Created broadcast 454 from broadcast at DAGScheduler.scala:839
15/05/13 11:44:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 302 (MapPartitionsRDD[2156] at map at JsonRDD.scala:51)
15/05/13 11:44:00 INFO TaskSchedulerImpl: Adding task set 302.0 with 1 tasks
15/05/13 11:44:00 INFO TaskSetManager: Starting task 0.0 in stage 302.0 (TID 302, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:44:00 INFO BlockManagerInfo: Added broadcast_454_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.8 MB)
15/05/13 11:44:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531540000
15/05/13 11:44:00 INFO CheckpointWriter: Checkpoint for time 1431531840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000', took 7665 bytes and 197 ms
15/05/13 11:44:00 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:44:00 INFO TaskSetManager: Finished task 0.0 in stage 302.0 (TID 302) in 309 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:44:00 INFO TaskSchedulerImpl: Removed TaskSet 302.0, whose tasks have all completed, from pool 
15/05/13 11:44:00 INFO DAGScheduler: Stage 302 (reduce at JsonRDD.scala:51) finished in 0.324 s
15/05/13 11:44:00 INFO DAGScheduler: Job 308 finished: reduce at JsonRDD.scala:51, took 0.377441 s
15/05/13 11:44:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:44:00 INFO DAGScheduler: Got job 309 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:44:00 INFO DAGScheduler: Final stage: Stage 303(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:44:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:44:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:44:00 INFO DAGScheduler: Submitting Stage 303 (MapPartitionsRDD[2163] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=35778732, maxMem=278302556
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_455 stored as values in memory (estimated size 20.4 KB, free 231.3 MB)
15/05/13 11:44:00 INFO MemoryStore: ensureFreeSpace(10900) called with curMem=35799652, maxMem=278302556
15/05/13 11:44:00 INFO MemoryStore: Block broadcast_455_piece0 stored as bytes in memory (estimated size 10.6 KB, free 231.3 MB)
15/05/13 11:44:00 INFO BlockManagerInfo: Added broadcast_455_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 260.9 MB)
15/05/13 11:44:00 INFO BlockManagerMaster: Updated info of block broadcast_455_piece0
15/05/13 11:44:00 INFO SparkContext: Created broadcast 455 from broadcast at DAGScheduler.scala:839
15/05/13 11:44:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 303 (MapPartitionsRDD[2163] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:44:00 INFO TaskSchedulerImpl: Adding task set 303.0 with 1 tasks
15/05/13 11:44:00 INFO TaskSetManager: Starting task 0.0 in stage 303.0 (TID 303, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:44:01 INFO BlockManagerInfo: Added broadcast_455_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.2 MB)
15/05/13 11:44:01 INFO BlockManagerInfo: Added broadcast_453_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:44:01 INFO TaskSetManager: Finished task 0.0 in stage 303.0 (TID 303) in 279 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:44:01 INFO TaskSchedulerImpl: Removed TaskSet 303.0, whose tasks have all completed, from pool 
15/05/13 11:44:01 INFO DAGScheduler: Stage 303 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.281 s
15/05/13 11:44:01 INFO DAGScheduler: Job 309 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.309872 s
15/05/13 11:44:01 INFO JobScheduler: Finished job streaming job 1431531840000 ms.0 from job set of time 1431531840000 ms
15/05/13 11:44:01 INFO JobScheduler: Total delay: 1.273 s for time 1431531840000 ms (execution: 1.062 s)
15/05/13 11:44:01 INFO MapPartitionsRDD: Removing RDD 2138 from persistence list
15/05/13 11:44:01 INFO BlockManager: Removing RDD 2138
15/05/13 11:44:01 INFO UnionRDD: Removing RDD 2137 from persistence list
15/05/13 11:44:01 INFO BlockManager: Removing RDD 2137
15/05/13 11:44:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531780000 ms: 1431531720000 ms
15/05/13 11:44:01 INFO JobGenerator: Checkpointing graph for time 1431531840000 ms
15/05/13 11:44:01 INFO DStreamGraph: Updating checkpoint data for time 1431531840000 ms
15/05/13 11:44:01 INFO DStreamGraph: Updated checkpoint data for time 1431531840000 ms
15/05/13 11:44:01 INFO CheckpointWriter: Saving checkpoint for time 1431531840000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000'
15/05/13 11:44:01 INFO CheckpointWriter: Checkpoint for time 1431531840000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531840000', took 7657 bytes and 53 ms
15/05/13 11:44:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531840000 ms
15/05/13 11:44:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531840000 ms
15/05/13 11:44:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:45:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 11:45:00 INFO FileInputDStream: New files at time 1431531900000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531815436.json
15/05/13 11:45:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=35810552, maxMem=278302556
15/05/13 11:45:00 INFO MemoryStore: Block broadcast_456 stored as values in memory (estimated size 232.9 KB, free 231.0 MB)
15/05/13 11:45:00 INFO MemoryStore: ensureFreeSpace(35710) called with curMem=36049084, maxMem=278302556
15/05/13 11:45:00 INFO MemoryStore: Block broadcast_456_piece0 stored as bytes in memory (estimated size 34.9 KB, free 231.0 MB)
15/05/13 11:45:00 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:45:00 INFO BlockManagerMaster: Updated info of block broadcast_456_piece0
15/05/13 11:45:00 INFO SparkContext: Created broadcast 456 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:45:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:45:00 INFO JobScheduler: Added jobs for time 1431531900000 ms
15/05/13 11:45:00 INFO JobGenerator: Checkpointing graph for time 1431531900000 ms
15/05/13 11:45:00 INFO DStreamGraph: Updating checkpoint data for time 1431531900000 ms
15/05/13 11:45:00 INFO JobScheduler: Starting job streaming job 1431531900000 ms.0 from job set of time 1431531900000 ms
15/05/13 11:45:00 INFO DStreamGraph: Updated checkpoint data for time 1431531900000 ms
15/05/13 11:45:00 INFO CheckpointWriter: Saving checkpoint for time 1431531900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531900000'
15/05/13 11:45:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531900000
15/05/13 11:45:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:45:00 INFO DAGScheduler: Got job 310 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:45:00 INFO DAGScheduler: Final stage: Stage 304(reduce at JsonRDD.scala:51)
15/05/13 11:45:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:45:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:45:00 INFO DAGScheduler: Submitting Stage 304 (MapPartitionsRDD[2170] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:45:00 INFO CheckpointWriter: Checkpoint for time 1431531900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531900000', took 7666 bytes and 128 ms
15/05/13 11:45:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36084794, maxMem=278302556
15/05/13 11:45:00 INFO MemoryStore: Block broadcast_457 stored as values in memory (estimated size 5.9 KB, free 231.0 MB)
15/05/13 11:45:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36090834, maxMem=278302556
15/05/13 11:45:00 INFO MemoryStore: Block broadcast_457_piece0 stored as bytes in memory (estimated size 4.1 KB, free 231.0 MB)
15/05/13 11:45:00 INFO BlockManagerInfo: Added broadcast_457_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:45:00 INFO BlockManagerMaster: Updated info of block broadcast_457_piece0
15/05/13 11:45:00 INFO SparkContext: Created broadcast 457 from broadcast at DAGScheduler.scala:839
15/05/13 11:45:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 304 (MapPartitionsRDD[2170] at map at JsonRDD.scala:51)
15/05/13 11:45:00 INFO TaskSchedulerImpl: Adding task set 304.0 with 1 tasks
15/05/13 11:45:00 INFO TaskSetManager: Starting task 0.0 in stage 304.0 (TID 304, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:45:00 INFO BlockManagerInfo: Added broadcast_457_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.2 MB)
15/05/13 11:45:00 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:45:00 INFO TaskSetManager: Finished task 0.0 in stage 304.0 (TID 304) in 392 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:45:00 INFO TaskSchedulerImpl: Removed TaskSet 304.0, whose tasks have all completed, from pool 
15/05/13 11:45:00 INFO DAGScheduler: Stage 304 (reduce at JsonRDD.scala:51) finished in 0.401 s
15/05/13 11:45:00 INFO DAGScheduler: Job 310 finished: reduce at JsonRDD.scala:51, took 0.450462 s
15/05/13 11:45:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:45:01 INFO DAGScheduler: Got job 311 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:45:01 INFO DAGScheduler: Final stage: Stage 305(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:45:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:45:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:45:01 INFO DAGScheduler: Submitting Stage 305 (MapPartitionsRDD[2177] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:45:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=36095059, maxMem=278302556
15/05/13 11:45:01 INFO MemoryStore: Block broadcast_458 stored as values in memory (estimated size 20.4 KB, free 231.0 MB)
15/05/13 11:45:01 INFO MemoryStore: ensureFreeSpace(10908) called with curMem=36115979, maxMem=278302556
15/05/13 11:45:01 INFO MemoryStore: Block broadcast_458_piece0 stored as bytes in memory (estimated size 10.7 KB, free 231.0 MB)
15/05/13 11:45:01 INFO BlockManagerInfo: Added broadcast_458_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 260.9 MB)
15/05/13 11:45:01 INFO BlockManagerMaster: Updated info of block broadcast_458_piece0
15/05/13 11:45:01 INFO SparkContext: Created broadcast 458 from broadcast at DAGScheduler.scala:839
15/05/13 11:45:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 305 (MapPartitionsRDD[2177] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:45:01 INFO TaskSchedulerImpl: Adding task set 305.0 with 1 tasks
15/05/13 11:45:01 INFO TaskSetManager: Starting task 0.0 in stage 305.0 (TID 305, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:45:01 INFO BlockManagerInfo: Added broadcast_458_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 263.7 MB)
15/05/13 11:45:01 INFO BlockManagerInfo: Added broadcast_456_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:45:01 INFO DAGScheduler: Stage 305 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.215 s
15/05/13 11:45:01 INFO TaskSetManager: Finished task 0.0 in stage 305.0 (TID 305) in 212 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:45:01 INFO TaskSchedulerImpl: Removed TaskSet 305.0, whose tasks have all completed, from pool 
15/05/13 11:45:01 INFO DAGScheduler: Job 311 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.248027 s
15/05/13 11:45:01 INFO JobScheduler: Finished job streaming job 1431531900000 ms.0 from job set of time 1431531900000 ms
15/05/13 11:45:01 INFO JobScheduler: Total delay: 1.415 s for time 1431531900000 ms (execution: 1.209 s)
15/05/13 11:45:01 INFO MapPartitionsRDD: Removing RDD 2152 from persistence list
15/05/13 11:45:01 INFO BlockManager: Removing RDD 2152
15/05/13 11:45:01 INFO UnionRDD: Removing RDD 2151 from persistence list
15/05/13 11:45:01 INFO BlockManager: Removing RDD 2151
15/05/13 11:45:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531840000 ms: 1431531780000 ms
15/05/13 11:45:01 INFO JobGenerator: Checkpointing graph for time 1431531900000 ms
15/05/13 11:45:01 INFO DStreamGraph: Updating checkpoint data for time 1431531900000 ms
15/05/13 11:45:01 INFO DStreamGraph: Updated checkpoint data for time 1431531900000 ms
15/05/13 11:45:01 INFO CheckpointWriter: Saving checkpoint for time 1431531900000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531900000'
15/05/13 11:45:01 INFO CheckpointWriter: Checkpoint for time 1431531900000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531900000', took 7657 bytes and 80 ms
15/05/13 11:45:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531900000 ms
15/05/13 11:45:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531900000 ms
15/05/13 11:45:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:46:00 INFO FileInputDStream: Finding new files took 45 ms
15/05/13 11:46:00 INFO FileInputDStream: New files at time 1431531960000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531878407.json
15/05/13 11:46:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36126887, maxMem=278302556
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_459 stored as values in memory (estimated size 232.9 KB, free 230.7 MB)
15/05/13 11:46:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=36365419, maxMem=278302556
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_459_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.7 MB)
15/05/13 11:46:00 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:46:00 INFO BlockManagerMaster: Updated info of block broadcast_459_piece0
15/05/13 11:46:00 INFO SparkContext: Created broadcast 459 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:46:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:46:00 INFO JobScheduler: Added jobs for time 1431531960000 ms
15/05/13 11:46:00 INFO JobGenerator: Checkpointing graph for time 1431531960000 ms
15/05/13 11:46:00 INFO DStreamGraph: Updating checkpoint data for time 1431531960000 ms
15/05/13 11:46:00 INFO DStreamGraph: Updated checkpoint data for time 1431531960000 ms
15/05/13 11:46:00 INFO JobScheduler: Starting job streaming job 1431531960000 ms.0 from job set of time 1431531960000 ms
15/05/13 11:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431531960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000'
15/05/13 11:46:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84192): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:46:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84192): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431531960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000'
15/05/13 11:46:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:46:00 INFO DAGScheduler: Got job 312 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:46:00 INFO DAGScheduler: Final stage: Stage 306(reduce at JsonRDD.scala:51)
15/05/13 11:46:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:46:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:46:00 INFO DAGScheduler: Submitting Stage 306 (MapPartitionsRDD[2184] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:46:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36401127, maxMem=278302556
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_460 stored as values in memory (estimated size 5.9 KB, free 230.7 MB)
15/05/13 11:46:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36407167, maxMem=278302556
15/05/13 11:46:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84194): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:46:00 INFO CheckpointWriter: Saving checkpoint for time 1431531960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000'
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_460_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.7 MB)
15/05/13 11:46:00 INFO BlockManagerInfo: Added broadcast_460_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:46:00 INFO BlockManagerMaster: Updated info of block broadcast_460_piece0
15/05/13 11:46:00 INFO SparkContext: Created broadcast 460 from broadcast at DAGScheduler.scala:839
15/05/13 11:46:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 306 (MapPartitionsRDD[2184] at map at JsonRDD.scala:51)
15/05/13 11:46:00 INFO TaskSchedulerImpl: Adding task set 306.0 with 1 tasks
15/05/13 11:46:00 INFO TaskSetManager: Starting task 0.0 in stage 306.0 (TID 306, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:46:00 INFO BlockManagerInfo: Added broadcast_460_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:46:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531660000.bk
15/05/13 11:46:00 INFO CheckpointWriter: Checkpoint for time 1431531960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000', took 7665 bytes and 217 ms
15/05/13 11:46:00 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:46:00 INFO TaskSetManager: Finished task 0.0 in stage 306.0 (TID 306) in 450 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:46:00 INFO DAGScheduler: Stage 306 (reduce at JsonRDD.scala:51) finished in 0.465 s
15/05/13 11:46:00 INFO TaskSchedulerImpl: Removed TaskSet 306.0, whose tasks have all completed, from pool 
15/05/13 11:46:00 INFO DAGScheduler: Job 312 finished: reduce at JsonRDD.scala:51, took 0.524293 s
15/05/13 11:46:00 INFO BlockManager: Removing broadcast 460
15/05/13 11:46:00 INFO BlockManager: Removing block broadcast_460_piece0
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_460_piece0 of size 4225 dropped from memory (free 241895389)
15/05/13 11:46:00 INFO BlockManagerInfo: Removed broadcast_460_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:46:00 INFO BlockManagerMaster: Updated info of block broadcast_460_piece0
15/05/13 11:46:00 INFO BlockManager: Removing block broadcast_460
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_460 of size 6040 dropped from memory (free 241901429)
15/05/13 11:46:00 INFO BlockManagerInfo: Removed broadcast_460_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:46:00 INFO ContextCleaner: Cleaned broadcast 460
15/05/13 11:46:00 INFO BlockManager: Removing broadcast 458
15/05/13 11:46:00 INFO BlockManager: Removing block broadcast_458_piece0
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_458_piece0 of size 10908 dropped from memory (free 241912337)
15/05/13 11:46:00 INFO BlockManagerInfo: Removed broadcast_458_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 260.9 MB)
15/05/13 11:46:00 INFO BlockManagerMaster: Updated info of block broadcast_458_piece0
15/05/13 11:46:00 INFO BlockManager: Removing block broadcast_458
15/05/13 11:46:00 INFO MemoryStore: Block broadcast_458 of size 20920 dropped from memory (free 241933257)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_458_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 263.7 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 458
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 457
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_457
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_457 of size 6040 dropped from memory (free 241939297)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_457_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_457_piece0 of size 4225 dropped from memory (free 241943522)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_457_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_457_piece0
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_457_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 457
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 453
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_453
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_453 of size 238532 dropped from memory (free 242182054)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_453_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_453_piece0 of size 35708 dropped from memory (free 242217762)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_453_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_453_piece0
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_453_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_453_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 453
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 452
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_452
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_452 of size 22240 dropped from memory (free 242240002)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_452_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_452_piece0 of size 11415 dropped from memory (free 242251417)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_452_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.1 KB, free: 260.9 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_452_piece0
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_452_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.1 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 452
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 451
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_451
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_451 of size 6040 dropped from memory (free 242257457)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_451_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_451_piece0 of size 4225 dropped from memory (free 242261682)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_451_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_451_piece0
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_451_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 451
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 450
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_450_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_450_piece0 of size 35708 dropped from memory (free 242297390)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_450_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_450_piece0
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_450
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_450 of size 238532 dropped from memory (free 242535922)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_450_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 450
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 449
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_449
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_449 of size 20920 dropped from memory (free 242556842)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_449_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_449_piece0 of size 10901 dropped from memory (free 242567743)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_449_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.0 MB)
15/05/13 11:46:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_449_piece0
15/05/13 11:46:01 INFO DAGScheduler: Got job 313 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:46:01 INFO DAGScheduler: Final stage: Stage 307(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:46:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_449_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 263.6 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 449
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 455
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_455
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_455 of size 20920 dropped from memory (free 242588663)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_455_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_455_piece0 of size 10900 dropped from memory (free 242599563)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_455_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 261.0 MB)
15/05/13 11:46:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_455_piece0
15/05/13 11:46:01 INFO DAGScheduler: Submitting Stage 307 (MapPartitionsRDD[2191] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_455_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 455
15/05/13 11:46:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=35702993, maxMem=278302556
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_461 stored as values in memory (estimated size 20.4 KB, free 231.3 MB)
15/05/13 11:46:01 INFO BlockManager: Removing broadcast 454
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_454
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_454 of size 6040 dropped from memory (free 242584683)
15/05/13 11:46:01 INFO BlockManager: Removing block broadcast_454_piece0
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_454_piece0 of size 4225 dropped from memory (free 242588908)
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_454_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 261.0 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_454_piece0
15/05/13 11:46:01 INFO BlockManagerInfo: Removed broadcast_454_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:46:01 INFO ContextCleaner: Cleaned broadcast 454
15/05/13 11:46:01 INFO MemoryStore: ensureFreeSpace(10849) called with curMem=35713648, maxMem=278302556
15/05/13 11:46:01 INFO MemoryStore: Block broadcast_461_piece0 stored as bytes in memory (estimated size 10.6 KB, free 231.3 MB)
15/05/13 11:46:01 INFO BlockManagerInfo: Added broadcast_461_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 261.0 MB)
15/05/13 11:46:01 INFO BlockManagerMaster: Updated info of block broadcast_461_piece0
15/05/13 11:46:01 INFO SparkContext: Created broadcast 461 from broadcast at DAGScheduler.scala:839
15/05/13 11:46:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 307 (MapPartitionsRDD[2191] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:46:01 INFO TaskSchedulerImpl: Adding task set 307.0 with 1 tasks
15/05/13 11:46:01 INFO TaskSetManager: Starting task 0.0 in stage 307.0 (TID 307, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:46:01 INFO BlockManagerInfo: Added broadcast_461_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO BlockManagerInfo: Added broadcast_459_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:46:01 INFO TaskSetManager: Finished task 0.0 in stage 307.0 (TID 307) in 283 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:46:01 INFO TaskSchedulerImpl: Removed TaskSet 307.0, whose tasks have all completed, from pool 
15/05/13 11:46:01 INFO DAGScheduler: Stage 307 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.285 s
15/05/13 11:46:01 INFO DAGScheduler: Job 313 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.336575 s
15/05/13 11:46:01 INFO JobScheduler: Finished job streaming job 1431531960000 ms.0 from job set of time 1431531960000 ms
15/05/13 11:46:01 INFO JobScheduler: Total delay: 1.487 s for time 1431531960000 ms (execution: 1.287 s)
15/05/13 11:46:01 INFO MapPartitionsRDD: Removing RDD 2166 from persistence list
15/05/13 11:46:01 INFO BlockManager: Removing RDD 2166
15/05/13 11:46:01 INFO UnionRDD: Removing RDD 2165 from persistence list
15/05/13 11:46:01 INFO BlockManager: Removing RDD 2165
15/05/13 11:46:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531900000 ms: 1431531840000 ms
15/05/13 11:46:01 INFO JobGenerator: Checkpointing graph for time 1431531960000 ms
15/05/13 11:46:01 INFO DStreamGraph: Updating checkpoint data for time 1431531960000 ms
15/05/13 11:46:01 INFO DStreamGraph: Updated checkpoint data for time 1431531960000 ms
15/05/13 11:46:01 INFO CheckpointWriter: Saving checkpoint for time 1431531960000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000'
15/05/13 11:46:01 INFO CheckpointWriter: Checkpoint for time 1431531960000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000', took 7654 bytes and 49 ms
15/05/13 11:46:01 INFO DStreamGraph: Clearing checkpoint data for time 1431531960000 ms
15/05/13 11:46:01 INFO DStreamGraph: Cleared checkpoint data for time 1431531960000 ms
15/05/13 11:46:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:47:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 11:47:00 INFO FileInputDStream: New files at time 1431532020000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431531938756.json
15/05/13 11:47:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=35724497, maxMem=278302556
15/05/13 11:47:00 INFO MemoryStore: Block broadcast_462 stored as values in memory (estimated size 232.9 KB, free 231.1 MB)
15/05/13 11:47:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=35963029, maxMem=278302556
15/05/13 11:47:00 INFO MemoryStore: Block broadcast_462_piece0 stored as bytes in memory (estimated size 34.9 KB, free 231.1 MB)
15/05/13 11:47:00 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:47:00 INFO BlockManagerMaster: Updated info of block broadcast_462_piece0
15/05/13 11:47:00 INFO SparkContext: Created broadcast 462 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:47:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:47:00 INFO JobScheduler: Added jobs for time 1431532020000 ms
15/05/13 11:47:00 INFO JobGenerator: Checkpointing graph for time 1431532020000 ms
15/05/13 11:47:00 INFO JobScheduler: Starting job streaming job 1431532020000 ms.0 from job set of time 1431532020000 ms
15/05/13 11:47:00 INFO DStreamGraph: Updating checkpoint data for time 1431532020000 ms
15/05/13 11:47:00 INFO DStreamGraph: Updated checkpoint data for time 1431532020000 ms
15/05/13 11:47:00 INFO CheckpointWriter: Saving checkpoint for time 1431532020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000'
15/05/13 11:47:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84202): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:47:00 INFO CheckpointWriter: Saving checkpoint for time 1431532020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000'
15/05/13 11:47:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:47:00 INFO DAGScheduler: Got job 314 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:47:00 INFO DAGScheduler: Final stage: Stage 308(reduce at JsonRDD.scala:51)
15/05/13 11:47:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:47:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:47:00 INFO DAGScheduler: Submitting Stage 308 (MapPartitionsRDD[2198] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:47:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=35998737, maxMem=278302556
15/05/13 11:47:00 INFO MemoryStore: Block broadcast_463 stored as values in memory (estimated size 5.9 KB, free 231.1 MB)
15/05/13 11:47:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36004777, maxMem=278302556
15/05/13 11:47:00 INFO MemoryStore: Block broadcast_463_piece0 stored as bytes in memory (estimated size 4.1 KB, free 231.1 MB)
15/05/13 11:47:00 INFO BlockManagerInfo: Added broadcast_463_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:47:00 INFO BlockManagerMaster: Updated info of block broadcast_463_piece0
15/05/13 11:47:00 INFO SparkContext: Created broadcast 463 from broadcast at DAGScheduler.scala:839
15/05/13 11:47:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 308 (MapPartitionsRDD[2198] at map at JsonRDD.scala:51)
15/05/13 11:47:00 INFO TaskSchedulerImpl: Adding task set 308.0 with 1 tasks
15/05/13 11:47:00 INFO TaskSetManager: Starting task 0.0 in stage 308.0 (TID 308, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:47:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531720000.bk
15/05/13 11:47:00 INFO CheckpointWriter: Checkpoint for time 1431532020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000', took 7661 bytes and 123 ms
15/05/13 11:47:00 INFO BlockManagerInfo: Added broadcast_463_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.2 MB)
15/05/13 11:47:00 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.2 MB)
15/05/13 11:47:00 INFO DAGScheduler: Stage 308 (reduce at JsonRDD.scala:51) finished in 0.461 s
15/05/13 11:47:00 INFO TaskSetManager: Finished task 0.0 in stage 308.0 (TID 308) in 447 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:47:00 INFO TaskSchedulerImpl: Removed TaskSet 308.0, whose tasks have all completed, from pool 
15/05/13 11:47:00 INFO DAGScheduler: Job 314 finished: reduce at JsonRDD.scala:51, took 0.491694 s
15/05/13 11:47:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:47:01 INFO DAGScheduler: Got job 315 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:47:01 INFO DAGScheduler: Final stage: Stage 309(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:47:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:47:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:47:01 INFO DAGScheduler: Submitting Stage 309 (MapPartitionsRDD[2205] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:47:01 INFO MemoryStore: ensureFreeSpace(21304) called with curMem=36009002, maxMem=278302556
15/05/13 11:47:01 INFO MemoryStore: Block broadcast_464 stored as values in memory (estimated size 20.8 KB, free 231.0 MB)
15/05/13 11:47:01 INFO MemoryStore: ensureFreeSpace(11135) called with curMem=36030306, maxMem=278302556
15/05/13 11:47:01 INFO MemoryStore: Block broadcast_464_piece0 stored as bytes in memory (estimated size 10.9 KB, free 231.0 MB)
15/05/13 11:47:01 INFO BlockManagerInfo: Added broadcast_464_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.9 MB)
15/05/13 11:47:01 INFO BlockManagerMaster: Updated info of block broadcast_464_piece0
15/05/13 11:47:01 INFO SparkContext: Created broadcast 464 from broadcast at DAGScheduler.scala:839
15/05/13 11:47:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 309 (MapPartitionsRDD[2205] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:47:01 INFO TaskSchedulerImpl: Adding task set 309.0 with 1 tasks
15/05/13 11:47:01 INFO TaskSetManager: Starting task 0.0 in stage 309.0 (TID 309, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:47:01 INFO BlockManagerInfo: Added broadcast_464_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:47:01 INFO BlockManagerInfo: Added broadcast_462_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:47:01 INFO TaskSetManager: Finished task 0.0 in stage 309.0 (TID 309) in 191 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:47:01 INFO TaskSchedulerImpl: Removed TaskSet 309.0, whose tasks have all completed, from pool 
15/05/13 11:47:01 INFO DAGScheduler: Stage 309 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.193 s
15/05/13 11:47:01 INFO DAGScheduler: Job 315 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.234637 s
15/05/13 11:47:01 INFO JobScheduler: Finished job streaming job 1431532020000 ms.0 from job set of time 1431532020000 ms
15/05/13 11:47:01 INFO JobScheduler: Total delay: 1.284 s for time 1431532020000 ms (execution: 1.135 s)
15/05/13 11:47:01 INFO MapPartitionsRDD: Removing RDD 2180 from persistence list
15/05/13 11:47:01 INFO BlockManager: Removing RDD 2180
15/05/13 11:47:01 INFO UnionRDD: Removing RDD 2179 from persistence list
15/05/13 11:47:01 INFO BlockManager: Removing RDD 2179
15/05/13 11:47:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431531960000 ms: 1431531900000 ms
15/05/13 11:47:01 INFO JobGenerator: Checkpointing graph for time 1431532020000 ms
15/05/13 11:47:01 INFO DStreamGraph: Updating checkpoint data for time 1431532020000 ms
15/05/13 11:47:01 INFO DStreamGraph: Updated checkpoint data for time 1431532020000 ms
15/05/13 11:47:01 INFO CheckpointWriter: Saving checkpoint for time 1431532020000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000'
15/05/13 11:47:01 INFO CheckpointWriter: Checkpoint for time 1431532020000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532020000', took 7655 bytes and 65 ms
15/05/13 11:47:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532020000 ms
15/05/13 11:47:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532020000 ms
15/05/13 11:47:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:48:00 INFO FileInputDStream: Finding new files took 46 ms
15/05/13 11:48:00 INFO FileInputDStream: New files at time 1431532080000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532004862.json
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36041441, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_465 stored as values in memory (estimated size 232.9 KB, free 230.8 MB)
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=36279973, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_465_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.8 MB)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_465_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:48:00 INFO BlockManagerMaster: Updated info of block broadcast_465_piece0
15/05/13 11:48:00 INFO SparkContext: Created broadcast 465 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:48:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:48:00 INFO JobScheduler: Added jobs for time 1431532080000 ms
15/05/13 11:48:00 INFO JobGenerator: Checkpointing graph for time 1431532080000 ms
15/05/13 11:48:00 INFO DStreamGraph: Updating checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO JobScheduler: Starting job streaming job 1431532080000 ms.0 from job set of time 1431532080000 ms
15/05/13 11:48:00 INFO DStreamGraph: Updated checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO CheckpointWriter: Saving checkpoint for time 1431532080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532080000'
15/05/13 11:48:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531780000.bk
15/05/13 11:48:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:48:00 INFO DAGScheduler: Got job 316 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:48:00 INFO DAGScheduler: Final stage: Stage 310(reduce at JsonRDD.scala:51)
15/05/13 11:48:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:48:00 INFO CheckpointWriter: Checkpoint for time 1431532080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532080000', took 7668 bytes and 76 ms
15/05/13 11:48:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:48:00 INFO DAGScheduler: Submitting Stage 310 (MapPartitionsRDD[2212] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36315681, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_466 stored as values in memory (estimated size 5.9 KB, free 230.8 MB)
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36321721, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_466_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.8 MB)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_466_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.9 MB)
15/05/13 11:48:00 INFO BlockManagerMaster: Updated info of block broadcast_466_piece0
15/05/13 11:48:00 INFO SparkContext: Created broadcast 466 from broadcast at DAGScheduler.scala:839
15/05/13 11:48:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 310 (MapPartitionsRDD[2212] at map at JsonRDD.scala:51)
15/05/13 11:48:00 INFO TaskSchedulerImpl: Adding task set 310.0 with 1 tasks
15/05/13 11:48:00 INFO TaskSetManager: Starting task 0.0 in stage 310.0 (TID 310, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_466_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_465_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:48:00 INFO TaskSetManager: Finished task 0.0 in stage 310.0 (TID 310) in 217 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:48:00 INFO TaskSchedulerImpl: Removed TaskSet 310.0, whose tasks have all completed, from pool 
15/05/13 11:48:00 INFO DAGScheduler: Stage 310 (reduce at JsonRDD.scala:51) finished in 0.228 s
15/05/13 11:48:00 INFO DAGScheduler: Job 316 finished: reduce at JsonRDD.scala:51, took 0.265431 s
15/05/13 11:48:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:48:00 INFO DAGScheduler: Got job 317 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:48:00 INFO DAGScheduler: Final stage: Stage 311(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:48:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:48:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:48:00 INFO DAGScheduler: Submitting Stage 311 (MapPartitionsRDD[2219] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=36325946, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_467 stored as values in memory (estimated size 20.4 KB, free 230.7 MB)
15/05/13 11:48:00 INFO MemoryStore: ensureFreeSpace(10909) called with curMem=36346866, maxMem=278302556
15/05/13 11:48:00 INFO MemoryStore: Block broadcast_467_piece0 stored as bytes in memory (estimated size 10.7 KB, free 230.7 MB)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_467_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 260.9 MB)
15/05/13 11:48:00 INFO BlockManagerMaster: Updated info of block broadcast_467_piece0
15/05/13 11:48:00 INFO SparkContext: Created broadcast 467 from broadcast at DAGScheduler.scala:839
15/05/13 11:48:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 311 (MapPartitionsRDD[2219] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:48:00 INFO TaskSchedulerImpl: Adding task set 311.0 with 1 tasks
15/05/13 11:48:00 INFO TaskSetManager: Starting task 0.0 in stage 311.0 (TID 311, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:48:00 INFO BlockManagerInfo: Added broadcast_467_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 263.6 MB)
15/05/13 11:48:00 INFO TaskSetManager: Finished task 0.0 in stage 311.0 (TID 311) in 70 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:48:00 INFO TaskSchedulerImpl: Removed TaskSet 311.0, whose tasks have all completed, from pool 
15/05/13 11:48:00 INFO DAGScheduler: Stage 311 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.071 s
15/05/13 11:48:00 INFO DAGScheduler: Job 317 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.102361 s
15/05/13 11:48:00 INFO JobScheduler: Finished job streaming job 1431532080000 ms.0 from job set of time 1431532080000 ms
15/05/13 11:48:00 INFO JobScheduler: Total delay: 0.848 s for time 1431532080000 ms (execution: 0.682 s)
15/05/13 11:48:00 INFO MapPartitionsRDD: Removing RDD 2194 from persistence list
15/05/13 11:48:00 INFO BlockManager: Removing RDD 2194
15/05/13 11:48:00 INFO UnionRDD: Removing RDD 2193 from persistence list
15/05/13 11:48:00 INFO BlockManager: Removing RDD 2193
15/05/13 11:48:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431532020000 ms: 1431531960000 ms
15/05/13 11:48:00 INFO JobGenerator: Checkpointing graph for time 1431532080000 ms
15/05/13 11:48:00 INFO DStreamGraph: Updating checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO DStreamGraph: Updated checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO CheckpointWriter: Saving checkpoint for time 1431532080000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532080000'
15/05/13 11:48:00 INFO CheckpointWriter: Checkpoint for time 1431532080000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532080000', took 7658 bytes and 70 ms
15/05/13 11:48:00 INFO DStreamGraph: Clearing checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO DStreamGraph: Cleared checkpoint data for time 1431532080000 ms
15/05/13 11:48:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:49:00 INFO FileInputDStream: Finding new files took 71 ms
15/05/13 11:49:00 INFO FileInputDStream: New files at time 1431532140000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532066005.json
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36357775, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_468 stored as values in memory (estimated size 232.9 KB, free 230.5 MB)
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=36596307, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_468_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.5 MB)
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_468_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:49:00 INFO BlockManagerMaster: Updated info of block broadcast_468_piece0
15/05/13 11:49:00 INFO SparkContext: Created broadcast 468 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:49:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:49:00 INFO JobScheduler: Added jobs for time 1431532140000 ms
15/05/13 11:49:00 INFO JobGenerator: Checkpointing graph for time 1431532140000 ms
15/05/13 11:49:00 INFO JobScheduler: Starting job streaming job 1431532140000 ms.0 from job set of time 1431532140000 ms
15/05/13 11:49:00 INFO DStreamGraph: Updating checkpoint data for time 1431532140000 ms
15/05/13 11:49:00 INFO DStreamGraph: Updated checkpoint data for time 1431532140000 ms
15/05/13 11:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431532140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000'
15/05/13 11:49:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84220): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431532140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000'
15/05/13 11:49:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:49:00 INFO DAGScheduler: Got job 318 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:49:00 INFO DAGScheduler: Final stage: Stage 312(reduce at JsonRDD.scala:51)
15/05/13 11:49:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:49:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:49:00 INFO DAGScheduler: Submitting Stage 312 (MapPartitionsRDD[2226] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36632015, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_469 stored as values in memory (estimated size 5.9 KB, free 230.5 MB)
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36638055, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_469_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.5 MB)
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_469_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:49:00 INFO BlockManagerMaster: Updated info of block broadcast_469_piece0
15/05/13 11:49:00 INFO SparkContext: Created broadcast 469 from broadcast at DAGScheduler.scala:839
15/05/13 11:49:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 312 (MapPartitionsRDD[2226] at map at JsonRDD.scala:51)
15/05/13 11:49:00 INFO TaskSchedulerImpl: Adding task set 312.0 with 1 tasks
15/05/13 11:49:00 INFO TaskSetManager: Starting task 0.0 in stage 312.0 (TID 312, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:49:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84222): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:49:00 INFO CheckpointWriter: Saving checkpoint for time 1431532140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000'
15/05/13 11:49:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84224): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:49:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532140000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000'
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_469_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.2 MB)
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_468_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:49:00 INFO TaskSetManager: Finished task 0.0 in stage 312.0 (TID 312) in 429 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:49:00 INFO TaskSchedulerImpl: Removed TaskSet 312.0, whose tasks have all completed, from pool 
15/05/13 11:49:00 INFO DAGScheduler: Stage 312 (reduce at JsonRDD.scala:51) finished in 0.441 s
15/05/13 11:49:00 INFO DAGScheduler: Job 318 finished: reduce at JsonRDD.scala:51, took 0.467432 s
15/05/13 11:49:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:49:00 INFO DAGScheduler: Got job 319 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:49:00 INFO DAGScheduler: Final stage: Stage 313(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:49:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:49:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:49:00 INFO DAGScheduler: Submitting Stage 313 (MapPartitionsRDD[2233] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(21424) called with curMem=36642280, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_470 stored as values in memory (estimated size 20.9 KB, free 230.4 MB)
15/05/13 11:49:00 INFO MemoryStore: ensureFreeSpace(11221) called with curMem=36663704, maxMem=278302556
15/05/13 11:49:00 INFO MemoryStore: Block broadcast_470_piece0 stored as bytes in memory (estimated size 11.0 KB, free 230.4 MB)
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_470_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 11.0 KB, free: 260.8 MB)
15/05/13 11:49:00 INFO BlockManagerMaster: Updated info of block broadcast_470_piece0
15/05/13 11:49:00 INFO SparkContext: Created broadcast 470 from broadcast at DAGScheduler.scala:839
15/05/13 11:49:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 313 (MapPartitionsRDD[2233] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:49:00 INFO TaskSchedulerImpl: Adding task set 313.0 with 1 tasks
15/05/13 11:49:00 INFO TaskSetManager: Starting task 0.0 in stage 313.0 (TID 313, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:49:00 INFO BlockManagerInfo: Added broadcast_470_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 11.0 KB, free: 262.1 MB)
15/05/13 11:49:01 INFO TaskSetManager: Finished task 0.0 in stage 313.0 (TID 313) in 127 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:49:01 INFO TaskSchedulerImpl: Removed TaskSet 313.0, whose tasks have all completed, from pool 
15/05/13 11:49:01 INFO DAGScheduler: Stage 313 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.127 s
15/05/13 11:49:01 INFO DAGScheduler: Job 319 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.164000 s
15/05/13 11:49:01 INFO JobScheduler: Finished job streaming job 1431532140000 ms.0 from job set of time 1431532140000 ms
15/05/13 11:49:01 INFO JobScheduler: Total delay: 1.146 s for time 1431532140000 ms (execution: 0.927 s)
15/05/13 11:49:01 INFO MapPartitionsRDD: Removing RDD 2208 from persistence list
15/05/13 11:49:01 INFO BlockManager: Removing RDD 2208
15/05/13 11:49:01 INFO UnionRDD: Removing RDD 2207 from persistence list
15/05/13 11:49:01 INFO BlockManager: Removing RDD 2207
15/05/13 11:49:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532080000 ms: 1431532020000 ms
15/05/13 11:49:01 INFO JobGenerator: Checkpointing graph for time 1431532140000 ms
15/05/13 11:49:01 INFO DStreamGraph: Updating checkpoint data for time 1431532140000 ms
15/05/13 11:49:01 INFO DStreamGraph: Updated checkpoint data for time 1431532140000 ms
15/05/13 11:49:01 INFO CheckpointWriter: Saving checkpoint for time 1431532140000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000'
15/05/13 11:49:01 INFO CheckpointWriter: Checkpoint for time 1431532140000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000', took 7654 bytes and 54 ms
15/05/13 11:49:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532140000 ms
15/05/13 11:49:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532140000 ms
15/05/13 11:49:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:50:00 INFO FileInputDStream: Finding new files took 26 ms
15/05/13 11:50:00 INFO FileInputDStream: New files at time 1431532200000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532127723.json
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36674925, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_471 stored as values in memory (estimated size 232.9 KB, free 230.2 MB)
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=36913457, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_471_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.2 MB)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_471_piece0
15/05/13 11:50:00 INFO SparkContext: Created broadcast 471 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:50:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:50:00 INFO JobScheduler: Added jobs for time 1431532200000 ms
15/05/13 11:50:00 INFO JobGenerator: Checkpointing graph for time 1431532200000 ms
15/05/13 11:50:00 INFO DStreamGraph: Updating checkpoint data for time 1431532200000 ms
15/05/13 11:50:00 INFO DStreamGraph: Updated checkpoint data for time 1431532200000 ms
15/05/13 11:50:00 INFO JobScheduler: Starting job streaming job 1431532200000 ms.0 from job set of time 1431532200000 ms
15/05/13 11:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431532200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000'
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 470
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_470_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_470_piece0 of size 11221 dropped from memory (free 241364612)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_470_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 11.0 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_470_piece0
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_470
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_470 of size 21424 dropped from memory (free 241386036)
15/05/13 11:50:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84230): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:50:00 INFO DAGScheduler: Got job 320 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431532200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000'
15/05/13 11:50:00 INFO DAGScheduler: Final stage: Stage 314(reduce at JsonRDD.scala:51)
15/05/13 11:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:50:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:50:00 INFO DAGScheduler: Submitting Stage 314 (MapPartitionsRDD[2240] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_470_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 11.0 KB, free: 262.1 MB)
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36916520, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_472 stored as values in memory (estimated size 5.9 KB, free 230.2 MB)
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36922560, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_472_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.2 MB)
15/05/13 11:50:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84232): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_472_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:50:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84232): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 470
15/05/13 11:50:00 INFO CheckpointWriter: Saving checkpoint for time 1431532200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000'
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_472_piece0
15/05/13 11:50:00 INFO SparkContext: Created broadcast 472 from broadcast at DAGScheduler.scala:839
15/05/13 11:50:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 314 (MapPartitionsRDD[2240] at map at JsonRDD.scala:51)
15/05/13 11:50:00 INFO TaskSchedulerImpl: Adding task set 314.0 with 1 tasks
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 469
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_469
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_469 of size 6040 dropped from memory (free 241381811)
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_469_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_469_piece0 of size 4225 dropped from memory (free 241386036)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_469_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_469_piece0
15/05/13 11:50:00 INFO TaskSetManager: Starting task 0.0 in stage 314.0 (TID 314, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_469_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 469
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 467
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_467_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_467_piece0 of size 10909 dropped from memory (free 241396945)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_467_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_467_piece0
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_467
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_467 of size 20920 dropped from memory (free 241417865)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_467_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.7 KB, free: 263.7 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 467
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 464
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_464_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_464_piece0 of size 11135 dropped from memory (free 241429000)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_464_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_464_piece0
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_464
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_464 of size 21304 dropped from memory (free 241450304)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_472_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_464_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:50:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000
15/05/13 11:50:00 INFO CheckpointWriter: Checkpoint for time 1431532200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000', took 7661 bytes and 201 ms
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 464
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 463
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_463_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_463_piece0 of size 4225 dropped from memory (free 241454529)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_463_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_463_piece0
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_463
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_463 of size 6040 dropped from memory (free 241460569)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_463_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 463
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 461
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_461
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_461 of size 20920 dropped from memory (free 241481489)
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_461_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_461_piece0 of size 10849 dropped from memory (free 241492338)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_461_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_461_piece0
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_461_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.1 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 461
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 466
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_466
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_466 of size 6040 dropped from memory (free 241498378)
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_466_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_466_piece0 of size 4225 dropped from memory (free 241502603)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_466_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_466_piece0
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_466_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 466
15/05/13 11:50:00 INFO BlockManager: Removing broadcast 465
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_465_piece0
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_465_piece0 of size 35708 dropped from memory (free 241538311)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_465_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.9 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_465_piece0
15/05/13 11:50:00 INFO BlockManager: Removing block broadcast_465
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_465 of size 238532 dropped from memory (free 241776843)
15/05/13 11:50:00 INFO BlockManagerInfo: Removed broadcast_465_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:50:00 INFO ContextCleaner: Cleaned broadcast 465
15/05/13 11:50:00 INFO TaskSetManager: Finished task 0.0 in stage 314.0 (TID 314) in 328 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:50:00 INFO TaskSchedulerImpl: Removed TaskSet 314.0, whose tasks have all completed, from pool 
15/05/13 11:50:00 INFO DAGScheduler: Stage 314 (reduce at JsonRDD.scala:51) finished in 0.345 s
15/05/13 11:50:00 INFO DAGScheduler: Job 320 finished: reduce at JsonRDD.scala:51, took 0.384219 s
15/05/13 11:50:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:50:00 INFO DAGScheduler: Got job 321 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:50:00 INFO DAGScheduler: Final stage: Stage 315(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:50:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:50:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:50:00 INFO DAGScheduler: Submitting Stage 315 (MapPartitionsRDD[2247] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=36525713, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_473 stored as values in memory (estimated size 20.4 KB, free 230.6 MB)
15/05/13 11:50:00 INFO MemoryStore: ensureFreeSpace(10840) called with curMem=36546633, maxMem=278302556
15/05/13 11:50:00 INFO MemoryStore: Block broadcast_473_piece0 stored as bytes in memory (estimated size 10.6 KB, free 230.5 MB)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_473_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 260.9 MB)
15/05/13 11:50:00 INFO BlockManagerMaster: Updated info of block broadcast_473_piece0
15/05/13 11:50:00 INFO SparkContext: Created broadcast 473 from broadcast at DAGScheduler.scala:839
15/05/13 11:50:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 315 (MapPartitionsRDD[2247] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:50:00 INFO TaskSchedulerImpl: Adding task set 315.0 with 1 tasks
15/05/13 11:50:00 INFO TaskSetManager: Starting task 0.0 in stage 315.0 (TID 315, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:50:00 INFO BlockManagerInfo: Added broadcast_473_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.6 KB, free: 262.1 MB)
15/05/13 11:50:01 INFO BlockManagerInfo: Added broadcast_471_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:50:01 INFO TaskSetManager: Finished task 0.0 in stage 315.0 (TID 315) in 277 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:50:01 INFO TaskSchedulerImpl: Removed TaskSet 315.0, whose tasks have all completed, from pool 
15/05/13 11:50:01 INFO DAGScheduler: Stage 315 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.277 s
15/05/13 11:50:01 INFO DAGScheduler: Job 321 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.321508 s
15/05/13 11:50:01 INFO JobScheduler: Finished job streaming job 1431532200000 ms.0 from job set of time 1431532200000 ms
15/05/13 11:50:01 INFO JobScheduler: Total delay: 1.263 s for time 1431532200000 ms (execution: 1.108 s)
15/05/13 11:50:01 INFO MapPartitionsRDD: Removing RDD 2222 from persistence list
15/05/13 11:50:01 INFO BlockManager: Removing RDD 2222
15/05/13 11:50:01 INFO UnionRDD: Removing RDD 2221 from persistence list
15/05/13 11:50:01 INFO BlockManager: Removing RDD 2221
15/05/13 11:50:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532140000 ms: 1431532080000 ms
15/05/13 11:50:01 INFO JobGenerator: Checkpointing graph for time 1431532200000 ms
15/05/13 11:50:01 INFO DStreamGraph: Updating checkpoint data for time 1431532200000 ms
15/05/13 11:50:01 INFO DStreamGraph: Updated checkpoint data for time 1431532200000 ms
15/05/13 11:50:01 INFO CheckpointWriter: Saving checkpoint for time 1431532200000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000'
15/05/13 11:50:01 INFO CheckpointWriter: Checkpoint for time 1431532200000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000', took 7655 bytes and 49 ms
15/05/13 11:50:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532200000 ms
15/05/13 11:50:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532200000 ms
15/05/13 11:50:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:51:00 INFO FileInputDStream: Finding new files took 43 ms
15/05/13 11:51:00 INFO FileInputDStream: New files at time 1431532260000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532188350.json
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36557473, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_474 stored as values in memory (estimated size 232.9 KB, free 230.3 MB)
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=36796005, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_474_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.3 MB)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:51:00 INFO BlockManagerMaster: Updated info of block broadcast_474_piece0
15/05/13 11:51:00 INFO SparkContext: Created broadcast 474 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:51:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:51:00 INFO JobScheduler: Added jobs for time 1431532260000 ms
15/05/13 11:51:00 INFO JobGenerator: Checkpointing graph for time 1431532260000 ms
15/05/13 11:51:00 INFO DStreamGraph: Updating checkpoint data for time 1431532260000 ms
15/05/13 11:51:00 INFO JobScheduler: Starting job streaming job 1431532260000 ms.0 from job set of time 1431532260000 ms
15/05/13 11:51:00 INFO DStreamGraph: Updated checkpoint data for time 1431532260000 ms
15/05/13 11:51:00 INFO CheckpointWriter: Saving checkpoint for time 1431532260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532260000'
15/05/13 11:51:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:51:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431531960000
15/05/13 11:51:00 INFO DAGScheduler: Got job 322 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:51:00 INFO DAGScheduler: Final stage: Stage 316(reduce at JsonRDD.scala:51)
15/05/13 11:51:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:51:00 INFO CheckpointWriter: Checkpoint for time 1431532260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532260000', took 7665 bytes and 73 ms
15/05/13 11:51:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:51:00 INFO DAGScheduler: Submitting Stage 316 (MapPartitionsRDD[2254] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=36831713, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_475 stored as values in memory (estimated size 5.9 KB, free 230.3 MB)
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=36837753, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_475_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.3 MB)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_475_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:51:00 INFO BlockManagerMaster: Updated info of block broadcast_475_piece0
15/05/13 11:51:00 INFO SparkContext: Created broadcast 475 from broadcast at DAGScheduler.scala:839
15/05/13 11:51:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 316 (MapPartitionsRDD[2254] at map at JsonRDD.scala:51)
15/05/13 11:51:00 INFO TaskSchedulerImpl: Adding task set 316.0 with 1 tasks
15/05/13 11:51:00 INFO TaskSetManager: Starting task 0.0 in stage 316.0 (TID 316, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_475_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.1 MB)
15/05/13 11:51:00 INFO TaskSetManager: Finished task 0.0 in stage 316.0 (TID 316) in 334 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:51:00 INFO TaskSchedulerImpl: Removed TaskSet 316.0, whose tasks have all completed, from pool 
15/05/13 11:51:00 INFO DAGScheduler: Stage 316 (reduce at JsonRDD.scala:51) finished in 0.343 s
15/05/13 11:51:00 INFO DAGScheduler: Job 322 finished: reduce at JsonRDD.scala:51, took 0.381101 s
15/05/13 11:51:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:51:00 INFO DAGScheduler: Got job 323 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:51:00 INFO DAGScheduler: Final stage: Stage 317(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:51:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:51:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:51:00 INFO DAGScheduler: Submitting Stage 317 (MapPartitionsRDD[2261] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=36841978, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_476 stored as values in memory (estimated size 21.0 KB, free 230.3 MB)
15/05/13 11:51:00 INFO MemoryStore: ensureFreeSpace(11113) called with curMem=36863458, maxMem=278302556
15/05/13 11:51:00 INFO MemoryStore: Block broadcast_476_piece0 stored as bytes in memory (estimated size 10.9 KB, free 230.2 MB)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_476_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.8 MB)
15/05/13 11:51:00 INFO BlockManagerMaster: Updated info of block broadcast_476_piece0
15/05/13 11:51:00 INFO SparkContext: Created broadcast 476 from broadcast at DAGScheduler.scala:839
15/05/13 11:51:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 317 (MapPartitionsRDD[2261] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:51:00 INFO TaskSchedulerImpl: Adding task set 317.0 with 1 tasks
15/05/13 11:51:00 INFO TaskSetManager: Starting task 0.0 in stage 317.0 (TID 317, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_476_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:51:00 INFO BlockManagerInfo: Added broadcast_474_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:51:01 INFO TaskSetManager: Finished task 0.0 in stage 317.0 (TID 317) in 150 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:51:01 INFO TaskSchedulerImpl: Removed TaskSet 317.0, whose tasks have all completed, from pool 
15/05/13 11:51:01 INFO DAGScheduler: Stage 317 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.152 s
15/05/13 11:51:01 INFO DAGScheduler: Job 323 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.173280 s
15/05/13 11:51:01 INFO JobScheduler: Finished job streaming job 1431532260000 ms.0 from job set of time 1431532260000 ms
15/05/13 11:51:01 INFO JobScheduler: Total delay: 1.032 s for time 1431532260000 ms (execution: 0.822 s)
15/05/13 11:51:01 INFO MapPartitionsRDD: Removing RDD 2236 from persistence list
15/05/13 11:51:01 INFO BlockManager: Removing RDD 2236
15/05/13 11:51:01 INFO UnionRDD: Removing RDD 2235 from persistence list
15/05/13 11:51:01 INFO BlockManager: Removing RDD 2235
15/05/13 11:51:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532200000 ms: 1431532140000 ms
15/05/13 11:51:01 INFO JobGenerator: Checkpointing graph for time 1431532260000 ms
15/05/13 11:51:01 INFO DStreamGraph: Updating checkpoint data for time 1431532260000 ms
15/05/13 11:51:01 INFO DStreamGraph: Updated checkpoint data for time 1431532260000 ms
15/05/13 11:51:01 INFO CheckpointWriter: Saving checkpoint for time 1431532260000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532260000'
15/05/13 11:51:01 INFO CheckpointWriter: Checkpoint for time 1431532260000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532260000', took 7656 bytes and 54 ms
15/05/13 11:51:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532260000 ms
15/05/13 11:51:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532260000 ms
15/05/13 11:51:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:52:00 INFO FileInputDStream: Finding new files took 66 ms
15/05/13 11:52:00 INFO FileInputDStream: New files at time 1431532320000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532248945.json
15/05/13 11:52:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=36874571, maxMem=278302556
15/05/13 11:52:00 INFO MemoryStore: Block broadcast_477 stored as values in memory (estimated size 232.9 KB, free 230.0 MB)
15/05/13 11:52:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=37113103, maxMem=278302556
15/05/13 11:52:00 INFO MemoryStore: Block broadcast_477_piece0 stored as bytes in memory (estimated size 34.9 KB, free 230.0 MB)
15/05/13 11:52:00 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:52:00 INFO BlockManagerMaster: Updated info of block broadcast_477_piece0
15/05/13 11:52:00 INFO SparkContext: Created broadcast 477 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:52:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:52:00 INFO JobScheduler: Added jobs for time 1431532320000 ms
15/05/13 11:52:00 INFO JobScheduler: Starting job streaming job 1431532320000 ms.0 from job set of time 1431532320000 ms
15/05/13 11:52:00 INFO JobGenerator: Checkpointing graph for time 1431532320000 ms
15/05/13 11:52:00 INFO DStreamGraph: Updating checkpoint data for time 1431532320000 ms
15/05/13 11:52:00 INFO DStreamGraph: Updated checkpoint data for time 1431532320000 ms
15/05/13 11:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431532320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000'
15/05/13 11:52:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:52:00 INFO DAGScheduler: Got job 324 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:52:00 INFO DAGScheduler: Final stage: Stage 318(reduce at JsonRDD.scala:51)
15/05/13 11:52:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:52:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:52:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84246): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431532320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000'
15/05/13 11:52:00 INFO DAGScheduler: Submitting Stage 318 (MapPartitionsRDD[2268] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:52:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=37148811, maxMem=278302556
15/05/13 11:52:00 INFO MemoryStore: Block broadcast_478 stored as values in memory (estimated size 5.9 KB, free 230.0 MB)
15/05/13 11:52:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=37154851, maxMem=278302556
15/05/13 11:52:00 INFO MemoryStore: Block broadcast_478_piece0 stored as bytes in memory (estimated size 4.1 KB, free 230.0 MB)
15/05/13 11:52:00 INFO BlockManagerInfo: Added broadcast_478_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:52:00 INFO BlockManagerMaster: Updated info of block broadcast_478_piece0
15/05/13 11:52:00 INFO SparkContext: Created broadcast 478 from broadcast at DAGScheduler.scala:839
15/05/13 11:52:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 318 (MapPartitionsRDD[2268] at map at JsonRDD.scala:51)
15/05/13 11:52:00 INFO TaskSchedulerImpl: Adding task set 318.0 with 1 tasks
15/05/13 11:52:00 INFO TaskSetManager: Starting task 0.0 in stage 318.0 (TID 318, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:52:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84248): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:52:00 INFO CheckpointWriter: Saving checkpoint for time 1431532320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000'
15/05/13 11:52:00 INFO BlockManagerInfo: Added broadcast_478_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.1 MB)
15/05/13 11:52:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84250): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:52:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532320000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000'
15/05/13 11:52:00 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:52:00 INFO TaskSetManager: Finished task 0.0 in stage 318.0 (TID 318) in 460 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:52:00 INFO TaskSchedulerImpl: Removed TaskSet 318.0, whose tasks have all completed, from pool 
15/05/13 11:52:00 INFO DAGScheduler: Stage 318 (reduce at JsonRDD.scala:51) finished in 0.471 s
15/05/13 11:52:00 INFO DAGScheduler: Job 324 finished: reduce at JsonRDD.scala:51, took 0.510184 s
15/05/13 11:52:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:52:01 INFO DAGScheduler: Got job 325 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:52:01 INFO DAGScheduler: Final stage: Stage 319(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:52:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:52:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:52:01 INFO DAGScheduler: Submitting Stage 319 (MapPartitionsRDD[2275] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:52:01 INFO MemoryStore: ensureFreeSpace(20488) called with curMem=37159076, maxMem=278302556
15/05/13 11:52:01 INFO MemoryStore: Block broadcast_479 stored as values in memory (estimated size 20.0 KB, free 230.0 MB)
15/05/13 11:52:01 INFO MemoryStore: ensureFreeSpace(10761) called with curMem=37179564, maxMem=278302556
15/05/13 11:52:01 INFO MemoryStore: Block broadcast_479_piece0 stored as bytes in memory (estimated size 10.5 KB, free 229.9 MB)
15/05/13 11:52:01 INFO BlockManagerInfo: Added broadcast_479_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.5 KB, free: 260.8 MB)
15/05/13 11:52:01 INFO BlockManagerMaster: Updated info of block broadcast_479_piece0
15/05/13 11:52:01 INFO SparkContext: Created broadcast 479 from broadcast at DAGScheduler.scala:839
15/05/13 11:52:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 319 (MapPartitionsRDD[2275] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:52:01 INFO TaskSchedulerImpl: Adding task set 319.0 with 1 tasks
15/05/13 11:52:01 INFO TaskSetManager: Starting task 0.0 in stage 319.0 (TID 319, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:52:01 INFO BlockManagerInfo: Added broadcast_479_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.5 KB, free: 263.6 MB)
15/05/13 11:52:01 INFO BlockManagerInfo: Added broadcast_477_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.5 MB)
15/05/13 11:52:01 INFO TaskSetManager: Finished task 0.0 in stage 319.0 (TID 319) in 341 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:52:01 INFO TaskSchedulerImpl: Removed TaskSet 319.0, whose tasks have all completed, from pool 
15/05/13 11:52:01 INFO DAGScheduler: Stage 319 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.343 s
15/05/13 11:52:01 INFO DAGScheduler: Job 325 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.377739 s
15/05/13 11:52:01 INFO JobScheduler: Finished job streaming job 1431532320000 ms.0 from job set of time 1431532320000 ms
15/05/13 11:52:01 INFO JobScheduler: Total delay: 1.476 s for time 1431532320000 ms (execution: 1.255 s)
15/05/13 11:52:01 INFO MapPartitionsRDD: Removing RDD 2250 from persistence list
15/05/13 11:52:01 INFO BlockManager: Removing RDD 2250
15/05/13 11:52:01 INFO UnionRDD: Removing RDD 2249 from persistence list
15/05/13 11:52:01 INFO BlockManager: Removing RDD 2249
15/05/13 11:52:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532260000 ms: 1431532200000 ms
15/05/13 11:52:01 INFO JobGenerator: Checkpointing graph for time 1431532320000 ms
15/05/13 11:52:01 INFO DStreamGraph: Updating checkpoint data for time 1431532320000 ms
15/05/13 11:52:01 INFO DStreamGraph: Updated checkpoint data for time 1431532320000 ms
15/05/13 11:52:01 INFO CheckpointWriter: Saving checkpoint for time 1431532320000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000'
15/05/13 11:52:01 INFO CheckpointWriter: Checkpoint for time 1431532320000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000', took 7657 bytes and 63 ms
15/05/13 11:52:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532320000 ms
15/05/13 11:52:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532320000 ms
15/05/13 11:52:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:53:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 11:53:00 INFO FileInputDStream: New files at time 1431532380000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532313477.json
15/05/13 11:53:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=37190325, maxMem=278302556
15/05/13 11:53:00 INFO MemoryStore: Block broadcast_480 stored as values in memory (estimated size 232.9 KB, free 229.7 MB)
15/05/13 11:53:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=37428857, maxMem=278302556
15/05/13 11:53:00 INFO MemoryStore: Block broadcast_480_piece0 stored as bytes in memory (estimated size 34.9 KB, free 229.7 MB)
15/05/13 11:53:00 INFO BlockManagerInfo: Added broadcast_480_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.7 MB)
15/05/13 11:53:00 INFO BlockManagerMaster: Updated info of block broadcast_480_piece0
15/05/13 11:53:00 INFO SparkContext: Created broadcast 480 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:53:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:53:00 INFO JobScheduler: Added jobs for time 1431532380000 ms
15/05/13 11:53:00 INFO JobGenerator: Checkpointing graph for time 1431532380000 ms
15/05/13 11:53:00 INFO DStreamGraph: Updating checkpoint data for time 1431532380000 ms
15/05/13 11:53:00 INFO JobScheduler: Starting job streaming job 1431532380000 ms.0 from job set of time 1431532380000 ms
15/05/13 11:53:00 INFO DStreamGraph: Updated checkpoint data for time 1431532380000 ms
15/05/13 11:53:00 INFO CheckpointWriter: Saving checkpoint for time 1431532380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000'
15/05/13 11:53:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84255): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:53:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84255): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:53:00 INFO CheckpointWriter: Saving checkpoint for time 1431532380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000'
15/05/13 11:53:00 WARN CheckpointWriter: Could not rename hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/temp to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000
15/05/13 11:53:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:53:00 INFO DAGScheduler: Got job 326 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:53:00 INFO DAGScheduler: Final stage: Stage 320(reduce at JsonRDD.scala:51)
15/05/13 11:53:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:53:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:53:00 INFO DAGScheduler: Submitting Stage 320 (MapPartitionsRDD[2282] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:53:00 INFO CheckpointWriter: Checkpoint for time 1431532380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000', took 7666 bytes and 89 ms
15/05/13 11:53:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=37464565, maxMem=278302556
15/05/13 11:53:00 INFO MemoryStore: Block broadcast_481 stored as values in memory (estimated size 5.9 KB, free 229.7 MB)
15/05/13 11:53:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=37470605, maxMem=278302556
15/05/13 11:53:00 INFO MemoryStore: Block broadcast_481_piece0 stored as bytes in memory (estimated size 4.1 KB, free 229.7 MB)
15/05/13 11:53:00 INFO BlockManagerInfo: Added broadcast_481_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:53:00 INFO BlockManagerMaster: Updated info of block broadcast_481_piece0
15/05/13 11:53:00 INFO SparkContext: Created broadcast 481 from broadcast at DAGScheduler.scala:839
15/05/13 11:53:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 320 (MapPartitionsRDD[2282] at map at JsonRDD.scala:51)
15/05/13 11:53:00 INFO TaskSchedulerImpl: Adding task set 320.0 with 1 tasks
15/05/13 11:53:00 INFO TaskSetManager: Starting task 0.0 in stage 320.0 (TID 320, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:53:00 INFO BlockManagerInfo: Added broadcast_481_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:53:00 INFO BlockManagerInfo: Added broadcast_480_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:53:00 INFO TaskSetManager: Finished task 0.0 in stage 320.0 (TID 320) in 438 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:53:00 INFO TaskSchedulerImpl: Removed TaskSet 320.0, whose tasks have all completed, from pool 
15/05/13 11:53:00 INFO DAGScheduler: Stage 320 (reduce at JsonRDD.scala:51) finished in 0.452 s
15/05/13 11:53:00 INFO DAGScheduler: Job 326 finished: reduce at JsonRDD.scala:51, took 0.485240 s
15/05/13 11:53:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:53:01 INFO DAGScheduler: Got job 327 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:53:01 INFO DAGScheduler: Final stage: Stage 321(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:53:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:53:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:53:01 INFO DAGScheduler: Submitting Stage 321 (MapPartitionsRDD[2289] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:53:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=37474830, maxMem=278302556
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_482 stored as values in memory (estimated size 21.0 KB, free 229.7 MB)
15/05/13 11:53:01 INFO MemoryStore: ensureFreeSpace(11135) called with curMem=37496310, maxMem=278302556
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_482_piece0 stored as bytes in memory (estimated size 10.9 KB, free 229.6 MB)
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 475
15/05/13 11:53:01 INFO BlockManagerInfo: Added broadcast_482_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.7 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_482_piece0
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_475_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_475_piece0 of size 4225 dropped from memory (free 240799336)
15/05/13 11:53:01 INFO SparkContext: Created broadcast 482 from broadcast at DAGScheduler.scala:839
15/05/13 11:53:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 321 (MapPartitionsRDD[2289] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_475_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:53:01 INFO TaskSchedulerImpl: Adding task set 321.0 with 1 tasks
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_475_piece0
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_475
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_475 of size 6040 dropped from memory (free 240805376)
15/05/13 11:53:01 INFO TaskSetManager: Starting task 0.0 in stage 321.0 (TID 321, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_475_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 475
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 474
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_474
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_474 of size 238532 dropped from memory (free 241043908)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_474_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_474_piece0 of size 35708 dropped from memory (free 241079616)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_474_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.7 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_474_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_474_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 263.7 MB)
15/05/13 11:53:01 INFO BlockManagerInfo: Added broadcast_482_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_474_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 474
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 473
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_473
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_473 of size 20920 dropped from memory (free 241100536)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_473_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_473_piece0 of size 10840 dropped from memory (free 241111376)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_473_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_473_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_473_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.6 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 473
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 472
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_472_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_472_piece0 of size 4225 dropped from memory (free 241115601)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_472_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_472_piece0
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_472
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_472 of size 6040 dropped from memory (free 241121641)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_472_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.7 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 472
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 476
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_476
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_476 of size 21480 dropped from memory (free 241143121)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_476_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_476_piece0 of size 11113 dropped from memory (free 241154234)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_476_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_476_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_476_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 476
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 481
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_481
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_481 of size 6040 dropped from memory (free 241160274)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_481_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_481_piece0 of size 4225 dropped from memory (free 241164499)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_481_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_481_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_481_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 481
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 479
15/05/13 11:53:01 INFO TaskSetManager: Finished task 0.0 in stage 321.0 (TID 321) in 137 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_479
15/05/13 11:53:01 INFO TaskSchedulerImpl: Removed TaskSet 321.0, whose tasks have all completed, from pool 
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_479 of size 20488 dropped from memory (free 241184987)
15/05/13 11:53:01 INFO DAGScheduler: Stage 321 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.138 s
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_479_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_479_piece0 of size 10761 dropped from memory (free 241195748)
15/05/13 11:53:01 INFO DAGScheduler: Job 327 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.220777 s
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_479_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.5 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_479_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_479_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.5 KB, free: 263.6 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 479
15/05/13 11:53:01 INFO BlockManager: Removing broadcast 478
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_478
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_478 of size 6040 dropped from memory (free 241201788)
15/05/13 11:53:01 INFO BlockManager: Removing block broadcast_478_piece0
15/05/13 11:53:01 INFO MemoryStore: Block broadcast_478_piece0 of size 4225 dropped from memory (free 241206013)
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_478_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.8 MB)
15/05/13 11:53:01 INFO BlockManagerMaster: Updated info of block broadcast_478_piece0
15/05/13 11:53:01 INFO BlockManagerInfo: Removed broadcast_478_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 262.0 MB)
15/05/13 11:53:01 INFO ContextCleaner: Cleaned broadcast 478
15/05/13 11:53:01 INFO JobScheduler: Finished job streaming job 1431532380000 ms.0 from job set of time 1431532380000 ms
15/05/13 11:53:01 INFO JobScheduler: Total delay: 1.274 s for time 1431532380000 ms (execution: 1.062 s)
15/05/13 11:53:01 INFO MapPartitionsRDD: Removing RDD 2264 from persistence list
15/05/13 11:53:01 INFO BlockManager: Removing RDD 2264
15/05/13 11:53:01 INFO UnionRDD: Removing RDD 2263 from persistence list
15/05/13 11:53:01 INFO BlockManager: Removing RDD 2263
15/05/13 11:53:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532320000 ms: 1431532260000 ms
15/05/13 11:53:01 INFO JobGenerator: Checkpointing graph for time 1431532380000 ms
15/05/13 11:53:01 INFO DStreamGraph: Updating checkpoint data for time 1431532380000 ms
15/05/13 11:53:01 INFO DStreamGraph: Updated checkpoint data for time 1431532380000 ms
15/05/13 11:53:01 INFO CheckpointWriter: Saving checkpoint for time 1431532380000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000'
15/05/13 11:53:01 INFO CheckpointWriter: Checkpoint for time 1431532380000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000', took 7655 bytes and 56 ms
15/05/13 11:53:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532380000 ms
15/05/13 11:53:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532380000 ms
15/05/13 11:53:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:54:00 INFO FileInputDStream: Finding new files took 45 ms
15/05/13 11:54:00 INFO FileInputDStream: New files at time 1431532440000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532375681.json
15/05/13 11:54:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=37096543, maxMem=278302556
15/05/13 11:54:00 INFO MemoryStore: Block broadcast_483 stored as values in memory (estimated size 232.9 KB, free 229.8 MB)
15/05/13 11:54:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=37335075, maxMem=278302556
15/05/13 11:54:00 INFO MemoryStore: Block broadcast_483_piece0 stored as bytes in memory (estimated size 34.9 KB, free 229.8 MB)
15/05/13 11:54:00 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.8 MB)
15/05/13 11:54:00 INFO BlockManagerMaster: Updated info of block broadcast_483_piece0
15/05/13 11:54:00 INFO SparkContext: Created broadcast 483 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:54:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:54:00 INFO JobScheduler: Added jobs for time 1431532440000 ms
15/05/13 11:54:00 INFO JobGenerator: Checkpointing graph for time 1431532440000 ms
15/05/13 11:54:00 INFO DStreamGraph: Updating checkpoint data for time 1431532440000 ms
15/05/13 11:54:00 INFO JobScheduler: Starting job streaming job 1431532440000 ms.0 from job set of time 1431532440000 ms
15/05/13 11:54:00 INFO DStreamGraph: Updated checkpoint data for time 1431532440000 ms
15/05/13 11:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431532440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000'
15/05/13 11:54:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84269): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:54:00 INFO CheckpointWriter: Saving checkpoint for time 1431532440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000'
15/05/13 11:54:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:54:00 INFO DAGScheduler: Got job 328 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:54:00 INFO DAGScheduler: Final stage: Stage 322(reduce at JsonRDD.scala:51)
15/05/13 11:54:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:54:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:54:00 INFO DAGScheduler: Submitting Stage 322 (MapPartitionsRDD[2296] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:54:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=37370783, maxMem=278302556
15/05/13 11:54:00 INFO MemoryStore: Block broadcast_484 stored as values in memory (estimated size 5.9 KB, free 229.8 MB)
15/05/13 11:54:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=37376823, maxMem=278302556
15/05/13 11:54:00 INFO MemoryStore: Block broadcast_484_piece0 stored as bytes in memory (estimated size 4.1 KB, free 229.8 MB)
15/05/13 11:54:00 INFO BlockManagerInfo: Added broadcast_484_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:54:00 INFO BlockManagerMaster: Updated info of block broadcast_484_piece0
15/05/13 11:54:00 INFO SparkContext: Created broadcast 484 from broadcast at DAGScheduler.scala:839
15/05/13 11:54:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 322 (MapPartitionsRDD[2296] at map at JsonRDD.scala:51)
15/05/13 11:54:00 INFO TaskSchedulerImpl: Adding task set 322.0 with 1 tasks
15/05/13 11:54:00 INFO TaskSetManager: Starting task 0.0 in stage 322.0 (TID 322, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:54:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532140000.bk
15/05/13 11:54:00 INFO CheckpointWriter: Checkpoint for time 1431532440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000', took 7666 bytes and 199 ms
15/05/13 11:54:00 INFO BlockManagerInfo: Added broadcast_484_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:54:00 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.5 MB)
15/05/13 11:54:00 INFO TaskSetManager: Finished task 0.0 in stage 322.0 (TID 322) in 392 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:54:00 INFO TaskSchedulerImpl: Removed TaskSet 322.0, whose tasks have all completed, from pool 
15/05/13 11:54:00 INFO DAGScheduler: Stage 322 (reduce at JsonRDD.scala:51) finished in 0.408 s
15/05/13 11:54:00 INFO DAGScheduler: Job 328 finished: reduce at JsonRDD.scala:51, took 0.459370 s
15/05/13 11:54:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:54:01 INFO DAGScheduler: Got job 329 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:54:01 INFO DAGScheduler: Final stage: Stage 323(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:54:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:54:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:54:01 INFO DAGScheduler: Submitting Stage 323 (MapPartitionsRDD[2303] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:54:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=37381048, maxMem=278302556
15/05/13 11:54:01 INFO MemoryStore: Block broadcast_485 stored as values in memory (estimated size 21.0 KB, free 229.7 MB)
15/05/13 11:54:01 INFO MemoryStore: ensureFreeSpace(11121) called with curMem=37402528, maxMem=278302556
15/05/13 11:54:01 INFO MemoryStore: Block broadcast_485_piece0 stored as bytes in memory (estimated size 10.9 KB, free 229.7 MB)
15/05/13 11:54:01 INFO BlockManagerInfo: Added broadcast_485_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.7 MB)
15/05/13 11:54:01 INFO BlockManagerMaster: Updated info of block broadcast_485_piece0
15/05/13 11:54:01 INFO SparkContext: Created broadcast 485 from broadcast at DAGScheduler.scala:839
15/05/13 11:54:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 323 (MapPartitionsRDD[2303] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:54:01 INFO TaskSchedulerImpl: Adding task set 323.0 with 1 tasks
15/05/13 11:54:01 INFO TaskSetManager: Starting task 0.0 in stage 323.0 (TID 323, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:54:01 INFO BlockManagerInfo: Added broadcast_485_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.0 MB)
15/05/13 11:54:01 INFO BlockManagerInfo: Added broadcast_483_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 262.0 MB)
15/05/13 11:54:01 INFO TaskSetManager: Finished task 0.0 in stage 323.0 (TID 323) in 249 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:54:01 INFO TaskSchedulerImpl: Removed TaskSet 323.0, whose tasks have all completed, from pool 
15/05/13 11:54:01 INFO DAGScheduler: Stage 323 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.250 s
15/05/13 11:54:01 INFO DAGScheduler: Job 329 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.284882 s
15/05/13 11:54:01 INFO JobScheduler: Finished job streaming job 1431532440000 ms.0 from job set of time 1431532440000 ms
15/05/13 11:54:01 INFO JobScheduler: Total delay: 1.412 s for time 1431532440000 ms (execution: 1.224 s)
15/05/13 11:54:01 INFO MapPartitionsRDD: Removing RDD 2278 from persistence list
15/05/13 11:54:01 INFO BlockManager: Removing RDD 2278
15/05/13 11:54:01 INFO UnionRDD: Removing RDD 2277 from persistence list
15/05/13 11:54:01 INFO BlockManager: Removing RDD 2277
15/05/13 11:54:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532380000 ms: 1431532320000 ms
15/05/13 11:54:01 INFO JobGenerator: Checkpointing graph for time 1431532440000 ms
15/05/13 11:54:01 INFO DStreamGraph: Updating checkpoint data for time 1431532440000 ms
15/05/13 11:54:01 INFO DStreamGraph: Updated checkpoint data for time 1431532440000 ms
15/05/13 11:54:01 INFO CheckpointWriter: Saving checkpoint for time 1431532440000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000'
15/05/13 11:54:01 INFO CheckpointWriter: Checkpoint for time 1431532440000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532440000', took 7656 bytes and 53 ms
15/05/13 11:54:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532440000 ms
15/05/13 11:54:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532440000 ms
15/05/13 11:54:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:55:00 INFO FileInputDStream: Finding new files took 56 ms
15/05/13 11:55:00 INFO FileInputDStream: New files at time 1431532500000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532436849.json
15/05/13 11:55:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=37413649, maxMem=278302556
15/05/13 11:55:00 INFO MemoryStore: Block broadcast_486 stored as values in memory (estimated size 232.9 KB, free 229.5 MB)
15/05/13 11:55:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=37652181, maxMem=278302556
15/05/13 11:55:00 INFO MemoryStore: Block broadcast_486_piece0 stored as bytes in memory (estimated size 34.9 KB, free 229.5 MB)
15/05/13 11:55:00 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.7 MB)
15/05/13 11:55:00 INFO BlockManagerMaster: Updated info of block broadcast_486_piece0
15/05/13 11:55:00 INFO SparkContext: Created broadcast 486 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:55:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:55:00 INFO JobScheduler: Starting job streaming job 1431532500000 ms.0 from job set of time 1431532500000 ms
15/05/13 11:55:00 INFO JobScheduler: Added jobs for time 1431532500000 ms
15/05/13 11:55:00 INFO JobGenerator: Checkpointing graph for time 1431532500000 ms
15/05/13 11:55:00 INFO DStreamGraph: Updating checkpoint data for time 1431532500000 ms
15/05/13 11:55:00 INFO DStreamGraph: Updated checkpoint data for time 1431532500000 ms
15/05/13 11:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431532500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000'
15/05/13 11:55:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:55:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84277): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431532500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000'
15/05/13 11:55:00 INFO DAGScheduler: Got job 330 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:55:00 INFO DAGScheduler: Final stage: Stage 324(reduce at JsonRDD.scala:51)
15/05/13 11:55:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:55:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:55:00 INFO DAGScheduler: Submitting Stage 324 (MapPartitionsRDD[2310] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:55:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=37687889, maxMem=278302556
15/05/13 11:55:00 INFO MemoryStore: Block broadcast_487 stored as values in memory (estimated size 5.9 KB, free 229.5 MB)
15/05/13 11:55:00 INFO MemoryStore: ensureFreeSpace(4228) called with curMem=37693929, maxMem=278302556
15/05/13 11:55:00 INFO MemoryStore: Block broadcast_487_piece0 stored as bytes in memory (estimated size 4.1 KB, free 229.5 MB)
15/05/13 11:55:00 INFO BlockManagerInfo: Added broadcast_487_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:55:00 INFO BlockManagerMaster: Updated info of block broadcast_487_piece0
15/05/13 11:55:00 INFO SparkContext: Created broadcast 487 from broadcast at DAGScheduler.scala:839
15/05/13 11:55:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 324 (MapPartitionsRDD[2310] at map at JsonRDD.scala:51)
15/05/13 11:55:00 INFO TaskSchedulerImpl: Adding task set 324.0 with 1 tasks
15/05/13 11:55:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84279): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:55:00 INFO CheckpointWriter: Saving checkpoint for time 1431532500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000'
15/05/13 11:55:00 INFO TaskSetManager: Starting task 0.0 in stage 324.0 (TID 324, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:55:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532200000.bk
15/05/13 11:55:00 INFO CheckpointWriter: Checkpoint for time 1431532500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000', took 7662 bytes and 160 ms
15/05/13 11:55:00 INFO BlockManagerInfo: Added broadcast_487_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.5 MB)
15/05/13 11:55:00 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.5 MB)
15/05/13 11:55:00 INFO TaskSetManager: Finished task 0.0 in stage 324.0 (TID 324) in 473 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 11:55:00 INFO TaskSchedulerImpl: Removed TaskSet 324.0, whose tasks have all completed, from pool 
15/05/13 11:55:00 INFO DAGScheduler: Stage 324 (reduce at JsonRDD.scala:51) finished in 0.488 s
15/05/13 11:55:00 INFO DAGScheduler: Job 330 finished: reduce at JsonRDD.scala:51, took 0.527386 s
15/05/13 11:55:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:55:01 INFO DAGScheduler: Got job 331 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:55:01 INFO DAGScheduler: Final stage: Stage 325(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:55:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:55:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:55:01 INFO DAGScheduler: Submitting Stage 325 (MapPartitionsRDD[2317] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:55:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=37698157, maxMem=278302556
15/05/13 11:55:01 INFO MemoryStore: Block broadcast_488 stored as values in memory (estimated size 21.0 KB, free 229.4 MB)
15/05/13 11:55:01 INFO MemoryStore: ensureFreeSpace(11143) called with curMem=37719637, maxMem=278302556
15/05/13 11:55:01 INFO MemoryStore: Block broadcast_488_piece0 stored as bytes in memory (estimated size 10.9 KB, free 229.4 MB)
15/05/13 11:55:01 INFO BlockManagerInfo: Added broadcast_488_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.7 MB)
15/05/13 11:55:01 INFO BlockManagerMaster: Updated info of block broadcast_488_piece0
15/05/13 11:55:01 INFO SparkContext: Created broadcast 488 from broadcast at DAGScheduler.scala:839
15/05/13 11:55:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 325 (MapPartitionsRDD[2317] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:55:01 INFO TaskSchedulerImpl: Adding task set 325.0 with 1 tasks
15/05/13 11:55:01 INFO TaskSetManager: Starting task 0.0 in stage 325.0 (TID 325, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:55:01 INFO BlockManagerInfo: Added broadcast_488_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 262.0 MB)
15/05/13 11:55:01 INFO BlockManagerInfo: Added broadcast_486_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:55:01 INFO TaskSetManager: Finished task 0.0 in stage 325.0 (TID 325) in 274 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:55:01 INFO TaskSchedulerImpl: Removed TaskSet 325.0, whose tasks have all completed, from pool 
15/05/13 11:55:01 INFO DAGScheduler: Stage 325 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.281 s
15/05/13 11:55:01 INFO DAGScheduler: Job 331 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.310296 s
15/05/13 11:55:01 INFO JobScheduler: Finished job streaming job 1431532500000 ms.0 from job set of time 1431532500000 ms
15/05/13 11:55:01 INFO JobScheduler: Total delay: 1.459 s for time 1431532500000 ms (execution: 1.248 s)
15/05/13 11:55:01 INFO MapPartitionsRDD: Removing RDD 2292 from persistence list
15/05/13 11:55:01 INFO BlockManager: Removing RDD 2292
15/05/13 11:55:01 INFO UnionRDD: Removing RDD 2291 from persistence list
15/05/13 11:55:01 INFO BlockManager: Removing RDD 2291
15/05/13 11:55:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532440000 ms: 1431532380000 ms
15/05/13 11:55:01 INFO JobGenerator: Checkpointing graph for time 1431532500000 ms
15/05/13 11:55:01 INFO DStreamGraph: Updating checkpoint data for time 1431532500000 ms
15/05/13 11:55:01 INFO DStreamGraph: Updated checkpoint data for time 1431532500000 ms
15/05/13 11:55:01 INFO CheckpointWriter: Saving checkpoint for time 1431532500000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000'
15/05/13 11:55:01 INFO CheckpointWriter: Checkpoint for time 1431532500000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532500000', took 7655 bytes and 53 ms
15/05/13 11:55:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532500000 ms
15/05/13 11:55:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532500000 ms
15/05/13 11:55:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:56:00 INFO FileInputDStream: Finding new files took 59 ms
15/05/13 11:56:00 INFO FileInputDStream: New files at time 1431532560000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532497455.json
15/05/13 11:56:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=37730780, maxMem=278302556
15/05/13 11:56:00 INFO MemoryStore: Block broadcast_489 stored as values in memory (estimated size 232.9 KB, free 229.2 MB)
15/05/13 11:56:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=37969312, maxMem=278302556
15/05/13 11:56:00 INFO MemoryStore: Block broadcast_489_piece0 stored as bytes in memory (estimated size 34.9 KB, free 229.2 MB)
15/05/13 11:56:00 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.7 MB)
15/05/13 11:56:00 INFO BlockManagerMaster: Updated info of block broadcast_489_piece0
15/05/13 11:56:00 INFO SparkContext: Created broadcast 489 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:56:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:56:00 INFO JobScheduler: Added jobs for time 1431532560000 ms
15/05/13 11:56:00 INFO JobGenerator: Checkpointing graph for time 1431532560000 ms
15/05/13 11:56:00 INFO DStreamGraph: Updating checkpoint data for time 1431532560000 ms
15/05/13 11:56:00 INFO JobScheduler: Starting job streaming job 1431532560000 ms.0 from job set of time 1431532560000 ms
15/05/13 11:56:00 INFO DStreamGraph: Updated checkpoint data for time 1431532560000 ms
15/05/13 11:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431532560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000'
15/05/13 11:56:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84286): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431532560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000'
15/05/13 11:56:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:56:00 INFO DAGScheduler: Got job 332 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:56:00 INFO DAGScheduler: Final stage: Stage 326(reduce at JsonRDD.scala:51)
15/05/13 11:56:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:56:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:56:00 INFO DAGScheduler: Submitting Stage 326 (MapPartitionsRDD[2324] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:56:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84288): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:56:00 INFO CheckpointWriter: Saving checkpoint for time 1431532560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000'
15/05/13 11:56:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38005020, maxMem=278302556
15/05/13 11:56:00 INFO MemoryStore: Block broadcast_490 stored as values in memory (estimated size 5.9 KB, free 229.2 MB)
15/05/13 11:56:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=38011060, maxMem=278302556
15/05/13 11:56:00 INFO MemoryStore: Block broadcast_490_piece0 stored as bytes in memory (estimated size 4.1 KB, free 229.2 MB)
15/05/13 11:56:00 INFO BlockManagerInfo: Added broadcast_490_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:56:00 INFO BlockManagerMaster: Updated info of block broadcast_490_piece0
15/05/13 11:56:00 INFO SparkContext: Created broadcast 490 from broadcast at DAGScheduler.scala:839
15/05/13 11:56:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 326 (MapPartitionsRDD[2324] at map at JsonRDD.scala:51)
15/05/13 11:56:00 INFO TaskSchedulerImpl: Adding task set 326.0 with 1 tasks
15/05/13 11:56:00 INFO TaskSetManager: Starting task 0.0 in stage 326.0 (TID 326, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:56:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532260000.bk
15/05/13 11:56:00 INFO CheckpointWriter: Checkpoint for time 1431532560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000', took 7665 bytes and 203 ms
15/05/13 11:56:00 INFO BlockManagerInfo: Added broadcast_490_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:56:00 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:56:00 INFO TaskSetManager: Finished task 0.0 in stage 326.0 (TID 326) in 466 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:56:00 INFO TaskSchedulerImpl: Removed TaskSet 326.0, whose tasks have all completed, from pool 
15/05/13 11:56:00 INFO DAGScheduler: Stage 326 (reduce at JsonRDD.scala:51) finished in 0.485 s
15/05/13 11:56:00 INFO DAGScheduler: Job 332 finished: reduce at JsonRDD.scala:51, took 0.530302 s
15/05/13 11:56:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:56:01 INFO DAGScheduler: Got job 333 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:56:01 INFO DAGScheduler: Final stage: Stage 327(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:56:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:56:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:56:01 INFO DAGScheduler: Submitting Stage 327 (MapPartitionsRDD[2331] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:56:01 INFO MemoryStore: ensureFreeSpace(21480) called with curMem=38015285, maxMem=278302556
15/05/13 11:56:01 INFO MemoryStore: Block broadcast_491 stored as values in memory (estimated size 21.0 KB, free 229.1 MB)
15/05/13 11:56:01 INFO MemoryStore: ensureFreeSpace(11145) called with curMem=38036765, maxMem=278302556
15/05/13 11:56:01 INFO MemoryStore: Block broadcast_491_piece0 stored as bytes in memory (estimated size 10.9 KB, free 229.1 MB)
15/05/13 11:56:01 INFO BlockManagerInfo: Added broadcast_491_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.6 MB)
15/05/13 11:56:01 INFO BlockManagerMaster: Updated info of block broadcast_491_piece0
15/05/13 11:56:01 INFO SparkContext: Created broadcast 491 from broadcast at DAGScheduler.scala:839
15/05/13 11:56:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 327 (MapPartitionsRDD[2331] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:56:01 INFO TaskSchedulerImpl: Adding task set 327.0 with 1 tasks
15/05/13 11:56:01 INFO TaskSetManager: Starting task 0.0 in stage 327.0 (TID 327, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:56:01 INFO BlockManagerInfo: Added broadcast_491_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.9 KB, free: 263.7 MB)
15/05/13 11:56:01 INFO BlockManagerInfo: Added broadcast_489_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:56:01 INFO TaskSetManager: Finished task 0.0 in stage 327.0 (TID 327) in 201 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:56:01 INFO TaskSchedulerImpl: Removed TaskSet 327.0, whose tasks have all completed, from pool 
15/05/13 11:56:01 INFO DAGScheduler: Stage 327 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.204 s
15/05/13 11:56:01 INFO DAGScheduler: Job 333 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.244379 s
15/05/13 11:56:01 INFO JobScheduler: Finished job streaming job 1431532560000 ms.0 from job set of time 1431532560000 ms
15/05/13 11:56:01 INFO JobScheduler: Total delay: 1.440 s for time 1431532560000 ms (execution: 1.177 s)
15/05/13 11:56:01 INFO MapPartitionsRDD: Removing RDD 2306 from persistence list
15/05/13 11:56:01 INFO BlockManager: Removing RDD 2306
15/05/13 11:56:01 INFO UnionRDD: Removing RDD 2305 from persistence list
15/05/13 11:56:01 INFO BlockManager: Removing RDD 2305
15/05/13 11:56:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532500000 ms: 1431532440000 ms
15/05/13 11:56:01 INFO JobGenerator: Checkpointing graph for time 1431532560000 ms
15/05/13 11:56:01 INFO DStreamGraph: Updating checkpoint data for time 1431532560000 ms
15/05/13 11:56:01 INFO DStreamGraph: Updated checkpoint data for time 1431532560000 ms
15/05/13 11:56:01 INFO CheckpointWriter: Saving checkpoint for time 1431532560000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000'
15/05/13 11:56:01 INFO CheckpointWriter: Checkpoint for time 1431532560000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000', took 7656 bytes and 48 ms
15/05/13 11:56:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532560000 ms
15/05/13 11:56:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532560000 ms
15/05/13 11:56:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:57:00 INFO FileInputDStream: Finding new files took 39 ms
15/05/13 11:57:00 INFO FileInputDStream: New files at time 1431532620000 ms:

15/05/13 11:57:00 INFO JobScheduler: Added jobs for time 1431532620000 ms
15/05/13 11:57:00 INFO JobGenerator: Checkpointing graph for time 1431532620000 ms
15/05/13 11:57:00 INFO DStreamGraph: Updating checkpoint data for time 1431532620000 ms
15/05/13 11:57:00 INFO JobScheduler: Starting job streaming job 1431532620000 ms.0 from job set of time 1431532620000 ms
15/05/13 11:57:00 INFO DStreamGraph: Updated checkpoint data for time 1431532620000 ms
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:57:00 INFO DAGScheduler: Job 334 finished: reduce at JsonRDD.scala:51, took 0.000199 s
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84295): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
An error occurred while calling o37.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:902)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:902)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:51)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:617)
	at sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)

15/05/13 11:57:00 INFO JobScheduler: Finished job streaming job 1431532620000 ms.0 from job set of time 1431532620000 ms
15/05/13 11:57:00 INFO JobScheduler: Total delay: 0.171 s for time 1431532620000 ms (execution: 0.111 s)
15/05/13 11:57:00 INFO MapPartitionsRDD: Removing RDD 2320 from persistence list
15/05/13 11:57:00 INFO BlockManager: Removing RDD 2320
15/05/13 11:57:00 INFO UnionRDD: Removing RDD 2319 from persistence list
15/05/13 11:57:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431532560000 ms: 1431532500000 ms
15/05/13 11:57:00 INFO BlockManager: Removing RDD 2319
15/05/13 11:57:00 INFO JobGenerator: Checkpointing graph for time 1431532620000 ms
15/05/13 11:57:00 INFO DStreamGraph: Updating checkpoint data for time 1431532620000 ms
15/05/13 11:57:00 INFO DStreamGraph: Updated checkpoint data for time 1431532620000 ms
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84297): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84299): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:57:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532620000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84301): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84301): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84303): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:57:00 INFO CheckpointWriter: Saving checkpoint for time 1431532620000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:57:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84305): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:57:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84305): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 11:57:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532620000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532620000'
15/05/13 11:58:00 INFO FileInputDStream: Finding new files took 78 ms
15/05/13 11:58:00 INFO FileInputDStream: New files at time 1431532680000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532560881.json
15/05/13 11:58:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=38047910, maxMem=278302556
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_492 stored as values in memory (estimated size 232.9 KB, free 228.9 MB)
15/05/13 11:58:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=38286442, maxMem=278302556
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_492_piece0 stored as bytes in memory (estimated size 34.9 KB, free 228.9 MB)
15/05/13 11:58:00 INFO BlockManagerInfo: Added broadcast_492_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_492_piece0
15/05/13 11:58:00 INFO SparkContext: Created broadcast 492 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:58:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:58:00 INFO JobScheduler: Added jobs for time 1431532680000 ms
15/05/13 11:58:00 INFO JobGenerator: Checkpointing graph for time 1431532680000 ms
15/05/13 11:58:00 INFO JobScheduler: Starting job streaming job 1431532680000 ms.0 from job set of time 1431532680000 ms
15/05/13 11:58:00 INFO DStreamGraph: Updating checkpoint data for time 1431532680000 ms
15/05/13 11:58:00 INFO DStreamGraph: Updated checkpoint data for time 1431532680000 ms
15/05/13 11:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431532680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000'
15/05/13 11:58:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84310): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431532680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000'
15/05/13 11:58:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:58:00 INFO DAGScheduler: Got job 335 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:58:00 INFO DAGScheduler: Final stage: Stage 328(reduce at JsonRDD.scala:51)
15/05/13 11:58:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:58:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:58:00 INFO DAGScheduler: Submitting Stage 328 (MapPartitionsRDD[2344] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:58:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38322150, maxMem=278302556
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_493 stored as values in memory (estimated size 5.9 KB, free 228.9 MB)
15/05/13 11:58:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=38328190, maxMem=278302556
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_493_piece0 stored as bytes in memory (estimated size 4.1 KB, free 228.9 MB)
15/05/13 11:58:00 INFO BlockManagerInfo: Added broadcast_493_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_493_piece0
15/05/13 11:58:00 INFO SparkContext: Created broadcast 493 from broadcast at DAGScheduler.scala:839
15/05/13 11:58:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 328 (MapPartitionsRDD[2344] at map at JsonRDD.scala:51)
15/05/13 11:58:00 INFO TaskSchedulerImpl: Adding task set 328.0 with 1 tasks
15/05/13 11:58:00 INFO TaskSetManager: Starting task 0.0 in stage 328.0 (TID 328, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:58:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84312): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 11:58:00 INFO CheckpointWriter: Saving checkpoint for time 1431532680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000'
15/05/13 11:58:00 INFO BlockManagerInfo: Added broadcast_493_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532320000
15/05/13 11:58:00 INFO CheckpointWriter: Checkpoint for time 1431532680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000', took 7643 bytes and 190 ms
15/05/13 11:58:00 INFO BlockManagerInfo: Added broadcast_492_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO TaskSetManager: Finished task 0.0 in stage 328.0 (TID 328) in 448 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:58:00 INFO TaskSchedulerImpl: Removed TaskSet 328.0, whose tasks have all completed, from pool 
15/05/13 11:58:00 INFO DAGScheduler: Stage 328 (reduce at JsonRDD.scala:51) finished in 0.492 s
15/05/13 11:58:00 INFO DAGScheduler: Job 335 finished: reduce at JsonRDD.scala:51, took 0.536461 s
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 485
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_485_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_485_piece0 of size 11121 dropped from memory (free 239981262)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_485_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_485_piece0
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_485
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_485 of size 21480 dropped from memory (free 240002742)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_485_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 485
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 484
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_484
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_484 of size 6040 dropped from memory (free 240008782)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_484_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_484_piece0 of size 4225 dropped from memory (free 240013007)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_484_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_484_piece0
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_484_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 484
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 482
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_482
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_482 of size 21480 dropped from memory (free 240034487)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_482_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_482_piece0 of size 11135 dropped from memory (free 240045622)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_482_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_482_piece0
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_482_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 482
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 488
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_488
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_488 of size 21480 dropped from memory (free 240067102)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_488_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_488_piece0 of size 11143 dropped from memory (free 240078245)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_488_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_488_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_488_piece0
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 488
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 487
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_487
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_487 of size 6040 dropped from memory (free 240084285)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_487_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_487_piece0 of size 4228 dropped from memory (free 240088513)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_487_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.6 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_487_piece0
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_487_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 4.1 KB, free: 263.5 MB)
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 487
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 486
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_486
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_486 of size 238532 dropped from memory (free 240327045)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_486_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_486_piece0 of size 35708 dropped from memory (free 240362753)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_486_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.7 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_486_piece0
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_486_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 34.9 KB, free: 263.5 MB)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_486_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:58:00 INFO ContextCleaner: Cleaned broadcast 486
15/05/13 11:58:00 INFO BlockManager: Removing broadcast 491
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_491
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_491 of size 21480 dropped from memory (free 240384233)
15/05/13 11:58:00 INFO BlockManager: Removing block broadcast_491_piece0
15/05/13 11:58:00 INFO MemoryStore: Block broadcast_491_piece0 of size 11145 dropped from memory (free 240395378)
15/05/13 11:58:00 INFO BlockManagerInfo: Removed broadcast_491_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.7 MB)
15/05/13 11:58:00 INFO BlockManagerMaster: Updated info of block broadcast_491_piece0
15/05/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_491_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 10.9 KB, free: 263.6 MB)
15/05/13 11:58:01 INFO ContextCleaner: Cleaned broadcast 491
15/05/13 11:58:01 INFO BlockManager: Removing broadcast 490
15/05/13 11:58:01 INFO BlockManager: Removing block broadcast_490
15/05/13 11:58:01 INFO MemoryStore: Block broadcast_490 of size 6040 dropped from memory (free 240401418)
15/05/13 11:58:01 INFO BlockManager: Removing block broadcast_490_piece0
15/05/13 11:58:01 INFO MemoryStore: Block broadcast_490_piece0 of size 4225 dropped from memory (free 240405643)
15/05/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_490_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.7 MB)
15/05/13 11:58:01 INFO BlockManagerMaster: Updated info of block broadcast_490_piece0
15/05/13 11:58:01 INFO BlockManagerInfo: Removed broadcast_490_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 11:58:01 INFO ContextCleaner: Cleaned broadcast 490
15/05/13 11:58:01 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:58:01 INFO DAGScheduler: Got job 336 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:58:01 INFO DAGScheduler: Final stage: Stage 329(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:58:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:58:01 INFO DAGScheduler: Missing parents: List()
15/05/13 11:58:01 INFO DAGScheduler: Submitting Stage 329 (MapPartitionsRDD[2351] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:58:01 INFO MemoryStore: ensureFreeSpace(21656) called with curMem=37896913, maxMem=278302556
15/05/13 11:58:01 INFO MemoryStore: Block broadcast_494 stored as values in memory (estimated size 21.1 KB, free 229.2 MB)
15/05/13 11:58:01 INFO MemoryStore: ensureFreeSpace(11158) called with curMem=37918569, maxMem=278302556
15/05/13 11:58:01 INFO MemoryStore: Block broadcast_494_piece0 stored as bytes in memory (estimated size 10.9 KB, free 229.2 MB)
15/05/13 11:58:01 INFO BlockManagerInfo: Added broadcast_494_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.9 KB, free: 260.7 MB)
15/05/13 11:58:01 INFO BlockManagerMaster: Updated info of block broadcast_494_piece0
15/05/13 11:58:01 INFO SparkContext: Created broadcast 494 from broadcast at DAGScheduler.scala:839
15/05/13 11:58:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 329 (MapPartitionsRDD[2351] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:58:01 INFO TaskSchedulerImpl: Adding task set 329.0 with 1 tasks
15/05/13 11:58:01 INFO TaskSetManager: Starting task 0.0 in stage 329.0 (TID 329, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:58:01 INFO BlockManagerInfo: Added broadcast_494_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.9 KB, free: 261.9 MB)
15/05/13 11:58:01 INFO TaskSetManager: Finished task 0.0 in stage 329.0 (TID 329) in 98 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:58:01 INFO TaskSchedulerImpl: Removed TaskSet 329.0, whose tasks have all completed, from pool 
15/05/13 11:58:01 INFO DAGScheduler: Stage 329 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.098 s
15/05/13 11:58:01 INFO DAGScheduler: Job 336 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.135710 s
15/05/13 11:58:01 INFO JobScheduler: Finished job streaming job 1431532680000 ms.0 from job set of time 1431532680000 ms
15/05/13 11:58:01 INFO JobScheduler: Total delay: 1.289 s for time 1431532680000 ms (execution: 1.059 s)
15/05/13 11:58:01 INFO MapPartitionsRDD: Removing RDD 2333 from persistence list
15/05/13 11:58:01 INFO BlockManager: Removing RDD 2333
15/05/13 11:58:01 INFO UnionRDD: Removing RDD 2332 from persistence list
15/05/13 11:58:01 INFO BlockManager: Removing RDD 2332
15/05/13 11:58:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532620000 ms: 1431532560000 ms
15/05/13 11:58:01 INFO JobGenerator: Checkpointing graph for time 1431532680000 ms
15/05/13 11:58:01 INFO DStreamGraph: Updating checkpoint data for time 1431532680000 ms
15/05/13 11:58:01 INFO DStreamGraph: Updated checkpoint data for time 1431532680000 ms
15/05/13 11:58:01 INFO CheckpointWriter: Saving checkpoint for time 1431532680000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000'
15/05/13 11:58:01 INFO CheckpointWriter: Checkpoint for time 1431532680000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532680000', took 7647 bytes and 44 ms
15/05/13 11:58:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532680000 ms
15/05/13 11:58:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532680000 ms
15/05/13 11:58:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 11:59:00 INFO FileInputDStream: Finding new files took 26 ms
15/05/13 11:59:00 INFO FileInputDStream: New files at time 1431532740000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532625123.json
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=37929727, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_495 stored as values in memory (estimated size 232.9 KB, free 229.0 MB)
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=38168259, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_495_piece0 stored as bytes in memory (estimated size 34.9 KB, free 229.0 MB)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.6 MB)
15/05/13 11:59:00 INFO BlockManagerMaster: Updated info of block broadcast_495_piece0
15/05/13 11:59:00 INFO SparkContext: Created broadcast 495 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 11:59:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 11:59:00 INFO JobScheduler: Added jobs for time 1431532740000 ms
15/05/13 11:59:00 INFO JobGenerator: Checkpointing graph for time 1431532740000 ms
15/05/13 11:59:00 INFO JobScheduler: Starting job streaming job 1431532740000 ms.0 from job set of time 1431532740000 ms
15/05/13 11:59:00 INFO DStreamGraph: Updating checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO DStreamGraph: Updated checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431532740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532740000'
15/05/13 11:59:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 11:59:00 INFO DAGScheduler: Got job 337 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 11:59:00 INFO DAGScheduler: Final stage: Stage 330(reduce at JsonRDD.scala:51)
15/05/13 11:59:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:59:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:59:00 INFO DAGScheduler: Submitting Stage 330 (MapPartitionsRDD[2358] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38203967, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_496 stored as values in memory (estimated size 5.9 KB, free 229.0 MB)
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(4224) called with curMem=38210007, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_496_piece0 stored as bytes in memory (estimated size 4.1 KB, free 229.0 MB)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_496_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.6 MB)
15/05/13 11:59:00 INFO BlockManagerMaster: Updated info of block broadcast_496_piece0
15/05/13 11:59:00 INFO SparkContext: Created broadcast 496 from broadcast at DAGScheduler.scala:839
15/05/13 11:59:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 330 (MapPartitionsRDD[2358] at map at JsonRDD.scala:51)
15/05/13 11:59:00 INFO TaskSchedulerImpl: Adding task set 330.0 with 1 tasks
15/05/13 11:59:00 INFO TaskSetManager: Starting task 0.0 in stage 330.0 (TID 330, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:59:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532380000
15/05/13 11:59:00 INFO CheckpointWriter: Checkpoint for time 1431532740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532740000', took 7644 bytes and 66 ms
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_496_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.6 MB)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 11:59:00 INFO DAGScheduler: Stage 330 (reduce at JsonRDD.scala:51) finished in 0.230 s
15/05/13 11:59:00 INFO DAGScheduler: Job 337 finished: reduce at JsonRDD.scala:51, took 0.245810 s
15/05/13 11:59:00 INFO TaskSetManager: Finished task 0.0 in stage 330.0 (TID 330) in 216 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 11:59:00 INFO TaskSchedulerImpl: Removed TaskSet 330.0, whose tasks have all completed, from pool 
15/05/13 11:59:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 11:59:00 INFO DAGScheduler: Got job 338 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 11:59:00 INFO DAGScheduler: Final stage: Stage 331(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:59:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 11:59:00 INFO DAGScheduler: Missing parents: List()
15/05/13 11:59:00 INFO DAGScheduler: Submitting Stage 331 (MapPartitionsRDD[2365] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=38214231, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_497 stored as values in memory (estimated size 20.6 KB, free 228.9 MB)
15/05/13 11:59:00 INFO MemoryStore: ensureFreeSpace(10947) called with curMem=38235327, maxMem=278302556
15/05/13 11:59:00 INFO MemoryStore: Block broadcast_497_piece0 stored as bytes in memory (estimated size 10.7 KB, free 228.9 MB)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_497_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 260.6 MB)
15/05/13 11:59:00 INFO BlockManagerMaster: Updated info of block broadcast_497_piece0
15/05/13 11:59:00 INFO SparkContext: Created broadcast 497 from broadcast at DAGScheduler.scala:839
15/05/13 11:59:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 331 (MapPartitionsRDD[2365] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 11:59:00 INFO TaskSchedulerImpl: Adding task set 331.0 with 1 tasks
15/05/13 11:59:00 INFO TaskSetManager: Starting task 0.0 in stage 331.0 (TID 331, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_497_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.7 KB, free: 261.9 MB)
15/05/13 11:59:00 INFO BlockManagerInfo: Added broadcast_495_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.9 MB)
15/05/13 11:59:00 INFO TaskSetManager: Finished task 0.0 in stage 331.0 (TID 331) in 173 ms on pti-base.insafanalytics.com (1/1)
15/05/13 11:59:00 INFO TaskSchedulerImpl: Removed TaskSet 331.0, whose tasks have all completed, from pool 
15/05/13 11:59:00 INFO DAGScheduler: Stage 331 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.173 s
15/05/13 11:59:00 INFO DAGScheduler: Job 338 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.194449 s
15/05/13 11:59:00 INFO JobScheduler: Finished job streaming job 1431532740000 ms.0 from job set of time 1431532740000 ms
15/05/13 11:59:00 INFO JobScheduler: Total delay: 0.734 s for time 1431532740000 ms (execution: 0.642 s)
15/05/13 11:59:00 INFO MapPartitionsRDD: Removing RDD 2340 from persistence list
15/05/13 11:59:00 INFO BlockManager: Removing RDD 2340
15/05/13 11:59:00 INFO UnionRDD: Removing RDD 2339 from persistence list
15/05/13 11:59:00 INFO BlockManager: Removing RDD 2339
15/05/13 11:59:00 INFO FileInputDStream: Cleared 1 old files that were older than 1431532680000 ms: 1431532620000 ms
15/05/13 11:59:00 INFO JobGenerator: Checkpointing graph for time 1431532740000 ms
15/05/13 11:59:00 INFO DStreamGraph: Updating checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO DStreamGraph: Updated checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO CheckpointWriter: Saving checkpoint for time 1431532740000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532740000'
15/05/13 11:59:00 INFO CheckpointWriter: Checkpoint for time 1431532740000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532740000', took 7655 bytes and 40 ms
15/05/13 11:59:00 INFO DStreamGraph: Clearing checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO DStreamGraph: Cleared checkpoint data for time 1431532740000 ms
15/05/13 11:59:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 12:00:00 INFO FileInputDStream: Finding new files took 66 ms
15/05/13 12:00:00 INFO FileInputDStream: New files at time 1431532800000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532686690.json
15/05/13 12:00:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=38246274, maxMem=278302556
15/05/13 12:00:00 INFO MemoryStore: Block broadcast_498 stored as values in memory (estimated size 232.9 KB, free 228.7 MB)
15/05/13 12:00:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=38484806, maxMem=278302556
15/05/13 12:00:00 INFO MemoryStore: Block broadcast_498_piece0 stored as bytes in memory (estimated size 34.9 KB, free 228.7 MB)
15/05/13 12:00:00 INFO BlockManagerInfo: Added broadcast_498_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.6 MB)
15/05/13 12:00:00 INFO BlockManagerMaster: Updated info of block broadcast_498_piece0
15/05/13 12:00:00 INFO SparkContext: Created broadcast 498 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 12:00:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 12:00:00 INFO JobScheduler: Added jobs for time 1431532800000 ms
15/05/13 12:00:00 INFO JobGenerator: Checkpointing graph for time 1431532800000 ms
15/05/13 12:00:00 INFO DStreamGraph: Updating checkpoint data for time 1431532800000 ms
15/05/13 12:00:00 INFO DStreamGraph: Updated checkpoint data for time 1431532800000 ms
15/05/13 12:00:00 INFO JobScheduler: Starting job streaming job 1431532800000 ms.0 from job set of time 1431532800000 ms
15/05/13 12:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431532800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000'
15/05/13 12:00:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84330): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431532800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000'
15/05/13 12:00:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 12:00:00 INFO DAGScheduler: Got job 339 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 12:00:00 INFO DAGScheduler: Final stage: Stage 332(reduce at JsonRDD.scala:51)
15/05/13 12:00:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:00:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:00:00 INFO DAGScheduler: Submitting Stage 332 (MapPartitionsRDD[2372] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 12:00:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38520514, maxMem=278302556
15/05/13 12:00:00 INFO MemoryStore: Block broadcast_499 stored as values in memory (estimated size 5.9 KB, free 228.7 MB)
15/05/13 12:00:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=38526554, maxMem=278302556
15/05/13 12:00:00 INFO MemoryStore: Block broadcast_499_piece0 stored as bytes in memory (estimated size 4.1 KB, free 228.7 MB)
15/05/13 12:00:00 INFO BlockManagerInfo: Added broadcast_499_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.6 MB)
15/05/13 12:00:00 INFO BlockManagerMaster: Updated info of block broadcast_499_piece0
15/05/13 12:00:00 INFO SparkContext: Created broadcast 499 from broadcast at DAGScheduler.scala:839
15/05/13 12:00:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 332 (MapPartitionsRDD[2372] at map at JsonRDD.scala:51)
15/05/13 12:00:00 INFO TaskSchedulerImpl: Adding task set 332.0 with 1 tasks
15/05/13 12:00:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84332): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:00:00 INFO CheckpointWriter: Saving checkpoint for time 1431532800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000'
15/05/13 12:00:00 INFO TaskSetManager: Starting task 0.0 in stage 332.0 (TID 332, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:00:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84334): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:00:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84334): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:00:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532800000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000'
15/05/13 12:00:00 INFO BlockManagerInfo: Added broadcast_499_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 261.9 MB)
15/05/13 12:00:00 INFO BlockManagerInfo: Added broadcast_498_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.8 MB)
15/05/13 12:00:00 INFO TaskSetManager: Finished task 0.0 in stage 332.0 (TID 332) in 392 ms on pti-base.insafanalytics.com (1/1)
15/05/13 12:00:00 INFO TaskSchedulerImpl: Removed TaskSet 332.0, whose tasks have all completed, from pool 
15/05/13 12:00:00 INFO DAGScheduler: Stage 332 (reduce at JsonRDD.scala:51) finished in 0.407 s
15/05/13 12:00:00 INFO DAGScheduler: Job 339 finished: reduce at JsonRDD.scala:51, took 0.441051 s
15/05/13 12:00:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 12:00:00 INFO DAGScheduler: Got job 340 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 12:00:00 INFO DAGScheduler: Final stage: Stage 333(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:00:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:00:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:00:00 INFO DAGScheduler: Submitting Stage 333 (MapPartitionsRDD[2379] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 12:00:00 INFO MemoryStore: ensureFreeSpace(19208) called with curMem=38530779, maxMem=278302556
15/05/13 12:00:01 INFO MemoryStore: Block broadcast_500 stored as values in memory (estimated size 18.8 KB, free 228.6 MB)
15/05/13 12:00:01 INFO MemoryStore: ensureFreeSpace(10428) called with curMem=38549987, maxMem=278302556
15/05/13 12:00:01 INFO MemoryStore: Block broadcast_500_piece0 stored as bytes in memory (estimated size 10.2 KB, free 228.6 MB)
15/05/13 12:00:01 INFO BlockManagerInfo: Added broadcast_500_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.2 KB, free: 260.6 MB)
15/05/13 12:00:01 INFO BlockManagerMaster: Updated info of block broadcast_500_piece0
15/05/13 12:00:01 INFO SparkContext: Created broadcast 500 from broadcast at DAGScheduler.scala:839
15/05/13 12:00:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 333 (MapPartitionsRDD[2379] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:00:01 INFO TaskSchedulerImpl: Adding task set 333.0 with 1 tasks
15/05/13 12:00:01 INFO TaskSetManager: Starting task 0.0 in stage 333.0 (TID 333, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:00:01 INFO BlockManagerInfo: Added broadcast_500_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 10.2 KB, free: 261.8 MB)
15/05/13 12:00:01 INFO TaskSetManager: Finished task 0.0 in stage 333.0 (TID 333) in 102 ms on pti-base.insafanalytics.com (1/1)
15/05/13 12:00:01 INFO TaskSchedulerImpl: Removed TaskSet 333.0, whose tasks have all completed, from pool 
15/05/13 12:00:01 INFO DAGScheduler: Stage 333 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.104 s
15/05/13 12:00:01 INFO DAGScheduler: Job 340 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.139653 s
15/05/13 12:00:01 INFO JobScheduler: Finished job streaming job 1431532800000 ms.0 from job set of time 1431532800000 ms
15/05/13 12:00:01 INFO JobScheduler: Total delay: 1.181 s for time 1431532800000 ms (execution: 1.008 s)
15/05/13 12:00:01 INFO MapPartitionsRDD: Removing RDD 2354 from persistence list
15/05/13 12:00:01 INFO BlockManager: Removing RDD 2354
15/05/13 12:00:01 INFO UnionRDD: Removing RDD 2353 from persistence list
15/05/13 12:00:01 INFO BlockManager: Removing RDD 2353
15/05/13 12:00:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532740000 ms: 1431532680000 ms
15/05/13 12:00:01 INFO JobGenerator: Checkpointing graph for time 1431532800000 ms
15/05/13 12:00:01 INFO DStreamGraph: Updating checkpoint data for time 1431532800000 ms
15/05/13 12:00:01 INFO DStreamGraph: Updated checkpoint data for time 1431532800000 ms
15/05/13 12:00:01 INFO CheckpointWriter: Saving checkpoint for time 1431532800000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000'
15/05/13 12:00:01 INFO CheckpointWriter: Checkpoint for time 1431532800000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532800000', took 7663 bytes and 61 ms
15/05/13 12:00:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532800000 ms
15/05/13 12:00:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532800000 ms
15/05/13 12:00:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 12:01:00 INFO FileInputDStream: Finding new files took 107 ms
15/05/13 12:01:00 INFO FileInputDStream: New files at time 1431532860000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532748770.json
15/05/13 12:01:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=38560415, maxMem=278302556
15/05/13 12:01:00 INFO MemoryStore: Block broadcast_501 stored as values in memory (estimated size 232.9 KB, free 228.4 MB)
15/05/13 12:01:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=38798947, maxMem=278302556
15/05/13 12:01:00 INFO MemoryStore: Block broadcast_501_piece0 stored as bytes in memory (estimated size 34.9 KB, free 228.4 MB)
15/05/13 12:01:00 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.6 MB)
15/05/13 12:01:00 INFO BlockManagerMaster: Updated info of block broadcast_501_piece0
15/05/13 12:01:00 INFO SparkContext: Created broadcast 501 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 12:01:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 12:01:00 INFO JobScheduler: Starting job streaming job 1431532860000 ms.0 from job set of time 1431532860000 ms
15/05/13 12:01:00 INFO JobScheduler: Added jobs for time 1431532860000 ms
15/05/13 12:01:00 INFO JobGenerator: Checkpointing graph for time 1431532860000 ms
15/05/13 12:01:00 INFO DStreamGraph: Updating checkpoint data for time 1431532860000 ms
15/05/13 12:01:00 INFO DStreamGraph: Updated checkpoint data for time 1431532860000 ms
15/05/13 12:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431532860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000'
15/05/13 12:01:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84340): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431532860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000'
15/05/13 12:01:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 12:01:00 INFO DAGScheduler: Got job 341 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 12:01:00 INFO DAGScheduler: Final stage: Stage 334(reduce at JsonRDD.scala:51)
15/05/13 12:01:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:01:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:01:00 INFO DAGScheduler: Submitting Stage 334 (MapPartitionsRDD[2386] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 12:01:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38834655, maxMem=278302556
15/05/13 12:01:00 INFO MemoryStore: Block broadcast_502 stored as values in memory (estimated size 5.9 KB, free 228.4 MB)
15/05/13 12:01:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=38840695, maxMem=278302556
15/05/13 12:01:00 INFO MemoryStore: Block broadcast_502_piece0 stored as bytes in memory (estimated size 4.1 KB, free 228.4 MB)
15/05/13 12:01:00 INFO BlockManagerInfo: Added broadcast_502_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.5 MB)
15/05/13 12:01:00 INFO BlockManagerMaster: Updated info of block broadcast_502_piece0
15/05/13 12:01:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84342): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:01:00 INFO CheckpointWriter: Saving checkpoint for time 1431532860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000'
15/05/13 12:01:00 INFO SparkContext: Created broadcast 502 from broadcast at DAGScheduler.scala:839
15/05/13 12:01:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 334 (MapPartitionsRDD[2386] at map at JsonRDD.scala:51)
15/05/13 12:01:00 INFO TaskSchedulerImpl: Adding task set 334.0 with 1 tasks
15/05/13 12:01:00 INFO TaskSetManager: Starting task 0.0 in stage 334.0 (TID 334, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:01:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84344): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:01:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84344): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:01:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532860000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000'
15/05/13 12:01:00 INFO BlockManagerInfo: Added broadcast_502_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 261.8 MB)
15/05/13 12:01:00 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.8 MB)
15/05/13 12:01:00 INFO TaskSetManager: Finished task 0.0 in stage 334.0 (TID 334) in 378 ms on pti-base.insafanalytics.com (1/1)
15/05/13 12:01:00 INFO TaskSchedulerImpl: Removed TaskSet 334.0, whose tasks have all completed, from pool 
15/05/13 12:01:00 INFO DAGScheduler: Stage 334 (reduce at JsonRDD.scala:51) finished in 0.393 s
15/05/13 12:01:00 INFO DAGScheduler: Job 341 finished: reduce at JsonRDD.scala:51, took 0.434157 s
15/05/13 12:01:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 12:01:00 INFO DAGScheduler: Got job 342 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 12:01:00 INFO DAGScheduler: Final stage: Stage 335(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:01:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:01:01 INFO DAGScheduler: Missing parents: List()
15/05/13 12:01:01 INFO DAGScheduler: Submitting Stage 335 (MapPartitionsRDD[2393] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 12:01:01 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=38844920, maxMem=278302556
15/05/13 12:01:01 INFO MemoryStore: Block broadcast_503 stored as values in memory (estimated size 20.4 KB, free 228.3 MB)
15/05/13 12:01:01 INFO MemoryStore: ensureFreeSpace(10836) called with curMem=38865840, maxMem=278302556
15/05/13 12:01:01 INFO MemoryStore: Block broadcast_503_piece0 stored as bytes in memory (estimated size 10.6 KB, free 228.3 MB)
15/05/13 12:01:01 INFO BlockManagerInfo: Added broadcast_503_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 260.5 MB)
15/05/13 12:01:01 INFO BlockManagerMaster: Updated info of block broadcast_503_piece0
15/05/13 12:01:01 INFO SparkContext: Created broadcast 503 from broadcast at DAGScheduler.scala:839
15/05/13 12:01:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 335 (MapPartitionsRDD[2393] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:01:01 INFO TaskSchedulerImpl: Adding task set 335.0 with 1 tasks
15/05/13 12:01:01 INFO TaskSetManager: Starting task 0.0 in stage 335.0 (TID 335, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:01:01 INFO BlockManagerInfo: Added broadcast_503_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 263.5 MB)
15/05/13 12:01:01 INFO BlockManagerInfo: Added broadcast_501_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.5 MB)
15/05/13 12:01:01 INFO TaskSetManager: Finished task 0.0 in stage 335.0 (TID 335) in 190 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 12:01:01 INFO TaskSchedulerImpl: Removed TaskSet 335.0, whose tasks have all completed, from pool 
15/05/13 12:01:01 INFO DAGScheduler: Stage 335 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.192 s
15/05/13 12:01:01 INFO DAGScheduler: Job 342 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.221960 s
15/05/13 12:01:01 INFO JobScheduler: Finished job streaming job 1431532860000 ms.0 from job set of time 1431532860000 ms
15/05/13 12:01:01 INFO JobScheduler: Total delay: 1.269 s for time 1431532860000 ms (execution: 1.034 s)
15/05/13 12:01:01 INFO MapPartitionsRDD: Removing RDD 2368 from persistence list
15/05/13 12:01:01 INFO BlockManager: Removing RDD 2368
15/05/13 12:01:01 INFO UnionRDD: Removing RDD 2367 from persistence list
15/05/13 12:01:01 INFO BlockManager: Removing RDD 2367
15/05/13 12:01:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532800000 ms: 1431532740000 ms
15/05/13 12:01:01 INFO JobGenerator: Checkpointing graph for time 1431532860000 ms
15/05/13 12:01:01 INFO DStreamGraph: Updating checkpoint data for time 1431532860000 ms
15/05/13 12:01:01 INFO DStreamGraph: Updated checkpoint data for time 1431532860000 ms
15/05/13 12:01:01 INFO CheckpointWriter: Saving checkpoint for time 1431532860000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000'
15/05/13 12:01:01 INFO CheckpointWriter: Checkpoint for time 1431532860000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532860000', took 7655 bytes and 68 ms
15/05/13 12:01:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532860000 ms
15/05/13 12:01:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532860000 ms
15/05/13 12:01:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 495
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_495_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_495_piece0 of size 35708 dropped from memory (free 239461588)
15/05/13 12:02:00 INFO FileInputDStream: Finding new files took 103 ms
15/05/13 12:02:00 INFO FileInputDStream: New files at time 1431532920000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532810409.json
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_495_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 34.9 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_495_piece0
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_495
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_495 of size 238532 dropped from memory (free 239700120)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_495_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 34.9 KB, free: 263.6 MB)
15/05/13 12:02:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=38602436, maxMem=278302556
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_504 stored as values in memory (estimated size 232.9 KB, free 228.4 MB)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_495_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 34.9 KB, free: 261.8 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 495
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 494
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_494
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_494 of size 21656 dropped from memory (free 239483244)
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_494_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_494_piece0 of size 11158 dropped from memory (free 239494402)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_494_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.9 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_494_piece0
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_494_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.9 KB, free: 261.8 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 494
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 497
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_497
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_497 of size 21096 dropped from memory (free 239515498)
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_497_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_497_piece0 of size 10947 dropped from memory (free 239526445)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_497_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.7 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_497_piece0
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_497_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.7 KB, free: 261.8 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 497
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 496
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_496
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_496 of size 6040 dropped from memory (free 239532485)
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_496_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_496_piece0 of size 4224 dropped from memory (free 239536709)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_496_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_496_piece0
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_496_piece0 on pti-node-1.insafanalytics.com:34093 in memory (size: 4.1 KB, free: 263.6 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 496
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 503
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_503_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_503_piece0 of size 10836 dropped from memory (free 239547545)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_503_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.6 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_503_piece0
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_503
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_503 of size 20920 dropped from memory (free 239568465)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_503_piece0 on pti-node-2.insafanalytics.com:33883 in memory (size: 10.6 KB, free: 263.5 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 503
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 502
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_502_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_502_piece0 of size 4225 dropped from memory (free 239572690)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_502_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_502_piece0
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_502
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_502 of size 6040 dropped from memory (free 239578730)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_502_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 502
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 500
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_500
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_500 of size 19208 dropped from memory (free 239597938)
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_500_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_500_piece0 of size 10428 dropped from memory (free 239608366)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_500_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 10.2 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_500_piece0
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_500_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 10.2 KB, free: 261.9 MB)
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 500
15/05/13 12:02:00 INFO BlockManager: Removing broadcast 499
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_499_piece0
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_499_piece0 of size 4225 dropped from memory (free 239612591)
15/05/13 12:02:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=38689965, maxMem=278302556
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_504_piece0 stored as bytes in memory (estimated size 34.9 KB, free 228.5 MB)
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_499_piece0 on pti-base.insafanalytics.com:44723 in memory (size: 4.1 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_499_piece0
15/05/13 12:02:00 INFO BlockManager: Removing block broadcast_499
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_499 of size 6040 dropped from memory (free 239582923)
15/05/13 12:02:00 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_504_piece0
15/05/13 12:02:00 INFO BlockManagerInfo: Removed broadcast_499_piece0 on pti-base.insafanalytics.com:60443 in memory (size: 4.1 KB, free: 261.9 MB)
15/05/13 12:02:00 INFO SparkContext: Created broadcast 504 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 12:02:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 12:02:00 INFO ContextCleaner: Cleaned broadcast 499
15/05/13 12:02:00 INFO JobScheduler: Added jobs for time 1431532920000 ms
15/05/13 12:02:00 INFO JobGenerator: Checkpointing graph for time 1431532920000 ms
15/05/13 12:02:00 INFO DStreamGraph: Updating checkpoint data for time 1431532920000 ms
15/05/13 12:02:00 INFO JobScheduler: Starting job streaming job 1431532920000 ms.0 from job set of time 1431532920000 ms
15/05/13 12:02:00 INFO DStreamGraph: Updated checkpoint data for time 1431532920000 ms
15/05/13 12:02:00 INFO CheckpointWriter: Saving checkpoint for time 1431532920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532920000'
15/05/13 12:02:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532560000
15/05/13 12:02:00 INFO CheckpointWriter: Checkpoint for time 1431532920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532920000', took 7672 bytes and 89 ms
15/05/13 12:02:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 12:02:00 INFO DAGScheduler: Got job 343 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 12:02:00 INFO DAGScheduler: Final stage: Stage 336(reduce at JsonRDD.scala:51)
15/05/13 12:02:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:02:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:02:00 INFO DAGScheduler: Submitting Stage 336 (MapPartitionsRDD[2400] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 12:02:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=38719633, maxMem=278302556
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_505 stored as values in memory (estimated size 5.9 KB, free 228.5 MB)
15/05/13 12:02:00 INFO MemoryStore: ensureFreeSpace(4225) called with curMem=38725673, maxMem=278302556
15/05/13 12:02:00 INFO MemoryStore: Block broadcast_505_piece0 stored as bytes in memory (estimated size 4.1 KB, free 228.5 MB)
15/05/13 12:02:00 INFO BlockManagerInfo: Added broadcast_505_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.6 MB)
15/05/13 12:02:00 INFO BlockManagerMaster: Updated info of block broadcast_505_piece0
15/05/13 12:02:00 INFO SparkContext: Created broadcast 505 from broadcast at DAGScheduler.scala:839
15/05/13 12:02:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 336 (MapPartitionsRDD[2400] at map at JsonRDD.scala:51)
15/05/13 12:02:00 INFO TaskSchedulerImpl: Adding task set 336.0 with 1 tasks
15/05/13 12:02:00 INFO TaskSetManager: Starting task 0.0 in stage 336.0 (TID 336, pti-base.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:02:00 INFO BlockManagerInfo: Added broadcast_505_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 4.1 KB, free: 261.9 MB)
15/05/13 12:02:00 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on pti-base.insafanalytics.com:60443 (size: 34.9 KB, free: 261.8 MB)
15/05/13 12:02:00 INFO TaskSetManager: Finished task 0.0 in stage 336.0 (TID 336) in 341 ms on pti-base.insafanalytics.com (1/1)
15/05/13 12:02:00 INFO TaskSchedulerImpl: Removed TaskSet 336.0, whose tasks have all completed, from pool 
15/05/13 12:02:00 INFO DAGScheduler: Stage 336 (reduce at JsonRDD.scala:51) finished in 0.354 s
15/05/13 12:02:00 INFO DAGScheduler: Job 343 finished: reduce at JsonRDD.scala:51, took 0.384629 s
15/05/13 12:02:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 12:02:01 INFO DAGScheduler: Got job 344 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 12:02:01 INFO DAGScheduler: Final stage: Stage 337(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:02:01 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:02:01 INFO DAGScheduler: Missing parents: List()
15/05/13 12:02:01 INFO DAGScheduler: Submitting Stage 337 (MapPartitionsRDD[2407] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 12:02:01 INFO MemoryStore: ensureFreeSpace(20912) called with curMem=38729898, maxMem=278302556
15/05/13 12:02:01 INFO MemoryStore: Block broadcast_506 stored as values in memory (estimated size 20.4 KB, free 228.5 MB)
15/05/13 12:02:01 INFO MemoryStore: ensureFreeSpace(10815) called with curMem=38750810, maxMem=278302556
15/05/13 12:02:01 INFO MemoryStore: Block broadcast_506_piece0 stored as bytes in memory (estimated size 10.6 KB, free 228.4 MB)
15/05/13 12:02:01 INFO BlockManagerInfo: Added broadcast_506_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 260.6 MB)
15/05/13 12:02:01 INFO BlockManagerMaster: Updated info of block broadcast_506_piece0
15/05/13 12:02:01 INFO SparkContext: Created broadcast 506 from broadcast at DAGScheduler.scala:839
15/05/13 12:02:01 INFO DAGScheduler: Submitting 1 missing tasks from Stage 337 (MapPartitionsRDD[2407] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:02:01 INFO TaskSchedulerImpl: Adding task set 337.0 with 1 tasks
15/05/13 12:02:01 INFO TaskSetManager: Starting task 0.0 in stage 337.0 (TID 337, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:02:01 INFO BlockManagerInfo: Added broadcast_506_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.6 KB, free: 263.6 MB)
15/05/13 12:02:01 INFO BlockManagerInfo: Added broadcast_504_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 12:02:01 INFO TaskSetManager: Finished task 0.0 in stage 337.0 (TID 337) in 184 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 12:02:01 INFO TaskSchedulerImpl: Removed TaskSet 337.0, whose tasks have all completed, from pool 
15/05/13 12:02:01 INFO DAGScheduler: Stage 337 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.186 s
15/05/13 12:02:01 INFO DAGScheduler: Job 344 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.224395 s
15/05/13 12:02:01 INFO JobScheduler: Finished job streaming job 1431532920000 ms.0 from job set of time 1431532920000 ms
15/05/13 12:02:01 INFO JobScheduler: Total delay: 1.247 s for time 1431532920000 ms (execution: 0.965 s)
15/05/13 12:02:01 INFO MapPartitionsRDD: Removing RDD 2382 from persistence list
15/05/13 12:02:01 INFO BlockManager: Removing RDD 2382
15/05/13 12:02:01 INFO UnionRDD: Removing RDD 2381 from persistence list
15/05/13 12:02:01 INFO BlockManager: Removing RDD 2381
15/05/13 12:02:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532860000 ms: 1431532800000 ms
15/05/13 12:02:01 INFO JobGenerator: Checkpointing graph for time 1431532920000 ms
15/05/13 12:02:01 INFO DStreamGraph: Updating checkpoint data for time 1431532920000 ms
15/05/13 12:02:01 INFO DStreamGraph: Updated checkpoint data for time 1431532920000 ms
15/05/13 12:02:01 INFO CheckpointWriter: Saving checkpoint for time 1431532920000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532920000'
15/05/13 12:02:01 INFO CheckpointWriter: Checkpoint for time 1431532920000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532920000', took 7664 bytes and 51 ms
15/05/13 12:02:01 INFO DStreamGraph: Clearing checkpoint data for time 1431532920000 ms
15/05/13 12:02:01 INFO DStreamGraph: Cleared checkpoint data for time 1431532920000 ms
15/05/13 12:02:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 12:03:00 INFO FileInputDStream: Finding new files took 54 ms
15/05/13 12:03:00 INFO FileInputDStream: New files at time 1431532980000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532872347.json
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=38761625, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_507 stored as values in memory (estimated size 232.9 KB, free 228.2 MB)
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=39000157, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_507_piece0 stored as bytes in memory (estimated size 34.9 KB, free 228.2 MB)
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_507_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.5 MB)
15/05/13 12:03:00 INFO BlockManagerMaster: Updated info of block broadcast_507_piece0
15/05/13 12:03:00 INFO SparkContext: Created broadcast 507 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 12:03:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 12:03:00 INFO JobScheduler: Added jobs for time 1431532980000 ms
15/05/13 12:03:00 INFO JobGenerator: Checkpointing graph for time 1431532980000 ms
15/05/13 12:03:00 INFO DStreamGraph: Updating checkpoint data for time 1431532980000 ms
15/05/13 12:03:00 INFO JobScheduler: Starting job streaming job 1431532980000 ms.0 from job set of time 1431532980000 ms
15/05/13 12:03:00 INFO DStreamGraph: Updated checkpoint data for time 1431532980000 ms
15/05/13 12:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84355): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 12:03:00 INFO DAGScheduler: Got job 345 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 12:03:00 INFO DAGScheduler: Final stage: Stage 338(reduce at JsonRDD.scala:51)
15/05/13 12:03:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:03:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:03:00 INFO DAGScheduler: Submitting Stage 338 (MapPartitionsRDD[2414] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=39035865, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_508 stored as values in memory (estimated size 5.9 KB, free 228.2 MB)
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(4226) called with curMem=39041905, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_508_piece0 stored as bytes in memory (estimated size 4.1 KB, free 228.2 MB)
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_508_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.5 MB)
15/05/13 12:03:00 INFO BlockManagerMaster: Updated info of block broadcast_508_piece0
15/05/13 12:03:00 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84357): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:03:00 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:00 INFO SparkContext: Created broadcast 508 from broadcast at DAGScheduler.scala:839
15/05/13 12:03:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 338 (MapPartitionsRDD[2414] at map at JsonRDD.scala:51)
15/05/13 12:03:00 INFO TaskSchedulerImpl: Adding task set 338.0 with 1 tasks
15/05/13 12:03:00 INFO TaskSetManager: Starting task 0.0 in stage 338.0 (TID 338, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_508_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 4.1 KB, free: 263.6 MB)
15/05/13 12:03:00 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84359): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:03:00 WARN CheckpointWriter: Could not write checkpoint for time 1431532980000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_507_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 34.9 KB, free: 263.6 MB)
15/05/13 12:03:00 INFO TaskSetManager: Finished task 0.0 in stage 338.0 (TID 338) in 187 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 12:03:00 INFO TaskSchedulerImpl: Removed TaskSet 338.0, whose tasks have all completed, from pool 
15/05/13 12:03:00 INFO DAGScheduler: Stage 338 (reduce at JsonRDD.scala:51) finished in 0.200 s
15/05/13 12:03:00 INFO DAGScheduler: Job 345 finished: reduce at JsonRDD.scala:51, took 0.238968 s
15/05/13 12:03:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 12:03:00 INFO DAGScheduler: Got job 346 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 12:03:00 INFO DAGScheduler: Final stage: Stage 339(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:03:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:03:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:03:00 INFO DAGScheduler: Submitting Stage 339 (MapPartitionsRDD[2421] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(21096) called with curMem=39046131, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_509 stored as values in memory (estimated size 20.6 KB, free 228.2 MB)
15/05/13 12:03:00 INFO MemoryStore: ensureFreeSpace(10945) called with curMem=39067227, maxMem=278302556
15/05/13 12:03:00 INFO MemoryStore: Block broadcast_509_piece0 stored as bytes in memory (estimated size 10.7 KB, free 228.1 MB)
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_509_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.7 KB, free: 260.5 MB)
15/05/13 12:03:00 INFO BlockManagerMaster: Updated info of block broadcast_509_piece0
15/05/13 12:03:00 INFO SparkContext: Created broadcast 509 from broadcast at DAGScheduler.scala:839
15/05/13 12:03:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 339 (MapPartitionsRDD[2421] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:03:00 INFO TaskSchedulerImpl: Adding task set 339.0 with 1 tasks
15/05/13 12:03:00 INFO TaskSetManager: Starting task 0.0 in stage 339.0 (TID 339, pti-node-1.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:03:00 INFO BlockManagerInfo: Added broadcast_509_piece0 in memory on pti-node-1.insafanalytics.com:34093 (size: 10.7 KB, free: 263.5 MB)
15/05/13 12:03:00 INFO TaskSetManager: Finished task 0.0 in stage 339.0 (TID 339) in 135 ms on pti-node-1.insafanalytics.com (1/1)
15/05/13 12:03:00 INFO TaskSchedulerImpl: Removed TaskSet 339.0, whose tasks have all completed, from pool 
15/05/13 12:03:00 INFO DAGScheduler: Stage 339 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.138 s
15/05/13 12:03:00 INFO DAGScheduler: Job 346 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.177403 s
15/05/13 12:03:00 INFO JobScheduler: Finished job streaming job 1431532980000 ms.0 from job set of time 1431532980000 ms
15/05/13 12:03:00 INFO JobScheduler: Total delay: 0.996 s for time 1431532980000 ms (execution: 0.808 s)
15/05/13 12:03:00 INFO MapPartitionsRDD: Removing RDD 2396 from persistence list
15/05/13 12:03:01 INFO BlockManager: Removing RDD 2396
15/05/13 12:03:01 INFO UnionRDD: Removing RDD 2395 from persistence list
15/05/13 12:03:01 INFO BlockManager: Removing RDD 2395
15/05/13 12:03:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532920000 ms: 1431532860000 ms
15/05/13 12:03:01 INFO JobGenerator: Checkpointing graph for time 1431532980000 ms
15/05/13 12:03:01 INFO DStreamGraph: Updating checkpoint data for time 1431532980000 ms
15/05/13 12:03:01 INFO DStreamGraph: Updated checkpoint data for time 1431532980000 ms
15/05/13 12:03:01 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:01 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84361): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:03:01 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84361): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:03:01 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:01 WARN CheckpointWriter: Error in attempt 2 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84363): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:03:01 INFO CheckpointWriter: Saving checkpoint for time 1431532980000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:03:01 WARN CheckpointWriter: Error in attempt 3 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84365): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFileInternal(FSNamesystem.java:3647)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3617)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:704)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.complete(AuthorizationProviderProxyClientProtocol.java:243)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:527)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.complete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:412)
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:2135)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:2119)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.spark.streaming.CheckpointWriter$CheckpointWriteHandler.run(Checkpoint.scala:140)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/05/13 12:03:01 WARN CheckpointWriter: Could not write checkpoint for time 1431532980000 ms to file hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532980000'
15/05/13 12:04:00 INFO FileInputDStream: Finding new files took 55 ms
15/05/13 12:04:00 INFO FileInputDStream: New files at time 1431533040000 ms:
hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/15-05-13/pti-twitter-stream.1431532935779.json
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(238532) called with curMem=39078172, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_510 stored as values in memory (estimated size 232.9 KB, free 227.9 MB)
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(35708) called with curMem=39316704, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_510_piece0 stored as bytes in memory (estimated size 34.9 KB, free 227.9 MB)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_510_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 34.9 KB, free: 260.5 MB)
15/05/13 12:04:00 INFO BlockManagerMaster: Updated info of block broadcast_510_piece0
15/05/13 12:04:00 INFO SparkContext: Created broadcast 510 from textFileStream at NativeMethodAccessorImpl.java:-2
15/05/13 12:04:00 INFO FileInputFormat: Total input paths to process : 1
15/05/13 12:04:00 INFO JobScheduler: Added jobs for time 1431533040000 ms
15/05/13 12:04:00 INFO JobGenerator: Checkpointing graph for time 1431533040000 ms
15/05/13 12:04:00 INFO DStreamGraph: Updating checkpoint data for time 1431533040000 ms
15/05/13 12:04:00 INFO JobScheduler: Starting job streaming job 1431533040000 ms.0 from job set of time 1431533040000 ms
15/05/13 12:04:00 INFO DStreamGraph: Updated checkpoint data for time 1431533040000 ms
15/05/13 12:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431533040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000'
15/05/13 12:04:00 WARN DFSClient: DataStreamer Exception
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84375): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:00 WARN CheckpointWriter: Error in attempt 1 of writing checkpoint to hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84375): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:00 INFO CheckpointWriter: Saving checkpoint for time 1431533040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000'
15/05/13 12:04:00 INFO CheckpointWriter: Deleting hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431532740000.bk
15/05/13 12:04:00 INFO CheckpointWriter: Checkpoint for time 1431533040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000', took 7673 bytes and 85 ms
15/05/13 12:04:00 INFO SparkContext: Starting job: reduce at JsonRDD.scala:51
15/05/13 12:04:00 INFO DAGScheduler: Got job 347 (reduce at JsonRDD.scala:51) with 1 output partitions (allowLocal=false)
15/05/13 12:04:00 INFO DAGScheduler: Final stage: Stage 340(reduce at JsonRDD.scala:51)
15/05/13 12:04:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:04:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:04:00 INFO DAGScheduler: Submitting Stage 340 (MapPartitionsRDD[2428] at map at JsonRDD.scala:51), which has no missing parents
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(6040) called with curMem=39352412, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_511 stored as values in memory (estimated size 5.9 KB, free 227.9 MB)
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(4227) called with curMem=39358452, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_511_piece0 stored as bytes in memory (estimated size 4.1 KB, free 227.9 MB)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_511_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 4.1 KB, free: 260.5 MB)
15/05/13 12:04:00 INFO BlockManagerMaster: Updated info of block broadcast_511_piece0
15/05/13 12:04:00 INFO SparkContext: Created broadcast 511 from broadcast at DAGScheduler.scala:839
15/05/13 12:04:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 340 (MapPartitionsRDD[2428] at map at JsonRDD.scala:51)
15/05/13 12:04:00 INFO TaskSchedulerImpl: Adding task set 340.0 with 1 tasks
15/05/13 12:04:00 INFO TaskSetManager: Starting task 0.0 in stage 340.0 (TID 340, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_511_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 4.1 KB, free: 263.5 MB)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_510_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 34.9 KB, free: 263.5 MB)
15/05/13 12:04:00 INFO TaskSetManager: Finished task 0.0 in stage 340.0 (TID 340) in 339 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 12:04:00 INFO TaskSchedulerImpl: Removed TaskSet 340.0, whose tasks have all completed, from pool 
15/05/13 12:04:00 INFO DAGScheduler: Stage 340 (reduce at JsonRDD.scala:51) finished in 0.351 s
15/05/13 12:04:00 INFO DAGScheduler: Job 347 finished: reduce at JsonRDD.scala:51, took 0.382304 s
15/05/13 12:04:00 INFO SparkContext: Starting job: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206
15/05/13 12:04:00 INFO DAGScheduler: Got job 348 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) with 1 output partitions (allowLocal=false)
15/05/13 12:04:00 INFO DAGScheduler: Final stage: Stage 341(call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:04:00 INFO DAGScheduler: Parents of final stage: List()
15/05/13 12:04:00 INFO DAGScheduler: Missing parents: List()
15/05/13 12:04:00 INFO DAGScheduler: Submitting Stage 341 (MapPartitionsRDD[2435] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206), which has no missing parents
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(20920) called with curMem=39362679, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_512 stored as values in memory (estimated size 20.4 KB, free 227.9 MB)
15/05/13 12:04:00 INFO MemoryStore: ensureFreeSpace(10854) called with curMem=39383599, maxMem=278302556
15/05/13 12:04:00 INFO MemoryStore: Block broadcast_512_piece0 stored as bytes in memory (estimated size 10.6 KB, free 227.8 MB)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_512_piece0 in memory on pti-base.insafanalytics.com:44723 (size: 10.6 KB, free: 260.5 MB)
15/05/13 12:04:00 INFO BlockManagerMaster: Updated info of block broadcast_512_piece0
15/05/13 12:04:00 INFO SparkContext: Created broadcast 512 from broadcast at DAGScheduler.scala:839
15/05/13 12:04:00 INFO DAGScheduler: Submitting 1 missing tasks from Stage 341 (MapPartitionsRDD[2435] at call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206)
15/05/13 12:04:00 INFO TaskSchedulerImpl: Adding task set 341.0 with 1 tasks
15/05/13 12:04:00 INFO TaskSetManager: Starting task 0.0 in stage 341.0 (TID 341, pti-node-2.insafanalytics.com, NODE_LOCAL, 1563 bytes)
15/05/13 12:04:00 INFO BlockManagerInfo: Added broadcast_512_piece0 in memory on pti-node-2.insafanalytics.com:33883 (size: 10.6 KB, free: 263.4 MB)
15/05/13 12:04:01 INFO TaskSetManager: Finished task 0.0 in stage 341.0 (TID 341) in 119 ms on pti-node-2.insafanalytics.com (1/1)
15/05/13 12:04:01 INFO TaskSchedulerImpl: Removed TaskSet 341.0, whose tasks have all completed, from pool 
15/05/13 12:04:01 INFO DAGScheduler: Stage 341 (call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206) finished in 0.119 s
15/05/13 12:04:01 INFO DAGScheduler: Job 348 finished: call at /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py:1206, took 0.148858 s
15/05/13 12:04:01 INFO JobScheduler: Finished job streaming job 1431533040000 ms.0 from job set of time 1431533040000 ms
15/05/13 12:04:01 INFO JobScheduler: Total delay: 1.071 s for time 1431533040000 ms (execution: 0.887 s)
15/05/13 12:04:01 INFO MapPartitionsRDD: Removing RDD 2410 from persistence list
15/05/13 12:04:01 INFO BlockManager: Removing RDD 2410
15/05/13 12:04:01 INFO UnionRDD: Removing RDD 2409 from persistence list
15/05/13 12:04:01 INFO BlockManager: Removing RDD 2409
15/05/13 12:04:01 INFO FileInputDStream: Cleared 1 old files that were older than 1431532980000 ms: 1431532920000 ms
15/05/13 12:04:01 INFO JobGenerator: Checkpointing graph for time 1431533040000 ms
15/05/13 12:04:01 INFO DStreamGraph: Updating checkpoint data for time 1431533040000 ms
15/05/13 12:04:01 INFO DStreamGraph: Updated checkpoint data for time 1431533040000 ms
15/05/13 12:04:01 INFO CheckpointWriter: Saving checkpoint for time 1431533040000 ms to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000'
15/05/13 12:04:01 INFO CheckpointWriter: Checkpoint for time 1431533040000 ms saved to file 'hdfs://pti-base.insafanalytics.com:8020/user/flume/tweet-stream/sc/checkpoint-1431533040000', took 7657 bytes and 54 ms
15/05/13 12:04:01 INFO DStreamGraph: Clearing checkpoint data for time 1431533040000 ms
15/05/13 12:04:01 INFO DStreamGraph: Cleared checkpoint data for time 1431533040000 ms
15/05/13 12:04:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83153
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83153): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83161
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83161): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83160
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83160): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83146
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83146): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83999
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83999): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83689
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83689): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83151
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83151): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83193
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83193): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83678
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83678): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83199
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83199): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84032
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84032): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84060
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84060): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84063
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84063): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84066
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84066): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83126
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83126): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83603
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83603): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83612
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83612): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83118
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84108
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84108): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83027
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83027): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83564
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83564): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83008
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83008): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84118
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84118): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83010
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83010): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83012
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83012): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83070
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83070): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83547
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83547): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83058
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83058): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83057
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83057): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83051
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83051): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84157
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84157): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83052
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83052): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84145
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84145): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82960
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82960): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82956
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82956): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82953
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82953): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82952
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82952): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84192
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84192): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82994
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82994): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82993
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82993): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83460
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83460): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84255
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84255): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82872
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82872): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82879
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82879): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82878
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82878): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82868
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82868): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84232
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84232): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82870
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82870): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83428
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83428): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83907
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83907): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83917
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83917): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82817
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82817): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83452
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83452): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84305
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84305): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82927
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82927): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83330
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83330): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82941
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82941): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82940
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82940): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83891
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83891): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83348
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83348): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83357
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83357): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84301
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84301): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84344
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84344): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82911
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82911): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82901
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82901): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84334
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84334): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82725
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82725): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84375
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84375): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83267
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83267): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 84361
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 84361): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82748
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82748): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82749
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82749): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83827
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83827): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83780
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83780): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83778
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83778): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83807
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83807): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83326
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83326): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82712
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83794
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83794): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82784
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82784): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83207
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83207): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83231
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83231): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82806
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82806): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83222
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83222): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82808
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82808): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3259)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83765
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83765): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83723
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83723): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 83712
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 83712): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82775
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82775): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
15/05/13 12:04:18 ERROR DFSClient: Failed to close inode 82773
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/flume/tweet-stream/sc/temp (inode 82773): File does not exist. [Lease.  Holder: DFSClient_NONMAPREDUCE_-1416208766_14, pendingcreates: 1]
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3559)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3356)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3212)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:645)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:212)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:483)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1411)
	at org.apache.hadoop.ipc.Client.call(Client.java:1364)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy12.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:368)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1449)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1270)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:526)
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/avro-tools-1.7.6-cdh5.4.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/jars/avro-tools-1.7.6-cdh5.4.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
15/05/14 16:16:57 INFO Worker: Registered signal handlers for [TERM, HUP, INT]
15/05/14 16:16:58 INFO SecurityManager: Changing view acls to: root
15/05/14 16:16:58 INFO SecurityManager: Changing modify acls to: root
15/05/14 16:16:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)
15/05/14 16:16:59 INFO Slf4jLogger: Slf4jLogger started
15/05/14 16:16:59 INFO Remoting: Starting remoting
15/05/14 16:16:59 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827]
15/05/14 16:16:59 INFO Utils: Successfully started service 'sparkWorker' on port 55827.
15/05/14 16:17:00 INFO Worker: Starting Spark worker pti-base.insafanalytics.com:55827 with 2 cores, 1024.0 MB RAM
15/05/14 16:17:00 INFO Worker: Running Spark version 1.3.0
15/05/14 16:17:00 INFO Worker: Spark home: /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark
15/05/14 16:17:00 INFO Server: jetty-8.y.z-SNAPSHOT
15/05/14 16:17:00 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:8081
15/05/14 16:17:00 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
15/05/14 16:17:00 INFO WorkerWebUI: Started WorkerWebUI at http://pti-base.insafanalytics.com:8081
15/05/14 16:17:00 INFO Worker: Connecting to master akka.tcp://sparkMaster@pti-base.insafanalytics.com:7079/user/Master...
15/05/14 16:17:00 INFO Worker: Successfully registered with master spark://pti-base.insafanalytics.com:7079
15/05/14 16:19:30 INFO Worker: Asked to launch executor app-20150514161930-0000/2 for PTITwitterStream
15/05/14 16:19:31 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/jre-1.7.0-openjdk.x86_64/bin/java" "-cp" "/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:::/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/conf:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/lib/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar:/etc/hadoop/conf:/opt/cloudera/parcels/CDH/lib/hadoop/client/*:/etc/hadoop/conf/:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop/.//*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/./:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/.//*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-yarn/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-yarn/.//*:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/./:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/lib/*:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/.//*:/opt/cloudera/parcels/CDH/lib/hive/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/flume-ng/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/../parquet/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/../avro/*" "-XX:MaxPermSize=128m" "-Dspark.driver.port=44922" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@pti-base.insafanalytics.com:44922/user/CoarseGrainedScheduler" "--executor-id" "2" "--hostname" "pti-base.insafanalytics.com" "--cores" "2" "--app-id" "app-20150514161930-0000" "--worker-url" "akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827/user/Worker"
15/05/14 20:09:01 INFO Worker: Executor app-20150514161930-0000/2 finished with state EXITED message Command exited with code 1 exitStatus 1
15/05/14 20:09:01 INFO LocalActorRef: Message [akka.remote.transport.ActorTransportAdapter$DisassociateUnderlying] from Actor[akka://sparkWorker/deadLetters] to Actor[akka://sparkWorker/system/transports/akkaprotocolmanager.tcp0/akkaProtocol-tcp%3A%2F%2FsparkWorker%4045.55.231.94%3A39952-2#-2136638413] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.
15/05/14 20:09:01 ERROR EndpointWriter: AssociationError [akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827] -> [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]: Error [Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]] [
akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]
Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: pti-base.insafanalytics.com/45.55.231.94:35153
]
15/05/14 20:09:01 ERROR EndpointWriter: AssociationError [akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827] -> [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]: Error [Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]] [
akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]
Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: pti-base.insafanalytics.com/45.55.231.94:35153
]
15/05/14 20:09:01 ERROR EndpointWriter: AssociationError [akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827] -> [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]: Error [Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]] [
akka.remote.EndpointAssociationException: Association failed with [akka.tcp://sparkExecutor@pti-base.insafanalytics.com:35153]
Caused by: akka.remote.transport.netty.NettyTransport$$anonfun$associate$1$$anon$2: Connection refused: pti-base.insafanalytics.com/45.55.231.94:35153
]
15/05/14 20:09:02 INFO Worker: Asked to kill unknown executor app-20150514161930-0000/2
15/05/14 20:09:02 INFO Worker: Cleaning up local directories for application app-20150514161930-0000
15/05/15 12:06:55 INFO Worker: Asked to launch executor app-20150515120655-0001/2 for PTITwitterStream
15/05/15 12:06:55 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/jre-1.7.0-openjdk.x86_64/bin/java" "-cp" "/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:/var/lib/hive/datafu-1.2.0.jar:/var/lib/hive/spark-csv_2.10-1.0.0.jar:/var/lib/hive/ddf_spark_2.10-1.1.jar:/var/lib/hive/spark-hbase-connector-0.9.5.jar:/var/lib/hive/nexr-hive-udf-0.2-SNAPSHOT.jar:/var/lib/hive/hive-serdes-1.0-SNAPSHOT.jar:/var/lib/hive/csv-serde-1.1.2-0.11.0-all.jar:/var/lib/hive/joda-time-2.4.jar:/var/lib/hive/nscala-time_2.10-0.2.0.jar:/var/lib/hive/esper-5.2.0.jar:/var/lib/hive/spark-streaming-kafka-assembly-1.3.0-SNAPSHOT.jar:::/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/conf:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/lib/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar:/etc/hadoop/conf:/opt/cloudera/parcels/CDH/lib/hadoop/client/*:/etc/hadoop/conf/:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop/.//*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/./:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-hdfs/.//*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-yarn/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/libexec/../../hadoop-yarn/.//*:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/./:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/lib/*:/opt/cloudera/parcels/CDH/lib/hadoop-0.20-mapreduce/.//*:/opt/cloudera/parcels/CDH/lib/hive/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/flume-ng/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/../parquet/lib/*:/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/hadoop/../avro/*" "-XX:MaxPermSize=128m" "-Dspark.driver.port=39139" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "akka.tcp://sparkDriver@pti-base.insafanalytics.com:39139/user/CoarseGrainedScheduler" "--executor-id" "2" "--hostname" "pti-base.insafanalytics.com" "--cores" "2" "--app-id" "app-20150515120655-0001" "--worker-url" "akka.tcp://sparkWorker@pti-base.insafanalytics.com:55827/user/Worker"
15/05/15 12:27:26 ERROR Worker: RECEIVED SIGNAL 15: SIGTERM
15/05/15 12:27:27 INFO ExecutorRunner: Killing process!
15/05/15 12:27:27 INFO ExecutorRunner: Killing process!
15/05/15 12:27:27 INFO Worker: Unknown Executor app-20150514161930-0000/2 finished with state EXITED message Worker shutting down exitStatus 1
15/05/15 12:27:27 ERROR FileAppender: Error writing stream to file /opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.27/lib/spark/work/app-20150515120655-0001/2/stderr
java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at java.io.FilterInputStream.read(FilterInputStream.java:107)
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
15/05/15 12:27:27 INFO Worker: Executor app-20150515120655-0001/2 finished with state EXITED message Command exited with code 143 exitStatus 143
